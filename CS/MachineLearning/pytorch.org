#+SETUPFILE: ../configOrg/level2.org
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:nil arch:headline author:t c:nil
#+OPTIONS: creator:nil d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t
#+OPTIONS: num:t p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t timestamp:t
#+OPTIONS: title:t toc:t todo:t |:t
#+TITLES: pytorch
#+DATE: <2021-12-21 Tue>
#+AUTHORS: weiwu
#+EMAIL: victor.wuv@gmail.com
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 24.5.1 (Org mode 8.3.4)

#+BEGIN_SRC python

#+END_SRC

- operation:
#+BEGIN_SRC txt
torch
├── (Tensor)
│   ├── view(*shape)      # e.g. x.view(-1, 3, 12) 
│   │                     ##  -1 automatically filled
│   └── item()            # get if Tensor is a scalar
│
├── empty(*size)          # e.g. x = torch.empty(2, 3)
├── stack(tensors, dim=0)
└── cat(tensors, dim=0)

#+END_SRC

- Data Preparation:
#+BEGIN_SRC txt
torch
└── utils
    └── data
        ├── Dataset     # A class to override 
        │               ##  `__len__` & `__getitem__`
        ├── TensorDataset(data_tensor, target_tensor)
        ├── DataLoader(dataset, batch_size=1,
        │              shuffle=False,
        │              collate_fn=\
        │                  <function default_collate>)
        │               # define `collate_fn` yourself
        └── sampler
            ├── SequentialSampler(data_source)
            └── RandomSampler(data_source)

#+END_SRC

- NN (Neural Network) Model Construction:
#+BEGIN_SRC txt
Training:
torch
├── (Tensor)
│   ├── backward()
│   │
│   ├── cpu()
│   ├── cuda()
│   └── to(torch.device)            # x = x.to(device)
├── cuda
│   └── is_available()
│       #  if torch.cuda.is_available():
│       ##     device = "cuda"
│       ## else: device = "cpu"
│
├── nn as nn
│   │### Models ###
│   ├── Module
│   │   ├── load_state_dict(torch.load(PATH))
│   │   ├── train()
│   │   └── eval()
│   ├── Sequential(layers)
│   │
│   │### Initializations ###
│   ├── init
│   │   └── uniform_(w)     # In-place, 
│   │                       ##  w is a `torch.Tensor`.
│   │
│   │### Layers ###
│   ├── Linear(in_feat, out_feat)
│   ├── Dropout(rate)
│   │
│   │### Activations ###
│   ├── Softmax(dim=None)
│   ├── Sigmoid()
│   ├── ReLU()
│   ├── LeakyReLU(negative_slope=0.01)
│   ├── Tanh()
│   ├── GELU()
│   ├── ReLU6() # Model Compression
│   │ # --> Corresponding functions
│   ├── functional as F  ────────────────────────────┐
│   │   ├── softmax(input, dim=None)                 │
│   │   ├── sigmoid(input)                           │
│   │   ├── relu(input)                              │
│   │   ├── leaky_relu(input,                        │
│   │   │              negative_slope=0.01)          │
│   │   ├── tanh(input)                              │
│   │   ├── gelu(input)                              │
│   │   └── relu6(input)                             │
│   │                                                │
│   │### Losses ###                                  │
│   ├── MSELoss()                                    │
│   ├── CrossEntropyLoss()                           │
│   ├── BCELoss()                                    │
│   ├── NLLLoss()                                    │
│   │ # --> Corresponding functions                  │
│   └──<functional as F> <───────────────────────────┘
│       ├── mse_loss(input, target)
│       ├── cross_entropy(input, 
│       │                 target: torch.LongTensor)
│       ├── binary_cross_entropy(input, target)
│       ├── log_softmax(input)
│       └── nll_loss(log_softmax_output, target)
│           # F.nll_loss(F.log_softmax(input), target)
│
│    ### Optimizers ###
├── optim
│   ├── (Optimizer)
│   │       ├── zero_grad()
│   │       ├── step()
│   │       └── state_dict()
│   │  
│   ├── SGD(model.parameters(), lr=0.1, momentum=0.9)
│   ├── Adagrad(model.parameters(), lr=0.01, 
│   │           lr_decay=0, weight_decay=0, 
│   │           initial_accumulator_value=0,eps=1e-10)
│   ├── RMSProp(model.parameters(), lr=0.01, 
│   │           alpha=0.99, eps=1e-08, weight_decay=0,
│   │           momentum=0)
│   ├── Adam(model.parameters(), lr=0.001, 
│   │        betas=(0.9, 0.999), eps=1e-08,
│   │        weight_decay=0)
│   │   
│   └── lr_scheduler
│       └── ReduceLROnPlateau(optimizer)
│
│── load(PATH)
│── save(model, PATH)
│
└── autograd
    └── backward(tensors)

Testing:
torch
├── nn
│   └── Module
│       ├── load_state_dict(torch.load(PATH))
│       └── eval()
├── optim
│   └── (Optimizer)
│       └── state_dict()
└── no_grad()              # with torch.no_grad(): ...

#+END_SRC

- CNN:
#+BEGIN_SRC txt
torch
├── (Tensor)
│   └── view(*shape)
├── nn
│   │### Layers ###
│   ├── Conv2d(in_channels, out_channels, 
│   │          kernel_size, stride=1, padding=0)
│   ├── ConvTranspose2d(in_channels, out_channels, 
│   │          kernel_size, stride=1, padding=0, 
│   │          output_padding=0)
│   ├── MaxPool2d(kernel_size, stride=None, 
│   │             padding=0, dilation=1)
│   │             # stride default: kernel_size
│   ├── BatchNorm2d(num_feat)
│   └── BatchNorm1d(num_feat)
├── stack(tensors, dim=0)
└── cat(tensors, dim=0)

torchvision
├── models as models # Useful pretrained
├── transforms as transforms
│   ├── Compose(transforms) # Wrapper
│   ├── ToPILImage(mode=None)
│   ├── RandomHorizontalFlip(p=0.5)
│   ├── RandomRotation(degrees)
│   ├── ToTensor()
│   └── Resize(size)
└── utils
    ├── make_grid(tensor, nrow=8, padding=2)
    └── save_image(tensor, filename, nrow=8,padding=2)

#+END_SRC

- RNN:
#+BEGIN_SRC txt
torch
├── nn
│   ├── Embedding(num_embed, embed_dim)
│   │   # embedding = nn.Embedding(
│   │   ##               *(w2vmodel.wv.vectors.shape))
│   ├── Parameter(params: torch.FloatTensor)
│   │   # embedding.weight = nn.Parameter(
│   │   ##  torch.FloatTensor(w2vmodel.wv.vectors))
│   ├── LongTensor          # Feeding Indices of words
│   │
│   ├── LSTM(inp_size, hid_size, num_layers)
│   │   # input: input, (h_0, c_0)
│   └── GRU(inp_size, hid_size, num_layers)
├── stack(tensors, dim=0)
└── cat(tensors, dim=0)
    
gensim
└── models
    └── word2Vec
        └── Word2Vec(sentences) # list or words/tokens
#+END_SRC

- custom dataloader:
#+BEGIN_SRC python

class CustomDataset(torch.utils.data.Dataset):#需要继承data.Dataset
    def __init__(self, train, label):
        # TODO
        # 1. Initialize file path or list of file names.
        self.toTensor = transforms.ToTensor()
        self.train_data = train
        self.label_data = label
        
    def __getitem__(self, index):
        # TODO
        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).
        # 2. Preprocess the data (e.g. torchvision.Transform).
        # 3. Return a data pair (e.g. image and label).
        #这里需要注意的是，第一步：read one data，是一个data
        train_data = self.train_data[index]
        label = self.label_data[index]
        train_data = np.array(train_data).astype(np.float)
        train_data = train_data.reshape(1, train_data.shape[0], train_data.shape[1])
        train_data = self.toTensor(train_data)
#         label_data = np.array(label).astype(np.float)
#         label_data = label_data.reshape(1, label_data.shape[0], label_data.shape[1])
#         label_data = self.toTensor(label_data)
        return train_data
    
    def __len__(self):
        # You should change 0 to the total size of your dataset.
        return len(self.train_data)
# 5. 创建数据集的可迭代对象，也就是说一个batch一个batch的读取数据
inputs = CustomDataset(X_train, y_train)
train_loader = torch.utils.data.DataLoader(dataset=inputs, batch_size=BATCH_SIZE, shuffle=False)
train_loader = torch.utils.data.DataLoader(dataset=list(zip(X_train, y_train)), batch_size=BATCH_SIZE, shuffle=False)
test_loader = torch.utils.data.DataLoader(dataset=list(zip(X_test, y_test)), batch_size=BATCH_SIZE, shuffle=True)

########## or :

tensor_x_train = torch.Tensor(X_train) # transform to torch tensor
tensor_y_train = torch.Tensor(y_train)
tensor_x_test = torch.Tensor(X_test) # transform to torch tensor
tensor_y_test = torch.Tensor(y_test)
my_dataset1 = torch.utils.data.TensorDataset(tensor_x_train,tensor_y_train) # create your datset
my_dataset2 = torch.utils.data.TensorDataset(tensor_x_test,tensor_y_test) # create your datset
train_loader = torch.utils.data.DataLoader(my_dataset1, batch_size=BATCH_SIZE, shuffle=True) # create your dataloader
test_loader = torch.utils.data.DataLoader(my_dataset2, batch_size=BATCH_SIZE, shuffle=True) # create your dataloader


######### or :
from torch.utils.data import Dataset, DataLoader


class FakeDataset(Dataset):

    def __init__(self, x, y):
        self.x = x
        self.y = y

    def __len__(self):
        return len(self.x)

    def __getitem__(self, idx):
        return self.x[idx], self.y[idx]

x = np.random.rand(100, 10)
y = np.random.rand(100)

dataset = FakeDataset(x, y)
dataloader = DataLoader(dataset, batch_size=4,
                        shuffle=True, num_workers=4)

for i_batch, sample_batched in enumerate(dataloader):
    print(i_batch, sample_batched)        
#+END_SRC

- turn numpy array into tensor:
#+BEGIN_SRC python
t = torch.from_numpy(arr)
#+END_SRC

- change dtype:
#+BEGIN_SRC python
t = target.to(torch.float)
#+END_SRC

- save and load model
#+BEGIN_SRC python
torch.save(the_model.state_dict(), PATH)
the_model.load_state_dict(torch.load(PATH))
#+END_SRC

- print parameters:
#+BEGIN_SRC python
for name,parameters in net.named_parameters():
    print(name,':',parameters.size())
#+END_SRC


