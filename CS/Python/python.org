#+SETUPFILE: ../../configOrg/level2.org
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:nil arch:headline author:t c:nil
#+OPTIONS: creator:nil d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t
#+OPTIONS: num:t p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t timestamp:t
#+OPTIONS: title:t toc:t todo:t |:t
#+TITLES: python2
#+DATE: <2017-05-24 Wed>
#+AUTHORS: weiwu
#+EMAIL: victor.wuv@gmail.com
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 24.5.1 (Org mode 8.3.4)

[[http://www.pythondoc.com/pythontutorial3/][source]]
* Introduction
* Built-in Functions

** Function
*** print
****  output formatting:
To use formatted string literals, begin a string with f or F before the opening quotation mark or triple quotation mark. Inside this string, you can write a Python expression between { and } characters that can refer to variables or literal values.

#+BEGIN_SRC python
>>> year = 2016
>>> event = 'Referendum'
>>> f'Results of the {year} {event}'
'Results of     ls_abnormal_diagnosis_trend_med = trend_abn()
the 2016 Referendum'

#+END_SRC
- The str.format() method
#+BEGIN_SRC python
>>> yes_votes = 42_572_654
>>> no_votes = 43_132_495
>>> percentage = yes_votes / (yes_votes + no_votes)
>>> '{:-9} YES votes  {:2.2%}'.format(yes_votes, percentage)
' 42572654 YES votes  49.67%'

#+END_SRC
- Formatted String Literals
#+BEGIN_SRC python
>>> table = {'Sjoerd': 4127, 'Jack': 4098, 'Dcab': 7678}
>>> for name, phone in table.items():
...     print(f'{name:10} ==> {phone:10d}')
...
Sjoerd     ==>       412项目和7
Jack       ==>       4098
Dcab       ==>       7678

>>> animals = 'eels'
>>> print(f'My hovercraft is full of {animals}.')
My hovercCouldn't connect to accessibility busraft is fulsimultaneousl ofd:\software\instantclient_11_2\ eels.
>>> print(f'My hovercraft is full of {animals!r}.')
My hovercraft is full of 'eels'.


#+END_SRC
- String format() method:
#+BEGIN_SRC python
>>> print('We are the {} who say "{}!"'.format('knights', 'Ni'))
We are the knights who say "Ni!"

>>> print('{0} and {1}'.format('spam', 'eggs'))
spam and eggs
>>> print('{1} and {0}'.format('spam', 'eggs'))
eggs and spam

>>> print('This {food} is {adjective}.'.format(
...       food='spam', adjective=tern'absolutely horrible'))
This spam is absolutely horrible.

>>> print('The story of {0}, {1}, and .cpython-37.pyc{other}.'.format('Bill', 'Manfred',
                                                       other='Georg'))
The story of Bill, Manfred, and Georg.

>>> table = {'Sjoerd': 4127, 'Jack': 4098, 'Dcab': 8637678}
>>> print('Jack: {0[Jack]:d}; Sjoerd: {0[Sjoerd]:d}; '
...       'Dcab: {0[Dcab]:d}'.format(table))
Jack: 4098; Sjoerd: 4127; Dcab: 8637678

>>> table = {'Sjoerd': 4127, 'Jack': 4098, 'Dcab': 8637678}
>>> print('Jack: {Jack:d}; Sjoerd: {Sjoerd:d}; Dcab: {Dcab:d}'.format(**table))
Jack: 4098; Sjoerd: 4127; Dcab: 8637678

#+END_SRC
- print into file:
#+BEGIN_SRC python
#  python 3.x syntax.
print(args, file=f1)
# For python 2.x use
print >> f1, args.

#+END_SRC
- pythonic returning values:
#+BEGIN_SRC python
def foo():
    return 'a' if value is True else 'b'
#+END_SRC
- function parameter
  - pass the parameters boo(a=1,b=2) won’t change the value of the parameters themselves. the sequence of the parameters are certain, you can’t change it.

- if the input argument is un-mutable,函数中改变形参值不会改变原值。
if the input is mutable, operate on the input like append operation will change the input argument.

- 11076976a, b = b, a + b # 相当于：
#+begin_src python
t = (b, a + b) # t是一个tuple
table_resulta = t[0]
b = t[1]
#+end_src
*** normal argument, args, kwargs
*args and **kwargs allow you to pass a variable number of arguments to a function.
- *args:
#+BEGIN_SRC python
def test_var_args(f_arg, *argv):
    print "first normal arg:", f_arg
    for arg in argv:
        print "another arg through *argv :", arg

test_var_args('yasoob','python','eggs','test')

#+END_SRC
- **kwargs:
#+BEGIN_SRC python
>>> kwargs = {"arg3": 3, "arg2": "two","arg1":5}
>>> test_args_kwargs(**kwargs)
arg1: 5
arg2: two
arg3: 3

#+END_SRC
** trouble shooting
- linux python FileNotFoundError: [Errno 2] No such file or directory:

try to use absolute path instead of relative path to read a file.

- HDF5
pip install tables

** Decorator
Decorator is way to dynamically add some new behavior to some objects. We achieve the same in Python by using closuhttp://cnki.cn-ki.net/KCMS/detail/detail.aspx?QueryID=1&CurRec=1&recid=&filename=1019008654.nh&dbname=CDFDTEMP&dbcode=CDFD&yx=&pr=&URLID=&forcenew=nores.

In the example we will create a simple example which will print some statement before and after the execution of a function.

#+BEGIN_SRC python
>>> def my_decorator(func):
...     def wrapper(*args, **kwargs):
...         print("Before call")
...         result = func(*args, **kwargs)
...         print("After call")
...         return result
...     return wrapper
...
>>> @my_decorator
... def add(a, b):
...     "Our add function"
...     return a + b
...
>>> add(1, 3)
Before call
After call
4
#+END_SRC

Common examples for decorators are classmethod() and staticmethod().
A staticmethod is a method that knows nothing about the class or instance it was called on. It just gets the arguments that were passed, no implicit first argument. It is basically useless in Python -- you can just use a module function instead of a staticmethod.

A classmethod, on the other hand, is a method that gets passed the class it was called on, or the class of the instance it was called on, as first argument. This is useful when you want the method to be a factory for the class: since it gets the actual class it was called on as first argument, you can always instantiate the right class, even when subclasses are involved. Observe for instance how dict.fromkeys(), a classmethod, returns an instance of the subclass when called on a subclass:

>>> class DictSubclass(dict):
...     def __repr__(self):
...         return "DictSubclass"
...
>>> dict.fromkeys("abc")
{'a': None, 'c': None, 'b': None}
>>> DictSubclass.fromkeys("abc")
DictSubclass
>>>
- runtime decorator:
#+BEGIN_SRC python
import time
def timeit(method):
    def timed(*args, **kw):
        ts = time.time()
        result = method(*args, **kw)
        te = time.time()
        if 'log_time' in kw:
            name = kw.get('log_name', method.__name__.upper())
            kw['log_time'][name] = int((te - ts) * 1000)
        else:
            print(( '%r  %2.2f ms') % \
                  (method.__name__, (te - ts) * 1000))
        return result
    return timed

@timeit
def read_sql(sql):
    df = pd.read_sql(sql, con=engine)
    return df
#+END_SRC

*** classmethod(function)
Return a class method for function.

A class method receives the class as implicit first argument, just like an instance method receives the instance. To declare a class method, use this idiom:

class C(object):
    @classmethod
    def f(cls, arg1, arg2, ...):
        ...
The @classmethod form is a function decorator – see the description of function definitions in Function definitions for details.

It can be called either on the class (such as C.f()) or on an instance (such as C().f()). The instance is ignored except for its class. If a class method is called for a derived class, the derived class object is passed as the implied first argument.

Class methods are different than C++ or Java static methods. If you want those, see staticmethod() in this section.

For more information on class methods, consult the documentation on the standard type hierarchy in The standard type hierarchy.
*** staticmethod(function)
Return a static method for function.

A static method does not receive an implicit first argument. To declare a static method, use this idiom:

class C(object):
    @staticmethod
    def f(arg1, arg2, ...):
        ...
The @staticmethod form is a function decorator – see the description of function definitions in Function definitions for details.

It can be called either on the class (such as C.f()) or on an instance (such as C().f()). The instance is ignored except for its class.

Static methods in Python are similar to those found in Java or C++. Also see classmethod() for a variant that is useful for creating alternate class constructors.

For more information on static methods, consult the documentation on the standard type hierarchy in The standard type hierarchy.

- break a function takes too long/timeout
Only for Linux:
#+BEGIN_SRC python
from functools import wraps
import errno
import os
import signal

class TimeoutError(Exception):
    pass

def timeout(seconds=10, error_message=os.strerror(errno.ETIME)):
    def decorator(func):
        def _handle_timeout(signum, frame):
            raise TimeoutError(error_message)

        def wrapper(*args, **kwargs):
            signal.signal(signal.SIGALRM, _handle_timeout)
            signal.alarm(seconds)
            try:
                result = func(*args, **kwargs)
            finally:
                signal.alarm(0)
            return result

        return wraps(func)(wrapper)

    return decorator

#+END_SRC

For Windows, because Windows doesn't support signal, timeout, .SIGNALRM function:
#+BEGIN_SRC python
from contextlib import contextmanager
import threading
import _thread

class TimeoutException(Exception):
    def __init__(self, msg=''):
        self.msg = msg

@contextmanager
def time_limit(seconds, msg=''):
    timer = threading.Timer(seconds, lambda: _thread.interrupt_main())
    timer.start()
    try:
        yield
    except KeyboardInterrupt:
        raise TimeoutException("Timed out for operation {}".format(msg))
    finally:
        # if the action ends in specified time, timer is canceled
        timer.cancel()

import time
# ends after 5 seconds
with time_limit(5, 'sleep'):
    for i in range(10):
        time.sleep(1)

# this will actually end after 10 seconds
with time_limit(5, 'sleep'):
    time.sleep(10)
#+END_SRC

** Closures
Closures are nothing but functions that are returned by another function. We use closures to remove code duplication. In the following example we create a simple closure for adding numbers.
#+BEGIN_SRC python
>>> def add_number(num):
...     def adder(number):
...         'adder is a closure'
...         return num + number
...     return adder
...
>>> a_10 = add_number(10)
>>> a_10(21)
31
>>> a_10(34)
44
>>> a_5 = add_number(5)
>>> a_5(3)
8

#+END_SRC
** iterable
An object capable of returning its members one at a time. Examples of iterables include all sequence types (such as list, str, and tuple) and some non-sequence types like dict and file and objects of any classes you define with an __iter__() or __getitem__() method. Iterables can be used in a for loop and in many other places where a sequence is needed (zip(), map(), …). When an iterable object is passed as an argument to the built-in function iter(), it returns an iterator for the object. This iterator is good for one pass over the set of values. When using iterables, it is usually not necessary to call iter() or deal with iterator objects yourself. The for statement does that automatically for you, creating a temporary unnamed variable to hold the iterator for the duration of the loop. See also iterator, sequence, and generator.

- check if an object is iterable
#+BEGIN_SRC python
>>> from collections import Iterable
>>> l = [1, 2, 3, 4]
>>> isinstance(l, Iterable)
True

#+END_SRC
** iterator
An object representing a stream of data. Repeated calls to the iterator’s next() method return successive items in the stream. When no more data are available a StopIteration exception is raised instead. At this point, the iterator object is exhausted and any further calls to its next() method just raise StopIteration again. Iterators are required to have an __iter__() method that returns the iterator object itself so every iterator is also iterable and may be used in most places where other iterables are accepted. One notable exception is code which attempts multiple iteration passes. A container object (such as a list) produces a fresh new iterator each time you pass it to the iter() function or use it in a for loop. Attempting this with an iterator will just return the same exhausted iterator object used in the previous iteration pass, making it appear like an empty container.
- skip next iteration in a for loop:
#+BEGIN_SRC python
p = iter(list)
for i in p:
    if xx:
        next(p)
        next(p)
#+END_SRC


** generator
A function which returns an iterator. It looks like a normal function except that it contains yield statements for producing a series of values usable in a for-loop or that can be retrieved one at a time with the next() function. Each yield temporarily suspends processing, remembering the location execution state (including local variables and pending try-statements). When the generator resumes, it picks-up where it left-off (in contrast to functions which start fresh on every invocation).
** generator expression
An expression that returns an iterator. It looks like a normal expression followed by a for expression defining a loop variable, range, and an optional if expression. The combined expression generates values for an enclosing function:
#+BEGIN_SRC python
>>> sum(i*i for i in range(10))         # sum of squares 0, 1, 4, ... 81
285

#+END_SRC



* Built-in Types
** Truth Value Testing
** Boolean Operations — and, or, not
- The ^ symbol
  - The ^ symbol is for the bitwise ‘xor’ operation, but in Python, the exponent operator symbol is **.
- the minimum value between nan and infinity is infinity.
min(np.nan, np.inf) = np.inf

- eval
eval the value of a variable name from string.
#+BEGIN_SRC python
text = '{'a':1}'
eval(text) # will turn text from a string object into a dictionary object
#+END_SRC

- convert string into variable:
#+BEGIN_SRC python
for ex in ['SZSE', 'SSE', 'CFFEX', 'SHFE', 'DCE', 'CZCE']:
    varname = "df_" + ex
    new_value = pro.opt_basic(exchange=ex)
    globals()[varname]=new_value
#+END_SRC
** Comparisons

- rich comparison, float compares with tuple or list:
#+BEGIN_SRC python
all(float < np.array(list))
#+END_SRC

** Numeric Types — int, float, complex
- ValueError: ("invalid literal for int() with base 10:
#+BEGIN_SRC python
>>> int('5')
5
>>> float('5.0')
5.0
>>> float('5')
5.0
>>> int(5.0)
5
>>> float(5)
5.0
>>> int('5.0')
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
ValueError: invalid literal for int() with base 10: '5.0'
>>> int(float('5.0'))
5
#+END_SRC
** Iterator Types
- xrange vs range:
there's no xrange in python 3.
- Finding the index of an item given a list:
#+BEGIN_SRC python
>>> ["foo", "bar", "baz"].index("bar")
1
#+END_SRC
- access index and value looping a list:
#+BEGIN_SRC python
for idx, val in enumerate(list):
    print(idx, val)

#+END_SRC
- iterable vs iterator vs generator:
The difference between iterables and generators: once you’ve burned through a generator once, you’re done, no more data.
#+BEGIN_SRC python
generator = (word + '!' for word in 'baby let me iterate ya'.split())
# The generator object is now created, ready to be iterated over.
# No exclamation marks added yet at this point.

for val in generator: # real processing happens here, during iteration
    print val,
baby! let! me! iterate! ya!

for val in generator:
    print val,
# Nothing printed! No more data, generator stream already exhausted above.
#+END_SRC
an iterable creates a new iterator every time it’s looped over (technically, every time iterable.__iter__() is called, such as when Python hits a “for” loop):
#+BEGIN_SRC python
class BeyonceIterable(object):
    def __iter__(self):
        """
        The iterable interface: return an iterator from __iter__().

        Every generator is an iterator implicitly (but not vice versa!),
        so implementing `__iter__` as a generator is the easiest way
        to create streamed iterables.

        """
        for word in 'baby let me iterate ya'.split():
            yield word + '!'  # uses yield => __iter__ is a generator

iterable = BeyonceIterable()

for val in iterable:  # iterator created here
    print val,
baby! let! me! iterate! ya!

for val in iterable:  # another iterator created here
    print val,
baby! let! me! iterate! ya!
#+END_SRC

- magic method __iter__:
Iterators are everywhere in Python. They are elegantly implemented within for loops, comprehensions, generators etc. but hidden in plain sight.

Iterator in Python is simply an object that can be iterated upon. An object which will return data, one element at a time.

Technically speaking, Python iterator object must implement two special methods, __iter__() and __next__(), collectively called the iterator protocol.

An object is called iterable if we can get an iterator from it. Most of built-in containers in Python like: list, tuple, string etc. are iterables.

The iter() function (which in turn calls the __iter__() method) returns an iterator from them.

- Iterating Through an Iterator in Python
use the $next()$ function to manually iterate through all the items of an iterator.
#+BEGIN_SRC python
# define a list
my_list = [4, 7, 0, 3]

# get an iterator using iter()
my_iter = iter(my_list)

## iterate through it using next()

#prints 4
print(next(my_iter))

#prints 7
print(next(my_iter))

## next(obj) is same as obj.__next__()

#prints 0
print(my_iter.__next__())

#prints 3
print(my_iter.__next__())

## This will raise error, no items left
next(my_iter)
#+END_SRC
A more elegant way of automatically iterating is by using the for loop. Using this, we can iterate over any object that can return an iterator, for example list, string, file etc.
#+BEGIN_SRC python
for element in my_list:
    print(element)
#+END_SRC
- How for loop actually works?
#+BEGIN_SRC python
for element in iterable:
    # do something with element
# Is actually implemented as.

# create an iterator object from that iterable
iter_obj = iter(iterable)

# infinite loop
while True:
    try:
        # get the next item
        element = next(iter_obj)
        # do something with element
    except StopIteration:
        # if StopIteration is raised, break from loop
        break

#+END_SRC
- example:
#+BEGIN_SRC python
class PowTwo:
    """Class to implement an iterator
    of powers of two"""

    def __init__(self, max = 0):
        self.max = max

    def __iter__(self):
        self.n = 0
        return self

    def __next__(self):
        if self.n <= self.max:
            result = 2 ** self.n
            self.n += 1
            return result
        else:
            raise StopIteration

# create an iterator and iterate through it as follows.

>>> a = PowTwo(4)
>>> i = iter(a)
>>> next(i)
1
>>> next(i)
2
>>> next(i)
4
>>> next(i)
8
>>> next(i)
16
>>> next(i)
Traceback (most recent call last):
...
StopIteration

# use a for loop to iterate over our iterator class.

>>> for i in PowTwo(5):
...     print(i)
#+END_SRC
** Sequence Types — list, tuple, range
- list boolean indexing:
#+BEGIN_SRC python
filtered_list = [i for (i, v) in zip(list_a, filter) if v]
#+END_SRC

- float range step:
#+BEGIN_SRC python
for i in numpy.arange(0, 1, 0.1):
    pass
#+END_SRC

- reverse order float range:
#+BEGIN_SRC python
def frange(start, stop=None, step=None):
    # if stop and step argument is None set start=0.0 and step = 1.0
    start = float(start)
    if stop == None:
        stop = start + 0.0
        start = 0.0
    if step == None:
        step = 1.0

    print("start= ", start, "stop= ", stop, "step= ", step)

    count = 0
    while True:
        temp = float(start + count * step)
        if step > 0 and temp >= stop:
            break
        elif step < 0 and temp <= stop:
            break
        yield temp
        count += 1

#+END_SRC
- create a0 to a9:
#+BEGIN_SRC python
import itertools
infectious_big_list = [['A{}'.format(i) for i in range(10, 100)], ['A0{}'.format(i) for i in range(0, 10)], ['B{}'.format(i) for i in range(10, 100)], ['B0{}'.format(i) for i in range(0, 10)]]
infectious_disease_cd = list(itertools.chain.from_iterable(infectious_big_list))
mapping_illustrated_disease = {x:True for x in infectious_disease_cd}
#+END_SRC

- multiply a list with another list:
#+BEGIN_SRC python
def multiply_strings(x, col=('med_code', 'med_usage')):
    # print(type(x))
    ls_x = x[col[0]].split(',')
    ls_y = x[col[1]].split(',')
    # assert len(ls_x) == len(ls_y)
    if len(ls_x) != len(ls_y):
        return x[col[0]]
    ls_new = []
    for i in range(len(ls_x)):
        # remove stop words
        if int(ls_y[i]) < 50:
            ls_new.append((ls_x[i]+',')*int(ls_y[i]))
        else:
            ls_new.append((ls_x[i]+','))
    return ''.join(ls_new)
df.apply(lambda x:multiply_strings(x), axis=1)
#+END_SRC

- find all occurrences of a substring
#+BEGIN_SRC python
import re
[m.start() for m in re.finditer('test', 'test test test test')]

#+END_SRC
- find position of sub list in a list
#+BEGIN_SRC python
greeting = ['hello','my','name','is','bob','how','are','you','my','name','is']

def find_sub_list(sl,l):
    sll=len(sl)
    for ind in (i for i,e in enumerate(l) if e==sl[0]):
        if l[ind:ind+sll]==sl:
            return ind,ind+sll-1

print find_sub_list(['my','name','is'], greeting)
#+END_SRC
- is list equal:
#+BEGIN_SRC python
a = [1,2,3]
b = [3,2,1]
a.sort()
b.sort()
a == b
#+END_SRC
- Unique value from list of lists:
#+BEGIN_SRC python
testdata = [list(x) for x in set(tuple(x) for x in testdata)]

#+END_SRC

- nested list comprehension:
#+BEGIN_SRC python
[x+y for x in [1,2,3] for y in [4,5,6]]
# equal to
res =[]
for x in [1,2,3]:
    for y in [4,5,6]:
        res.append(x+y)
[y+1 for x in [[1,2],[2,2],[3,2]] for y in x]

# equal to
res =[]
for x in [[1,2],[2,2],[3,2]]:
    for y in x:
        res.append(1+y)
#+END_SRC
- remove value in a list:
#+BEGIN_SRC python
list.remove('value')
#+END_SRC
- tuple vs set
A set is a slightly different concept from a list or a tuple. A set, in Python, is just like the mathematical set. It does not hold duplicate values, and is unordered. However, it is not immutable unlike a tuple.Jan 9, 2018

- combine list of lists into one list, join list of lists
#+BEGIN_SRC python
import itertools
a = [["a","b"], ["c"]]
print list(itertools.chain.from_iterable(a))
# or
lambda: (lambda b: map(b.extend, big_list))([])
#+END_SRC
- find difference of two lists:
#+begin_src python
a = [1,2,3,2,1,5,6,5,5,5]
import collections
print [item for item, count in collections.Counter(a).items() if count > 1]
#+end_src

- 列表生成式list comprehension
#+begin_src python
[a.lower() for a in x=['Hello', 'World', 18, 'Apple', None] if isinstance(a,str)]
#+end_src

- read file to a list:
#+begin_src python
with open(r'y:\codes\data\smart_beta_etf_list.txt', 'rb') as f:
etf_list = f.readlines()
etf_list = [x.strip() for x in etf_list]
# you may also want to remove whitespace characters like `\n` at the end of each line
#+end_src

- save a list to a file:
#+begin_src python
thefile = open('test.txt', 'w')
import pickle

with open('outfile', 'wb') as fp:
    pickle.dump(itemlist, fp)
# To read it back:

with open ('outfile', 'rb') as fp:
    itemlist = pickle.load(fp)
#+end_src
- save a list of Chinese string to a file:
#+BEGIN_SRC python
values = [u'股市']
import codecs
with codecs.open("file.txt", "w", encoding="utf-8") as d:
    d.write(str(x0))
#+END_SRC
- for item in the list:
#+begin_src python
thefile.write("%s\n" % item)
#+end_src

- replace comma as next line (enter):
choose extend mode: replace ',' as \r\n

- split strings by space delimiter from reverse:
#+begin_src python
text.rsplit(' ', 1)[0]
#+end_src

- split strings by space delimiter from beginning:
#+begin_src python
text.split(' ', 1)[0]
>>>a.split('.',1)
['alvy','test.txt']
后面多了一个参数1，以第一个'.'分界，分成两个字符串，组成一个list
>>>a.rsplit('.',1)
['alvy.test','txt']
现在是rsplit函数，从右边第一个'.'分界，分成两个字符串，组成一个list
#+end_src
- split by comma:
#+BEGIN_SRC python
string.split(",")
#+END_SRC
- split by multiple delimiter:
#+BEGIN_SRC python
import re
re.split('; |, |\*|\n',str)
#+END_SRC
** 生成器generator
通过列表生成式，我们可以直接创建一个列表。但是，受到内存限制，列表容量肯定是有限的。
而且，创建一个包含100万个元素的列表，不仅占用很大的存储空间，如果我们仅仅需要访问前面几个元素，那后面绝大多数元素占用的空间都白白浪费了。
要创建一个generator，有很多种方法。第一种方法很简单，只要把一个列表生成式的[]改成()，就创建了一个generator：
如果要一个一个打印出来，可以通过next()函数获得generator的下一个返回值：
next(g)
这里，最难理解的就是generator和函数的执行流程不一样。函数是顺序执行，遇到return语句或者最后一行函数语句就返回。
而变成generator的函数，在每次调用next()的时候执行，遇到yield语句返回，再次执行时从上次返回的yield语句处继续执行。

#+begin_src python
def odd():
    print('step 1')

    yield 1
    print('step 2')
    yield(3)
    print('step 3')
    yield(5)
>>> o = odd()
>>> next(o)
step 1
1
>>> next(o)
step 2
3
>>> next(o)
step 3
5
>>> next(o)
Traceback (most recent call last):

  File "<stdin>", line 1, in <module>
StopIteration
#+end_src
A Generator is an Iterator

A function with yield in it is still a function, that, when called, returns an instance of a generator object:
#+BEGIN_SRC python
def a_function():
    "when called, returns generator object"
    yield

#+END_SRC
A generator expression also *returns a generator*:
#+BEGIN_SRC python
a_generator = (i for i in range(0))

#+END_SRC

A Generator is an Iterator

An Iterator is an Iterable

Iterators require a next or __next__ method

*** loop
- loop with batches:
#+BEGIN_SRC python
for i in tqdm(range(0, len(category), batch_size)):
    re_batch = {}
    for j in range(batch_size):
        re_batch[j] = wiki_category_re.search(category, last_span)
        if re_batch[j] is not None:
            last_span = re_batch[j].span()[1]
    upload_cat_node(re_batch)
#+END_SRC
- don't care the interator sequence:
#+BEGIN_SRC python
for _ in range(10):
    print(_)
#+END_SRC

- fetch several pairs from a dictionary:
#+BEGIN_SRC python
from itertools import islice

def take(n, iterable):
    "Return first n items of the iterable as a list"
    return list(islice(iterable, n))

n_items = take(3, dict_df_ret.items())
n_items

# or
list(islice(dictionary.items(), 3))
#+END_SRC

- iterate key and value in a dictionary:
#+begin_src python
# python 2
for index, value in dict.iteritems():
# python 3
for index, value in dict.items():
    print index, value
#+end_src
- iterate keys in a dictionary:
#+begin_src python
for k in dict:
#+end_src

- iterate a row in pandas dataframe:
#+begin_src python
DataFrame.iterrows():
return generator.
>>> df = pd.DataFrame([[1, 1.5]], columns=['int', 'float'])
>>> row = next(df.iterrows())[1]
>>> row
int      1.0
float    1.5
Name: 0, dtype: float64
>>> print(row['int'].dtype)
float64
>>> print(df['int'].dtype)
int64
#+end_src

- To preserve dtypes while iterating over the rows, it is better to use itertuples()
  - which returns tuples of the values and which is generally faster as iterrows.
** Text Sequence Type — str
** Binary Sequence Types — bytes, bytearray, memoryview
- convert bytes into string:
#+BEGIN_SRC python
b"abcde".decode("utf-8")
#+END_SRC
** Set Types — set, frozenset

- turn list into set keep sequence/order of list
#+BEGIN_SRC python
>>> from  more_itertools import unique_everseen
>>> items = [1, 2, 0, 1, 3, 2]
>>> list(unique_everseen(items))
[1, 2, 0, 3]

#+END_SRC

- union all:
#+BEGIN_SRC python
from functools import reduce
def join_ratio(data):
    #newdata = data.apply(set)
    join = reduce(lambda x, y: x & y , [x for x in data])

    union = reduce(lambda x, y: x | y , [x for x in data])
    ratio = len(join) / len(union)
    return  join, ratio
#+END_SRC

- access an element in a set.
#+BEGIN_SRC python
a=set([1,2,3])
element = a.pop(0)

list(a)[0]
#+END_SRC

#+BEGIN_SRC python
from random import sample

def ForLoop(s):
    for e in s:
        break
    return e

def IterNext(s):
    return next(iter(s))

def ListIndex(s):
    return list(s)[0]

def PopAdd(s):
    e = s.pop()
    s.add(e)
    return e

def RandomSample(s):
    return sample(s, 1)

def SetUnpacking(s):
    e, *_ = s
    return e

from simple_benchmark import benchmark

b = benchmark([ForLoop, IterNext, ListIndex, PopAdd, RandomSample, SetUnpacking],
              {2**i: set(range(2**i)) for i in range(1, 20)},
              argument_name='set size',
              function_aliases={first: 'First'})

b.plot()
#+END_SRC
** Mapping Types — dict
- concat two dictionary:
#+BEGIN_SRC python
dict1.update(dict2)
#+END_SRC

- rename key:
#+BEGIN_SRC python
dict['new'] = dict.pop('old')
#+END_SRC

- dump dictionary into pickle:
#+BEGIN_SRC python
with open('./data/disease.pkl', 'wb') as f:
    pickle.dump(dict_intravenous_thrombolysis, f)
#+END_SRC

- load dictionary pickle:
#+BEGIN_SRC python
pickle.load(open("save.p", "rb" ))
#+END_SRC

- dump dictionary into json:
#+BEGIN_SRC python
import json
with open('multiple_paths.json', 'w', encoding='utf-8') as fp:
    js_obj = json.dumps(filtered_dict)
    fp.write(js_obj)
#+END_SRC
- get key from value:
#+BEGIN_SRC python
for name, age in word2id.items():    # for name, age in list.items():  (for Python 3.x)
    if age == 16116:
        print(name)

# or
mydict = {'george':16,'amber':19}
print(list(mydict.keys())[list(mydict.values()).index(16)]) # Prints george

print(list(word2id.keys())[list(word2id.values()).index(16116)]) # Prints
#+END_SRC
- get some keys value according to a list in a dictionary:
#+BEGIN_SRC python
value = {}
for key in finance_vocab:
    value[key] = dict_vocab.get(key)

#+END_SRC
- filter dictionary by value:
#+BEGIN_SRC python
filtered_dict = {k:v for k,v in dict.items() if v<0}
#+END_SRC
- set all values in a dict:
#+BEGIN_SRC python
visited = dict.fromkeys(self.graph, False)
#+END_SRC
- check if a value is in a dict:
#+BEGIN_SRC python
'红鲱鱼招股书' in g.graph.values()
#+END_SRC
- check if a value is in a defaultdict collection list:
#+BEGIN_SRC python
any('波动性' in v for v in g.graph.values())
# or
def in_values(s, d):
    """Does `s` appear in any of the values in `d`?"""
    for v in d.values():
        if s in v:
            return True
    return False

in_values('cow', animals)
#+END_SRC

- get location:
#+BEGIN_SRC python
import json
from urllib.request import urlopen, quote
import requests
def getlnglat(address):
    uachoice = random.choice(ua)
    headers["User-Agent"] = uachoice

    url = 'http://api.map.baidu.com/geocoding/v3/'
    output = 'json'
    ak =  # 百度地图ak，具体申请自行百度，提醒需要在“控制台”-“设置”-“启动服务”-“正逆地理编码”，启动
    address = quote(address) # 由于本文地址变量为中文，为防止乱码，先用quote进行编码
    uri = url + '?' + 'address=' + address  + '&output=' + output + '&ak=' + ak
    response = requests.get(uri, headers=headers, timeout=20) #使用有代理参数的请求
#     if response.status_code == 200:
#         print("请求成功！")
#     req = requests.get(url=uri)
    res = response.content.decode()
    temp = json.loads(res)
    lat = temp['result']['location']['lat']
    lng = temp['result']['location']['lng']
    level = temp['result']['level']
    return lat, lng, level   # 纬度 latitude   ，   经度 longitude  ，
#+END_SRC

- merge/combine two dictionaries:
#+BEGIN_SRC python
d1 = {1:1}
d2 = {2:2}
d1.update(d2)
#+END_SRC

- sort a dict by its value:
#+BEGIN_SRC python
s = [(k, d[k]) for k in sorted(d, key=d.get, reverse=True)]
#+END_SRC
- count key values in a dict:
#+BEGIN_SRC python
d = defaultdict(list)
for name in g.graph.keys():
	key = len(g.graph[name])
	d[name] = key
#+END_SRC
- convert a list of tuples with the same key into a dictionary:
#+BEGIN_SRC python
from collections import defaultdict
d = defaultdict(list)
for k, v in list(graph.out_edges('财务管理')):
    d[k].append(v)
# or
d = {}
for k, v in list(graph.out_edges('财务管理')):
    d.setdefault(k,[]).append(v)
#+END_SRC

- multiply diction by another dictionary:
#+BEGIN_SRC python
from copy import copy
my_dict = copy(another_dict)
my_dict.update((x, y*2) for x, y in my_dict.items())
#+END_SRC
- add value in a dictionary:
#+BEGIN_SRC python
In [231]: d
Out[231]:
defaultdict(list,
            {'上海证券交易所上市公司': 383,
             '各证券交易所上市公司': 37,
             '深圳证券交易所上市公司': 511,
             '证券': 64,
             '证券交易所': 8})

sum(d.values())
#+END_SRC

- write defaultdict to a json file:
#+BEGIN_SRC python
import json
# writing
json.dump(yourdict, open(filename, 'w'))
# reading
yourdict = json.load(open(filename))
#+END_SRC
** Context Manager Types
** Other Built-in Types
** Special Attributes -- magic method:
- getitem in a class allows its instances to use the [ ] (indexer) operators
- setitem Called to implement assignment to self[key]
- call magic method in a class causes its instances to become callables – in other words, those instances now behave like functions.
- getattr overrides Python’s default mechanism for member access.
- getattr magic method only gets invoked for attributes that are not in the dict magic attribute. Implementing getattr causes the hasattr built-in function to always return True, unless an exception is raised from within getattr.
- setattr allows you to override Python’s default mechanism for member assignment.
- The repr function also converts an object to a string. It can also be invoked using the reverse quotes (`), also called accent grave, (underneath the tilde, ~, on most keyboards). But it will convert unambitiously the object. For example, repr(datetime.datetime.now) = datetime.datetime(2018, 1, 20, 13, 32, 51, 483232).
- __str__
get string of elements inside.
#+begin_src python :tangle yes
print `a`
print repr(a)
#+end_src

- find out where module is installed
#+BEGIN_SRC python
import os
import spacy
print(os.path.dirname(spacy.__file__))
#+END_SRC

* Built-in Exceptions
** Base classes

- inherit from base class super:
#+BEGIN_SRC python
class Rectangle:
    def __init__(self, length, width):
        self.length = length
        self.width = width

    def area(self):
        return self.length * self.width

    def perimeter(self):
        return 2 * self.length + 2 * self.width

class Square(Rectangle):
    def __init__(self, length):
        super().__init__(length, length)

#+END_SRC

** Concrete exceptions
** Warnings
** Exception hierarchy
** exception
- oracle cx error:
#+BEGIN_SRC python
try:
    xx
except cx.DatabaseError:
    continue
#+END_SRC
- retry:
#+BEGIN_SRC python
response = None
error = None
while response is None:
  try:
    response = doing_something()
    if response is not None:
      if 'good' in response:
        print("successfully uploaded")
      else:
        exit("reason %s"%response)
  except HttpError as e:
    if e.code in RETRIABLE_STATUS_CODES:
      error = 'A retriable HTTP error %d occurred:\n%s' % (e.resp.status,
                                                             e.content)
    else:
      raise
  except RETRIABLE_EXCEPTIONS as e:
    error = 'A retriable error occurred: %s' % e

    if error is not None:
      print error
      retry += 1
      if retry > MAX_RETRIES:
        exit('No longer attempting to retry.')

      max_sleep = 2 ** retry
      sleep_seconds = random.random() * max_sleep
      print 'Sleeping %f seconds and then retrying...' % sleep_seconds
      time.sleep(sleep_seconds)
#+END_SRC
- capture urllib error:
#+BEGIN_SRC python
import urllib2

req = urllib2.Request('http://www.python.org/fish.html')
try:
    resp = urllib2.urlopen(req)
except urllib2.HTTPError as e:
    if e.code == 404:
        # do something...
    else:
        # ...
except urllib2.URLError as e:
    # Not an HTTP-specific error (e.g. connection refused)
    # ...
else:
    # 200
    body = resp.read()

#+END_SRC
- create an exception:
#+BEGIN_SRC python
class ConstraintError(Exception):
    def __init__(self, arg):
        self.args = arg


if error:
    raise ConstraintError("error")


class Networkerror(RuntimeError):
    def __init__(self, arg):
        self.args = arg


try:
    raise Networkerror("Bad hostname")
except Networkerror,e:
    print e.args
    print e.message
#+END_SRC
- clean-up actions
#+BEGIN_SRC python
>>> def divide(x, y):
...     try:
...         result = x / y
...     except ZeroDivisionError:
...         print "division by zero!"
...     else:
...         print "result is", result
...     finally:
...         print "executing finally clause"
...
>>> divide(2, 1)
result is 2
executing finally clause
>>> divide(2, 0)
division by zero!
executing finally clause
>>> divide("2", "1")
executing finally clause
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "<stdin>", line 3, in divide
TypeError: unsupported operand type(s) for /: 'str' and 'str'

#+END_SRC
* Text Processing Services
** string — Common string operations
- check if string is Chinese:
#+BEGIN_SRC python
from googletrans import Translator
translator = Translator(proxies ={
             'http': 'http://192.168.1.126:1080',
             'https': 'http://192.168.1.126:1080'
         }
)
input_text_language_0 = translator.detect(input_text_0).lang
#+END_SRC

- find if strings are almost equal:
#+BEGIN_SRC python
from difflib import SequenceMatcher
s_1 = 'Mohan Mehta'
s_2 = 'Mohan Mehte'
print(SequenceMatcher(a=s_1,b=s_2).ratio())
0.909090909091
#+END_SRC
- check if string is empty:
#+BEGIN_SRC python
if not text:
  print('text is empty')
#+END_SRC
- if the string is an English word.
#+BEGIN_SRC python
from nltk.corpus import wordnet
if not wordnet.synsets(word) and not word.isdigit()
#+END_SRC

- jieba cut, remove signs.
#+BEGIN_SRC python
punct = set(u''':!),.:;?]}¢'"、。〉》」』】〕〗〞︰︱︳﹐､﹒
﹔﹕﹖﹗﹚﹜﹞！），．：；？｜｝︴︶︸︺︼︾﹀﹂﹄﹏､～￠
々‖•·ˇˉ―--′’”([{£¥'"‵〈《「『【〔〖（［｛￡￥〝︵︷︹︻
︽︿﹁﹃﹙﹛﹝（｛“‘-—_…''')

str_in = u"小明硕士毕业于中国科学院计算所，\
后在日本京都大学深造，凭借过人天赋，旁人若在另一方面爱他，他每即躲开。"

# 对str/unicode
filterpunt = lambda s: ''.join(filter(lambda x: x not in punct, s))
# 对list
filterpuntl = lambda l: list(filter(lambda x: x not in punct, l))
seg_list = jieba.cut(str_in, cut_all=False)
sent_list = filterpuntl(seg_list)
#+END_SRC
- jieba cut on bash:
#+BEGIN_SRC bash
python -m jieba news.txt > cut_result.txt
#+END_SRC

- create a list from jieba generator:
   sentence = [x for x in seg_list]

- manually download nltk tokenizer:
unzip downloaded file [[http://www.nltk.org/nltk_data/][nltk_data]] to /usr/local/share/nltk_data/tokenizers

- tokenize unicode or string to sentence list.
#+BEGIN_SRC python
from nltk import tokenize as n_tokenize
sent= n_tokenize.sent_tokenize(page)
# or
sent_list = page.split()
#+END_SRC
- list comprehension
#+BEGIN_SRC python
[x for x in t if x not in s if x.isdigit()]
l = [22, 13, 45, 50, 98, 69, 43, 44, 1]
[True if x >= 45 else False for x in l]
#+END_SRC
- if string are digits.
#+BEGIN_SRC python
str.isdigit()
#+END_SRC
- concatenate two strings
#+BEGIN_SRC python
" ".join((str1, str2))
#+END_SRC
- 移除字符串头尾指定的字符（默认为空格）
#+BEGIN_SRC python
#!/usr/bin/python

str = "0000000this is string example....wow!!!0000000";
print(str.strip( '0' ))
#+END_SRC

- checks whether the string consists of alphabetic characters only.
#+BEGIN_SRC python 2
#!/usr/bin/python

str = "this";  # No space & digit in this string
print(str.isalpha())

str = "this is string example....wow!!!";
print(str.isalpha())
#+END_SRC

#+RESULTS:
: True
: False

** re — Regular expression operations
*** useage:
- find strings
- convert strings
- convert syntax from python2 to python3
#+BEGIN_SRC txt
regular expression: find print(\S*), replace with print(\1)
#+END_SRC
*** types:
In : rex_property.search?
Signature: rex_property.search(string=None, pos=0, endpos=9223372036854775807, *, pattern=None)
Docstring:
Scan through string looking for a match, and return a corresponding match object instance.
This object has start, end, group, groups, span.

Return None if no position in the string matches.
Type:      builtin_function_or_method

In : rex_property.findall?
Signature: rex_property.findall(string=None, pos=0, endpos=9223372036854775807, *, source=None)
Docstring: *Return a list* of all non-overlapping matches of pattern in string.
Type:      builtin_function_or_method

In : rex_property.match?
Signature: rex_property.match(string=None, pos=0, endpos=9223372036854775807, *, pattern=None)
Docstring: Matches zero or more characters at the beginning of the string.
Type:      builtin_function_or_method

*** string array
[Pp]ython: find Python or python

**** parts
re.search('[a-zA-Z0-9]', 'x')

**** not
re.search('[^0-9]', 'x')

**** shortcut

- word: \w
- number: \d
- space, tab, next line: \s
- 0 length sub string: \b
re.search('\bcorn\b', 'corner')

**** start and end with strings
#+BEGIN_SRC python
re.search('^Python', 'Python 3')
re.search('Python$', 'this is Python')
#+END_SRC

**** any character
"."

**** all lines contain a specific string
#+BEGIN_SRC python
^.*Deeplearning4j$
#+END_SRC
*** optional words
'color' vs 'colour'
re.search('colou?r', 'my favoriate color')

*** repeat
{N}

#+BEGIN_SRC python
# find a telephone number
re.search(r'[\d]{3}-[\d]{4}', '867-5309 /Jenny')

# find 32big GID
[x for x in risk_model_merge.keys() if re.match("[A-Z0-9]{32}$", x)]
#+END_SRC

**** boundary of repeated times
[\d]{3,4}

**** open selection
[\d]{3,}

**** speed selection
- +: {1,}
- *: {0,}

*** search for a pattern within a text file
- bulk read:
#+BEGIN_SRC python
import re

textfile = open(filename, 'r')
filetext = textfile.read()
textfile.close()
matches = re.findall("(<(\d{4,5})>)?", filetext)

#+END_SRC

- replace cell string in py file downloaded from jupyter notebook:
#+BEGIN_SRC python
# In\[[\d]+\]:
#+END_SRC

- re replace remove digits from end of a string:
#+BEGIN_SRC python
import re

cleaned = re.sub(r'\d+$', '', yourstring)
#+END_SRC

- read line by line:
#+BEGIN_SRC python
import re

textfile = open(filename, 'r')
matches = []
reg = re.compile("(<(\d{4,5})>)?")
for line in textfile:
    matches += reg.findall(line)
textfile.close()
#+END_SRC
- replace string with a dictionary in a sentence:
#+BEGIN_SRC python

d ={"C":"-", "ABC":"-", "CSAMPLEABC":"-:)-"}

def find_replace_multi_ordered(string, dictionary):
    # sort keys by length, in reverse order
    for item in sorted(dictionary.keys(), key = len, reverse = True):
        string = re.sub(item, dictionary[item], string)
    return string

find_replace_multi_ordered(s, d)
#+END_SRC

- search for Chinese:
#+BEGIN_SRC python
zhmodel = re.compile(u'[\u4e00-\u9fa5]')
#+END_SRC

- if a column contains Chinese:
#+BEGIN_SRC python
for i in df.columns:
    if i in ['股票简称', 'code', '股票代码']:
        continue
    elif df[i].astype(str).str.contains(u'[\u4e00-\u9fa5]').any():
        continue
    else:
        df[i] = df[i].astype(float)
#+END_SRC

** difflib — Helpers for computing deltas
** textwrap — Text wrapping and filling
** unicodedata — Unicode Database
https://docs.python.org/2.7/howto/unicode.html
** stringprep — Internet String Preparation
** readline — GNU readline interface
- read certain line from a file:
#+BEGIN_SRC python
import linecache
linecache.getline('Sample.txt', Number_of_Line)

#+END_SRC
* Data Types
** datetime — Basic date and time types
- Converting unix timestamp string to readable date in Python
#+BEGIN_SRC python
import datetime
print(
    datetime.datetime.fromtimestamp(
        int("1284101485")
    ).strftime('%Y-%m-%d %H:%M:%S')
)

- create a datetime
datetime.datetime.strptime('2000-01-01', '%Y-%m-%d')
pd.datetime.now().date()
#+END_SRC
** Time
- convert datetime into year month:
#+BEGIN_SRC python
df['month_year'] = pd.to_datetime(df['birth_date']).dt.to_period('M')
#+END_SRC

- create timestamp:
#+BEGIN_SRC python
>>> pd.to_datetime('13000101', format='%Y%m%d', errors='ignore')
datetime.datetime(1300, 1, 1, 0, 0)
#+END_SRC

- int datetime into epoch timestamp
#+BEGIN_SRC python
lambda x: int(time.mktime(datetime.datetime.strptime(str(x), '%Y%m%d').timetuple()))
#+END_SRC

- remove timezone
#+BEGIN_SRC python
# check if column type is datetime64
from pandas.api.types import is_datetime64_any_dtype as is_datetime
if is_datetime(df['IN_HOSP_DATE'].dtype):
    df['IN_HOSP_DATE'] = df['IN_HOSP_DATE'].dt.tz_localize(None)
    df['OUT_HOSP_DATE'] = df['OUT_HOSP_DATE'].dt.tz_localize(None)
#+END_SRC

- get specific timezone datetime
#+begin_src python
tz = pytz.timezone('America/Los_Angeles')
#date = date.today()
now = datetime.now()
los_angeles_time = datetime.now(tz)
#+end_src

- use tqdm as a status bar:
#+begin_src python
from tqdm import tqdm
# without starting a new line
# from tqdm import notebook.tqdm as tqdm
from time import sleep
for i in tqdm(range(10)):
    sleep(0.1)

# enumerate
for i in enumerate(tqdm(list)):
    do things()
# pandas
df = pd.DataFrame(np.random.randint(0, int(1e8), (10000, 1000)))

# Create and register a new `tqdm` instance with `pandas`
# (can use tqdm_gui, optional kwargs, etc.)
tqdm.pandas()

# Now you can use `progress_apply` instead of `apply`
df.groupby(0).progress_apply(lambda x: x**2)

#+end_src
- replace space in dataframe
#+BEGIN_SRC python
df.replace('\s+', '_',regex=True,inplace=True)
#+END_SRC

- a column of list of strings into a list of float :
#+BEGIN_SRC python
df_patient['item_cost'] = df_patient['item_cost'].apply(lambda x : pd.to_numeric(x))
#+END_SRC

- remove all digit strings:
#+BEGIN_SRC python
df_cq['name'] = df_cq['soc_srt_dire_nm'].str.replace('\d+', '',regex=True)
ls_cq = df_cq[df_cq['name'].apply(len)>0]['soc_srt_dire_nm'].tolist()
#+END_SRC

- parse all digit from string column:
#+BEGIN_SRC python
df_stock_daily['rank'].str.extract('(\d+)').astype(int)
#+END_SRC

- string to datetime:
#+begin_src python
time.strptime(string[, format])
#+end_src

- datetime, Timestamp, datetime64
pandas, Timestamp
np.dtype('<M8[ns]')
timestamp has timezone information.

-- DatetimeIndex is composed by Timestamps.
#+BEGIN_SRC python
#Timestamp to string:i
str_timestamp = pd.to_datetime(Timestamp, format = '%Y%m%d')
str_timestamp = str_timestamp.strftime('%Y-%m-%d')
#+END_SRC
datetime, utc
datetime64
- get the location of a date in datetimeindex:
#+BEGIN_SRC python
pd.DatetimeIndex.get_loc(datetime)
#+END_SRC
- datetime off set, subtract
#+BEGIN_SRC python
TimeStamp +/- pd.DateOffset(years=1)
pd.Timedelta(days=365) #allowed keywords are [weeks, days, hours, minutes, seconds, milliseconds, microseconds, nanoseconds]
#+END_SRC

- sum of timedelta:
#+BEGIN_SRC python
np.sum(timedelta)
#+END_SRC

- timedelta to hours:
#+BEGIN_SRC python
td / np.timedelta64(1, 'h')
#+END_SRC

- convert into 亿:
#+BEGIN_SRC python
0!.00,,"亿"
#+END_SRC

- change the column order to the first column:
#+BEGIN_SRC python
# shift column 'Name' to first position
first_column = df.pop('股票简称')
  
# insert column using insert(position,column_name,
# first_column) function
df.insert(0, '股票简称', first_column)
#+END_SRC

- pandas date range selection:
#+BEGIN_SRC python
start = ls_dates[d]
start = pd.to_datetime(start)
period = start + pd.DateOffset(30)
print(start, period)
df_range = df_simul[(df_simul.index < period) & (df_simul.index > start)]
#+END_SRC
** calendar — General calendar-related functions
** collections — Container datatypes
** collections — High-performance container datatypes

| module      | function                                                             |
|-------------+----------------------------------------------------------------------|
| deque       | list-like container with fast appends and pops on either end         |
| Counter     | dict subclass for counting hashable objects                          |
| defaultdict | dict subclass that calls a factory function to supply missing values |

** collections.abc — Abstract Base Classes for Containers
** heapq — Heap queue algorithm
** bisect — Array bisection algorithm
** array — Efficient arrays of numeric values
** weakref — Weak references
** types — Dynamic type creation and names for built-in types
** copy — Shallow and deep copy operations
#+BEGIN_SRC python
from copy import copy
#+END_SRC
** pprint — Data pretty printer
适合打印列表。
#+BEGIN_SRC python
>>> import pprint
>>> tup = ('spam', ('eggs', ('lumberjack', ('knights', ('ni', ('dead',
... ('parrot', ('fresh fruit',))))))))
>>> stuff = ['a' * 10, tup, ['a' * 30, 'b' * 30], ['c' * 20, 'd' * 20]]
>>> pprint.pprint(stuff)
['aaaaaaaaaa',
 ('spam',
  ('eggs',
   ('lumberjack',
    ('knights', ('ni', ('dead', ('parrot', ('fresh fruit',)))))))),
 ['aaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', 'bbbbbbbbbbbbbbbbbbbbbbbbbbbbbb'],
 ['cccccccccccccccccccc', 'dddddddddddddddddddd']]
#+END_SRC
** reprlib — Alternate repr() implementation
** enum — Support for enumerations
* Numeric and Mathematical Modules
** numbers — Numeric abstract base classes
** math — Mathematical functions
** cmath — Mathematical functions for complex numbers
** decimal — Decimal fixed point and floating point arithmetic
Floating-point numbers are represented in computer hardware as base 2 (binary) fractions. For example, the decimal fraction 0.001 has value 0/2 + 0/4 + 1/8.
On a typical machine running Python, there are 53 bits of precision available for a Python float, so the value stored internally when you enter the decimal number 0.1 is the binary fraction.
#+begin_src emacs-lisp :tangle yes
0.00011001100110011001100110011001100110011001100110011010
#+end_src
#+begin_src emacs-lisp :tangle yes
>>> round(2.675, 2)
2.67
#+end_src
it’s again replaced with a binary approximation, whose exact value is

2.67499999999999982236431605997495353221893310546875

- precision, scientific number:
#+BEGIN_SRC python
import numpy as np
np.set_printoptions(suppress=True)
%precision %.4g
pd.set_option('display.float_format', lambda x: '%.4f' % x)
#+END_SRC

** fractions — Rational numbers
** random — Generate pseudo-random numbers
** statistics — Mathematical statistics functions
* Functional Programming Modules
** itertools — Functions creating iterators for efficient looping
** functools — Higher-order functions and operations on callable objects
** operator — Standard operators as functions
* File and Directory Access
** pathlib — Object-oriented filesystem paths
- basics:
#+BEGIN_SRC python
from pathlib import Path
p = Path('.')
#+END_SRC

- join path:
#+BEGIN_SRC python
pathlib.Path(__file__).parent.parent / 'qhrb_overview_2021_04_03_total.pkl'
#+END_SRC

** os.path — Common pathname manipulations
- delete a file:
#+BEGIN_SRC python
if os.path.exists("demofile.txt"):
  os.remove("demofile.txt")
#+END_SRC

- change file name
#+BEGIN_SRC python
import os
def change_filename(dir_name, filename, suffix, extension=None):
    # name = filename.split('/')[-1]
    path, ext = os.path.splitext(filename)
    if extension is None:
        extension = ext
    return os.path.join(dir_name, path + suffix + extension)


#+END_SRC

- move file
#+BEGIN_SRC python
import os
os.move(source_file_path, destination)
#+END_SRC
- find current working dir:
#+BEGIN_SRC python
import sys, os
# run python file.py
ROOTDIR = os.path.join(os.path.dirname(__file__), os.pardir)
sys.path.append(os.path.join(ROOTDIR, "lib"))
# run in python
ROOTDIR = os.path.join(os.path.dirname("__file__"), os.pardir)
#+END_SRC
- temperary folder:
#+BEGIN_SRC python
import os
import tempfile
TEMP_FOLDER = tempfile.gettempdir()
print('Folder "{}" will be used to save temporary dictionary and corpus.'.format(TEMP_FOLDER))
#+END_SRC
- walk all file from a directory and its sub-directory
Directory tree generator.

For each directory in the directory tree rooted at top (including top
itself, but excluding '.' and '..'), yields a 3-tuple

    dirpath, dirnames, filenames
#+BEGIN_SRC  python
import os
from os.path import join, getsize
for root, dirs, files in os.walk('/home/weiwu/share/deep_learning/data/enwiki'):
    print(root, "consumes, ")
    print(sum([getsize(join(root, name)) for name in files]), '\s')
    print("bytes in", len(files), "non-directory files")
#+END_SRC

- check if file exist
#+BEGIN_SRC python
os.path.isfile(os.path.join(path,name))
#+END_SRC
- get current work directory
#+BEGIN_SRC python
import os
cwd = os.getcwd()
#+END_SRC
- get temporary work directory
#+BEGIN_SRC python
from tempfile import gettempdir
tmp_dir = gettempdir()
#+END_SRC
** fileinput — Iterate over lines from multiple input streams
- open
open() returns a file object, and is most commonly used with two arguments: open(filename, mode).
#+BEGIN_SRC python
>>> f = open('workfile', 'w')
>>> print f
<open file 'workfile', mode 'w' at 80a0960>
#+END_SRC
The first argument is a string containing the filename. The second argument is another string containing a few characters describing the way in which the file will be used. mode can be 'r' when the file will only be read, 'w' for only writing (an existing file with the same name will be erased), and 'a' opens the file for appending; any data written to the file is automatically added to the end. 'r+' opens the file for both reading and writing. The mode argument is optional; *r* will be assumed if it’s omitted.

On Windows, 'b' appended to the mode opens the file in binary mode, so there are also modes like 'rb', 'wb', and 'r+b'.
- write text at the end of a file without overwrite that file:
#+BEGIN_SRC python
f = open('filename.txt', 'a')
f.write("stuff")
f.close()
#+END_SRC
- read specific lines
#+BEGIN_SRC python
fp = open("file")
for i, line in enumerate(fp):
    if i == 25:
        # 26th line
    elif i == 29:
        # 30th line
    elif i > 29:
        break
fp.close()
# Note that i == n-1 for the nth line.

# In Python 2.6 or later:
with open("file") as fp:
    for i, line in enumerate(fp):
        if i == 25:
            # 26th line
        elif i == 29:
            # 30th line
        elif i > 29:
            break
#+END_SRC
** stat — Interpreting stat() results
** filecmp — File and Directory Comparisons
** tempfile — Generate temporary files and directories
** glob — Unix style pathname pattern expansion
** fnmatch — Unix filename pattern matching
** linecache — Random access to text lines
** shutil — High-level file operations
** macpath — Mac OS 9 path manipulation functions
* Data Persistence
** pickle — Python object serialization
*** dump:
#+BEGIN_SRC python
import pickle

data1 = {'a': [1, 2.0, 3, 4+6j],
         'b': ('string', u'Unicode string'),
         'c': None}

selfref_list = [1, 2, 3]
selfref_list.append(selfref_list)

output = open('data.pkl', 'wb')

# Pickle dictionary using protocol 0.
pickle.dump(data1, output)

# Pickle the list using the highest protocol available.
pickle.dump(selfref_list, output, -1)

output.close()
pickle.dump( x0, open( "x0.pkl", "wb" ) )
#+END_SRC

*** load:
- read all the objects in the pickle dump file:
#+BEGIN_SRC python
pickle_file = open('./data/city_20190228.pkl', 'rb')
dict_disease_seed_graph = []
while True:
    try:
        dict_disease_seed_graph.append(pickle.load(pickle_file))
    except EOFError:
        pickle_file.close()
        break
#+END_SRC

#+BEGIN_SRC python
import pprint, pickle

pkl_file = open('data.pkl', 'rb')

data1 = pickle.load(pkl_file)
pprint.pprint(data1)

data2 = pickle.load(pkl_file)
pprint.pprint(data2)

pkl_file.close()
#+END_SRC
** copyreg — Register pickle support functions
** shelve — Python object persistence
** marshal — Internal Python object serialization
** dbm — Interfaces to Unix “databases”
** sqlite3 — DB-API interface for SQLite databases

- show tables:
#+BEGIN_SRC python
import sqlite3
import pandas as pd
# Create your connection.
conn = sqlite3.connect('link_1_label.sqlite')

cursor = conn.cursor()
cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
print(cursor.fetchall())

sql = 'select * from HS_tags_sort'
pd.read_sql_query(sql, conn)

#+END_SRC

- ModuleNotFoundError: No module named 'MySQLdb'
#+BEGIN_SRC bash
pip install mysqlclient
#+END_SRC
- install mysql connector
python ModuleNotFoundError: No module named 'mysql'
#+BEGIN_SRC bash
pip search mysql-connector | grep --color mysql-connector-python
pip install mysql-connector-python-rf
#+END_SRC
** protobuf
*** tutorial
- need a .proto file for the structure:
#+BEGIN_SRC proto
syntax = "proto3"; // or proto2
package tutorial;

import "google/protobuf/timestamp.proto";
// [END declaration]

// [START java_declaration]
option java_package = "com.example.tutorial";
option java_outer_classname = "AddressBookProtos";
// [END java_declaration]

// [START csharp_declaration]
option csharp_namespace = "Google.Protobuf.Examples.AddressBook";
// [END csharp_declaration]

// [START messages]
message Person {
  string name = 1;
  int32 id = 2;  // Unique ID number for this person.
  string email = 3;

  enum PhoneType {
    MOBILE = 0;
    HOME = 1;
    WORK = 2;
  }

  message PhoneNumber {
    string number = 1;
    PhoneType type = 2;
  }

  repeated PhoneNumber phones = 4;

  google.protobuf.Timestamp last_updated = 5;
}

// Our address book file is just one of these.
message AddressBook {
  repeated Person people = 1;
}

#+END_SRC
- Compiling Your Protocol Buffers in shell to generate a class:
#+BEGIN_SRC bash
protoc -I=$SRC_DIR --python_out=$DST_DIR $SRC_DIR/addressbook.proto
protoc --proto_path=src --python_out=build/gen src/foo.proto src/bar/baz.proto
# The compiler will read the files src/foo.proto and src/bar/baz.proto and produce two output files: build/gen/foo_pb2.py and build/gen/bar/baz_pb2.py. The compiler will automatically create the directory build/gen/bar if necessary, but it will not create build or build/gen; they must already exist.
#+END_SRC
- add_person.py
#+BEGIN_SRC python
#! /usr/bin/python

import addressbook_pb2
import sys

# This function fills in a Person message based on user input.
def PromptForAddress(person):
  person.id = int(raw_input("Enter person ID number: "))
  person.name = raw_input("Enter name: ")

  email = raw_input("Enter email address (blank for none): ")
  if email != "":
    person.email = email

  while True:
    number = raw_input("Enter a phone number (or leave blank to finish): ")
    if number == "":
      break

    phone_number = person.phones.add()
    phone_number.number = number

    type = raw_input("Is this a mobile, home, or work phone? ")
    if type == "mobile":
      phone_number.type = addressbook_pb2.Person.MOBILE
    elif type == "home":
      phone_number.type = addressbook_pb2.Person.HOME
    elif type == "work":
      phone_number.type = addressbook_pb2.Person.WORK
    else:
      print "Unknown phone type; leaving as default value."

# Main procedure:  Reads the entire address book from a file,
#   adds one person based on user input, then writes it back out to the same
#   file.
if len(sys.argv) != 2:
  print "Usage:", sys.argv[0], "ADDRESS_BOOK_FILE"
  sys.exit(-1)

address_book = addressbook_pb2.AddressBook()

# Read the existing address book.
try:
  f = open(sys.argv[1], "rb")
  address_book.ParseFromString(f.read())
  f.close()
except IOError:
  print sys.argv[1] + ": Could not open file.  Creating a new one."

# Add an address.
PromptForAddress(address_book.people.add())

# Write the new address book back to disk.
f = open(sys.argv[1], "wb")
f.write(address_book.SerializeToString())
f.close()
#+END_SRC
- try to run above python code in shell:
#+BEGIN_SRC bash
python add_person.py ADDRESS_BOOK_FILE
#+END_SRC
- list_person.py
#+BEGIN_SRC python
#! /usr/bin/python

import addressbook_pb2
import sys

# Iterates though all people in the AddressBook and prints info about them.
def ListPeople(address_book):
  for person in address_book.people:
    print "Person ID:", person.id
    print "  Name:", person.name
    if person.HasField('email'):
      print "  E-mail address:", person.email

    for phone_number in person.phones:
      if phone_number.type == addressbook_pb2.Person.MOBILE:
        print "  Mobile phone #: ",
      elif phone_number.type == addressbook_pb2.Person.HOME:
        print "  Home phone #: ",
      elif phone_number.type == addressbook_pb2.Person.WORK:
        print "  Work phone #: ",
      print phone_number.number

# Main procedure:  Reads the entire address book from a file and prints all
#   the information inside.
if len(sys.argv) != 2:
  print "Usage:", sys.argv[0], "ADDRESS_BOOK_FILE"
  sys.exit(-1)

address_book = addressbook_pb2.AddressBook()

# Read the existing address book.
f = open(sys.argv[1], "rb")
address_book.ParseFromString(f.read())
f.close()

ListPeople(address_book)
#+END_SRC
- read a message:
#+BEGIN_SRC bash
python list_person.py ADDRESS_BOOK_FILE
#+END_SRC
** neo4j
*** install
#+BEGIN_SRC bash
pip install neo4j-driver py2neo

#+END_SRC
*** basic operations
- create cursor:
#+BEGIN_SRC python
g = Graph(host="192.168.4.36",  # neo4j 搭载服务器的ip地址，ifconfig可获取到
            http_port=7474,  # neo4j 服务器监听的端口号
            user="neo4j",  # 数据库user name，如果没有更改过，应该是neo4j
            password="neo4j123")
#+END_SRC

- insert nodes and edges:
#+BEGIN_SRC python
from py2neo import Graph, Node
g = Graph(host="192.168.4.36",  # neo4j 搭载服务器的ip地址，ifconfig可获取到
        http_port=7474,  # neo4j 服务器监听的端口号
        user="neo4j",  # 数据库user name，如果没有更改过，应该是neo4j
        password="neo4j123")
def create_node(label, nodes):
    count = 0
    for node_name in tqdm(nodes):
        node = Node(label, name=node_name)
        # g.schema.create_uniqueness_constraint(label, node_name)
        try:
            g.create(node)
            count += 1
        except ClientError:
            continue
        # debug(count)
    return


'''创建实体关联边'''
def create_relationship(start_node, end_node, edges, rel_type, rel_name):
    count = 0
    # 去重处理
    set_edges = []
    for edge in edges:
        try:
            set_edges.append('###'.join(edge))
        except TypeError:
            continue
    all = len(set(set_edges))
    for edge in tqdm(set(set_edges)):
        edge = edge.split('###')
        p = edge[0]
        q = edge[1]
        if p==q:
            continue
        query = "match(p:%s),(q:%s) where p.name='%s'and q.name='%s' create (p)-[rel:%s{name:'%s'}]->(q)" % (
            start_node, end_node, p, q, rel_type, rel_name)
        try:
            g.run(query)
            count += 1
            # debug(rel_type)
        except Exception as e:
            info(e)
    return

'''创建知识图谱中心疾病的节点'''
def create_diseases_nodes(disease_infos):
    count = 0
    for disease_dict in tqdm(disease_infos):
        node = Node("Disease", name=disease_dict['name'], desc=disease_dict['desc'],
                    prevent=disease_dict['prevent'] ,cause=disease_dict['cause'],
                    easy_get=disease_dict['easy_get'],cure_lasttime=disease_dict['cure_lasttime'],
                    cure_department=disease_dict['cure_department']
                    ,cure_way=disease_dict['cure_way'] , cured_prob=disease_dict['cured_prob'])
        g.create(node)
        # count += 1
        # debug(count)
    return

#+END_SRC

** mysql
*** create
- create table
#+BEGIN_SRC sql
-- Create table
create table PATIENT_PROFILE
(
  PERSON_ID                VARCHAR2(50) not null,
  PERSON_AGE               INTEGER not null,
  PERSON_SEX               VARCHAR2(10) not null,
  AGE_GROUP                VARCHAR2(10) not null,
  RISK_SCORE               VARCHAR2(10) default 1,
  CHRONIC_DIS              VARCHAR2(10) default 'False',
  INFECTIOUS_DIS           VARCHAR2(10) default 'False',
  TUMOR                    VARCHAR2(10) default 'False',
  PSYCHIATRIC              VARCHAR2(10) default 'False',
  IMPLANTABLE_DEV          VARCHAR2(10) default 'False',
  TREATMENT_PERC_LV        VARCHAR2(10) default 'low',
  INSPECTION_PERC_LV       VARCHAR2(10) default 'low',
  DRUG_PERC_LV             VARCHAR2(10) default 'low',
  OUTPATIENT_PERC_LV       VARCHAR2(10) default 'low',
  DRUG_PURCH_AVG_LV        VARCHAR2(10) default 'low',
  SELF_SURPP_PERC_LV       VARCHAR2(10) default 'low',
  CUM_OUTPATIENT_LV        VARCHAR2(10) default 'low',
  CUM_HOSP_LV              VARCHAR2(10) default 'low',
  HOSP_AVG_LV              VARCHAR2(10) default 'low',
  OUTPATIENT_AVG_LV        VARCHAR2(10) default 'low',
  DRUG_PURCH_FREQ_LV       VARCHAR2(10) default 'low',
  OUTPATIENT_FREQ_LV       VARCHAR2(10) default 'low',
  HOSP_FREQ_LV             VARCHAR2(10) default 'low',
  HOSP_PREFERENCE          VARCHAR2(10),
  FIRST_VISIT_PREFERENCE   VARCHAR2(10),
  TRAILING_12_MONTHS_LEVEL VARCHAR2(10) default 'low',
  TRAILING_36_MONTHS_LEVEL VARCHAR2(10) default 'low',
  SHORT_TERM               VARCHAR2(10) default 'False',
  LONG_TERM                VARCHAR2(10) default 'False'
)
tablespace WUXI
  pctfree 10
  initrans 1
  maxtrans 255;
-- Add comments to the table
comment on table PATIENT_PROFILE
  is '个人用户画像';
-- Add comments to the columns
comment on column PATIENT_PROFILE.PERSON_SEX
  is '描述患者的生理性别
';
comment on column PATIENT_PROFILE.AGE_GROUP
  is '0-6 童年，7-17 少年， 18-40 青年， 41-65 中年， 66以上老年
';
comment on column PATIENT_PROFILE.RISK_SCORE
  is '健康风险等级
';
comment on column PATIENT_PROFILE.CHRONIC_DIS
  is '慢性病患者';
comment on column PATIENT_PROFILE.INFECTIOUS_DIS
  is '传染病病原携带者 ';
comment on column PATIENT_PROFILE.TUMOR
  is '恶性肿瘤患者 ';
comment on column PATIENT_PROFILE.PSYCHIATRIC
  is '精神疾病患者 ';
comment on column PATIENT_PROFILE.IMPLANTABLE_DEV
  is '植入性器材';
comment on column PATIENT_PROFILE.TREATMENT_PERC_LV
  is '治疗费用占比';
comment on column PATIENT_PROFILE.INSPECTION_PERC_LV
  is '检查费用占比';
comment on column PATIENT_PROFILE.DRUG_PERC_LV
  is '药物费用占比';
comment on column PATIENT_PROFILE.OUTPATIENT_PERC_LV
  is '门诊住院费用比例';
comment on column PATIENT_PROFILE.DRUG_PURCH_AVG_LV
  is '单次平均购药金额';
comment on column PATIENT_PROFILE.SELF_SURPP_PERC_LV
  is '自负费用占比';
comment on column PATIENT_PROFILE.CUM_OUTPATIENT_LV
  is '累计门诊金额
';
comment on column PATIENT_PROFILE.CUM_HOSP_LV
  is '累计住院金额';
comment on column PATIENT_PROFILE.HOSP_AVG_LV
  is '平均住院金额';
comment on column PATIENT_PROFILE.OUTPATIENT_AVG_LV
  is '平均门诊金额';
comment on column PATIENT_PROFILE.DRUG_PURCH_FREQ_LV
  is '药店购药频繁程度';
comment on column PATIENT_PROFILE.OUTPATIENT_FREQ_LV
  is '门诊就诊频繁程度';
comment on column PATIENT_PROFILE.HOSP_FREQ_LV
  is '住院就诊频繁程度
';
comment on column PATIENT_PROFILE.HOSP_PREFERENCE
  is '就诊医院偏好
1门诊、2药店购药、3住院
';
comment on column PATIENT_PROFILE.FIRST_VISIT_PREFERENCE
  is '首选就诊偏好，对一次健康问题的首次就医方式的选择偏好。1门诊、2药店购药、3住院
';
comment on column PATIENT_PROFILE.TRAILING_12_MONTHS_LEVEL
  is '近一年费用支出水平
';
comment on column PATIENT_PROFILE.TRAILING_36_MONTHS_LEVEL
  is '近三年费用支出水平
';
comment on column PATIENT_PROFILE.SHORT_TERM
  is '短期内再入院';
comment on column PATIENT_PROFILE.LONG_TERM
  is '长周期住院';

#+END_SRC
- create from select:
#+BEGIN_SRC sql
create table temp_pairs_id as
select t1.GRID person_1, t2.GRID person_2, sysdate as crt_date
  from MMAP_SHB_SPECIAL.Ck10_Ghdj t1
#+END_SRC
- create index
- create multiple index
*** select
- select the first row in each group:
#+BEGIN_SRC sql
WITH summary AS (
select person_id, hosp_lev, clinic_type, substr(out_diag_dis_cd, 0, 3) as icd3, to_date(substr(out_hosp_date, 0, 8), 'yyyymmdd') as discharge_date,
ROW_NUMBER() OVER(PARTITION BY person_id, substr(out_diag_dis_cd, 0, 3)
                                 ORDER BY to_date(substr(out_hosp_date, 0, 8), 'yyyymmdd')) AS rk from t_kc21
where out_diag_dis_cd is not null and clinic_type is not null)
SELECT s.*
  FROM summary s
 WHERE s.rk = 1
#+END_SRC
*** insert
- insert by batches:
#+BEGIN_SRC python
from tqdm import tqdm
def chunker(seq, size):
    return (seq[pos:pos + size] for pos in range(0, len(seq), size))
chunksize = 500
with tqdm(total=len(df_disease)) as pbar:
    for i, cdf in enumerate(chunker(df_disease, chunksize)):
        cdf.to_sql(con=engine, name="disease_profile", if_exists="append", index=False)
        pbar.update(chunksize)
#+END_SRC

- insert data into sql:
#+BEGIN_SRC python
cursor = CONN.cursor()
s_insert = """INSERT INTO t_jbhlx (TBBH, JB_CD, HLFYSX, HLFYXX, HLZYTSSX, HLZYTSXX, LOGIN_ID, MODIFY_ID, DELE_FLG) VALUES (:1, :2, :3, :4, :5, :6, :7, :8, :9)"""
for idx, row in df_sim.iterrows():
    if counter % 1000 == 0:
        print(idx)
        print(counter)
    counter += 1
try:
    cursor.execute(s_insert, (str(data.table_index.iloc[0]), #TBBH,
                              accident_id, #JB_CD,
                            str(cost_upper_bnd), #HLFYSX
                            str(cost_lower_bnd), #HLFYXX
                            str(period_upper_bnd), #HLZYTSSX
                            str(period_lower_bnd), #HLZYTSXX
                            'admin', #LOGIN_ID
                            'admin', #MODIFY_ID
                            '1'  #DELE_FLG
    ))
except cx_Oracle.IntegrityError:
    pass
CONN.commit()
#+END_SRC
- True\False
can't insert True or False directly into table, need to turn True/False into string.
- insert via engine:
#+BEGIN_SRC python
from sqlalchemy import create_engine
from sqlalchemy import MetaData
from sqlalchemy import Table
from sqlalchemy.orm import sessionmaker
conn = engine.connect()
# Open the session
Session = sessionmaker(bind=engine)
session = Session()
# Create MetaData instance
metadata = MetaData()
#print(metadata.tables)
# reflect db schema to MetaData
metadata.reflect(bind=engine)
# Get Table
table_name = 't_kc22'
ex_table = metadata.tables[table_name]
print(ex_table)
listToWrite = df_22_temp.to_dict(orient='records')
try:
    conn.execute(ex_table.insert(), listToWrite)
except Exception as e:
    print(e)
#+END_SRC
*** delete
- clear a table
The TRUNCATE TABLE statement is used to remove all records from a table in Oracle. It performs the same function as a DELETE statement without a WHERE clause.
#+BEGIN_SRC sql
TRUNCATE TABLE [schema_name.]table_name
#+END_SRC

- delete a table
#+BEGIN_SRC sql
drop table name
#+END_SRC
*** update
- update with python:
#+BEGIN_SRC python
s_insert = """update patient_profile set CHRONIC_DIS='{0}',INFECTIOUS_DIS='{1}', TUMOR='{2}', PSYCHIATRIC='{3}' where person_id={4}"""
counter = 0
for idx, row in df_disease.iterrows():
    if counter % 100000 == 0:
        print(counter)
    counter += 1
    cursor.execute(s_insert.format(str(row['慢性病患者']),
                                   str(row['传染病病原携带者']),
                                   str(row['恶性肿瘤患者']),
                                   str(row['精神疾病患者']),
                             str(idx)))
#+END_SRC
* Data Compression and Archiving
** zip
zip([iterable, ...])
This function returns a list of tuples, where the i-th tuple contains the i-th element from each of the argument sequences or iterables. The returned list is truncated in length to the length of the shortest argument sequence. When there are multiple arguments which are all of the same length, zip() is similar to map() with an initial argument of None. With a single sequence argument, it returns a list of 1-tuples. With no arguments, it returns an empty list.

The left-to-right evaluation order of the iterables is guaranteed. This makes possible an idiom for clustering a data series into n-length groups using zip(*[iter(s)]*n).

zip() in conjunction with the * operator can be used to unzip a list:
#+BEGIN_SRC shell
>>>
>>> x = [1, 2, 3]
>>> y = [4, 5, 6]
>>> zipped = zip(x, y)
>>> zipped
[(1, 4), (2, 5), (3, 6)]
>>> x2, y2 = zip(*zipped)
>>> x == list(x2) and y == list(y2)
True
#+END_SRC
- create a dictionary with two iterables
#+BEGIN_SRC python
>>> x = [1, 2, 3]
>>> y = [4, 5, 6]
>>> zipped = zip(x, y)
>>> zipped
[(1, 4), (2, 5), (3, 6)]
In [172]: dict(zipped)
Out[179]: {1: 4, 2: 5, 3: 6}

#+END_SRC
** zlib — Compression compatible with gzip
** gzip — Support for gzip files
** bz2 — Support for bzip2 compression
** lzma — Compression using the LZMA algorithm
** zipfile — Work with ZIP archives
** tarfile — Read and write tar archive files
* File Formats
** csv — CSV File Reading and Writing
** *args, **kwargs
如果我们不确定要往函数中传入多少个参数，或者我们想往函数中以列表和元组的形式传参数时，那就使要用*args；
#+BEGIN_SRC python
>>> args = ("two", 3,5)
>>> test_args_kwargs(*args)
arg1: two
arg2: 3
arg3: 5

#+END_SRC
如果我们不知道要往函数中传入多少个关键词参数，或者想传入字典的值作为关键词参数时，那就要使用**kwargs。args和kwargs这两个标识符是约定俗成的用法，你当然还可以用*bob和**billy，但是这样就并不太妥。
#+BEGIN_SRC python
>>> kwargs = {"arg3": 3, "arg2": "two","arg1":5}
>>> test_args_kwargs(**kwargs)
arg1: 5
arg2: two
arg3: 3

#+END_SRC
- construct argparse for test:
#+BEGIN_SRC python
class argparse(dict):
    """
    Example:
    m = Map({'first_name': 'Eduardo'}, last_name='Pool', age=24, sports=['Soccer'])
    """
    def __init__(self, *args, **kwargs):
        super(argparse, self).__init__(*args, **kwargs)
        for arg in args:
            if isinstance(arg, dict):
                for k, v in arg.iteritems():
                    self[k] = v

        if kwargs:
            for k, v in kwargs.iteritems():
                self[k] = v
    def add_argument(self, *args, **kwargs):
        # super(Map, self).__init__(*args, **kwargs)
        for i in args:
            self[i.strip('-')] = kwargs.get('default', None)
            if 'action' in kwargs:
                if kwargs['action'] == 'store_true':
                    self[i.strip('-')] = True
                else:
                    self[i.strip('-')] = False
    def parse_args(self):
        return self

    def __getattr__(self, attr):
        return self.get(attr)

    def __setattr__(self, key, value):
        self.__setitem__(key, value)

    def __setitem__(self, key, value):
        super(argparse, self).__setitem__(key, value)
        self.__dict__.update({key: value})

    def __delattr__(self, item):
        self.__delitem__(item)

    def __delitem__(self, key):
        super(argparse, self).__delitem__(key)
        del self.__dict__[key]
#+END_SRC
** configparser — Configuration file parser
- use yaml and config file.
#+BEGIN_SRC yaml
# config.yaml
engine:
  user:
    'jack'
  password:
    'password'
#+END_SRC

#+BEGIN_SRC python
import yaml
with open(r'config.yaml', 'rb') as f:
    config = yaml.load(f)

#+END_SRC
- ylib.yaml_config
#+BEGIN_SRC python
from ylib.yaml_config import Configuraion
config = Configuraion()
config.load('../config.yaml')
print(config.__str__)

USER_AGENT = config.USER_AGENT
DOMAIN = config.DOMAIN
BLACK_DOMAIN = config.BLACK_DOMAIN
URL_SEARCH = config.URL_SEARCH

#+END_SRC
** netrc — netrc file processing
** xdrlib — Encode and decode XDR data
** plistlib — Generate and parse Mac OS X .plist files
* Cryptographic Services
** hashlib — Secure hashes and message digests

#+BEGIN_SRC python
import binascii
def encrypt(string: str):
#     return str(int.from_bytes(string.encode('utf-8'), byteorder='big'))
    return str(int(binascii.hexlify(string.encode("utf-8")), 16))

def num2str(number: str):
    return binascii.unhexlify(format(number, "x").encode("utf-8")).decode("utf-8")

def encrypt(string: str):
    return base64.encodestring(string.encode("utf-8")).decode('utf-8')
#+END_SRC

** hmac — Keyed-Hashing for Message Authentication
** secrets — Generate secure random numbers for managing secrets
* Generic Operating System Services
** os — Miscellaneous operating system interfaces
** io — Core tools for working with streams
** time — Time access and conversions
** argparse — Parser for command-line options, arguments and sub-commands
*** 16.4.2. ArgumentParser objects
class argparse.ArgumentParser(prog=None, usage=None, description=None, epilog=None, parents=[], formatter_class=argparse.HelpFormatter, prefix_chars='-', fromfile_prefix_chars=None, argument_default=None, conflict_handler='error', add_help=True, allow_abbrev=True)
Create a new ArgumentParser object. All parameters should be passed as keyword arguments. Each parameter has its own more detailed description below, but in short they are:

prog - The name of the program (default: sys.argv[ 0])
usage - The string describing the program usage (default: generated from arguments added to parser)
description - Text to display before the argument help (default: none)
epilog - Text to display after the argument help (default: none)
parents - A list of ArgumentParser objects whose arguments should also be included
formatter_class - A class for customizing the help output
prefix_chars - The set of characters that prefix optional arguments (default: ‘-‘)
fromfile_prefix_chars - The set of characters that prefix files from which additional arguments should be read (default: None)
argument_default - The global default value for arguments (default: None)
conflict_handler - The strategy for resolving conflicting optionals (usually unnecessary)
add_help - Add a -h/--help option to the parser (default: True)
allow_abbrev - Allows long options to be abbreviated if the abbreviation is unambiguous. (default: True)
*** argument_default
#+BEGIN_SRC python
>>> parser = argparse.ArgumentParser(argument_default=argparse.SUPPRESS)
>>> parser.add_argument('--foo')
>>> parser.add_argument('bar', nargs='?')
>>> parser.parse_args(['--foo', '1', 'BAR'])
Namespace(bar='BAR', foo='1')
>>> parser.parse_args([])
Namespace()
#+END_SRC
*** example
#+BEGIN_SRC python
import argparse

logger = logging.getLogger()
handler = logging.StreamHandler()
formatter = logging.Formatter(
    '%(asctime)s %(name)-12s %(levelname)-8s %(message)s')
handler.setFormatter(formatter)
if not logger.handlers:
    logger.addHandler(handler)
    logger.setLevel(logging.DEBUG)

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-i", "--input", required=False, help="Input word2vec model")
    parser.add_argument(
        "-o", "--output", required=False, help="Output tensor file name prefix")
    parser.add_argument(
        "-b",
        "--binary",
        required=False,
        help="If word2vec model in binary format, set True, else False")
    parser.add_argument(
        "-l",
        "--logdir",
        required=False,
        help="periodically save model variables in a checkpoint")
    parser.add_argument(
        "--host",
        required=False,
        help="host where holding the tensorboard projector service")
    parser.add_argument("-p", "--port", required=False, help="browser port")
    args = parser.parse_args()

    word2vec2tensor(args.input, args.output, args.binary)

#+END_SRC
*** another way to define function parameters and provide parameters:
- define function parameters inside a function:
#+BEGIN_SRC python
def convert_pdf2txt(args=None):
    import argparse
    P = argparse.ArgumentParser(description=__doc__)
    P.add_argument(
        "-m", "--maxpages", type=int, default=0, help="Maximum pages to parse")

    A = P.parse_args(args=args)
    print(A.maxpages)
#+END_SRC
- provide parameters:
#+BEGIN_SRC python
# all parameters should be strings
convert_pdf2txt(['--maxpages', '123'])
#+END_SRC
** getopt — C-style parser for command line options
** logging — Logging facility for Python
#+BEGIN_SRC python

import logging
logger = logging.getLogger()
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s %(name)-12s %(levelname)-8s %(message)s')
handler.setFormatter(formatter)
if not logger.handler:
    logger.addHandler(handler)
logger.setLevel(logging.DEBUG)
logger
# or
logging.basicConfig(
    format='%(asctime)s : %(levelname)s : %(message)s', level=logging.DEBUG)

# at the end of the program
handler.close()
logger.removeHandler(handler)
#+END_SRC

- ylog
#+BEGIN_SRC python
from ylib import ylog
import logging

ylog.set_level(logging.DEBUG)
ylog.console_on()
ylog.filelog_on("app")
#+END_SRC

- replace the logging with loguru:
#+BEGIN_SRC python
import logging
from loguru import logger
class InterceptHandler(logging.Handler):
    def emit(self, record):
        # Get corresponding Loguru level if it exists
        try:
            level = logger.level(record.levelname).name
        except ValueError:
            level = record.levelno

        # Find caller from where originated the logged message
        frame, depth = logging.currentframe(), 2
        while frame.f_code.co_filename == logging.__file__:
            frame = frame.f_back
            depth += 1

        logger.opt(depth=depth, exception=record.exc_info).log(level, record.getMessage())

logging.basicConfig(handlers=[InterceptHandler()], level=0)
#+END_SRC

** logging.config — Logging configuration
** logging.handlers — Logging handlers
** getpass — Portable password input
** curses — Terminal handling for character-cell displays
** curses.textpad — Text input widget for curses programs
** curses.ascii — Utilities for ASCII characters
** curses.panel — A panel stack extension for curses
** platform — Access to underlying platform’s identifying data
** errno — Standard errno system symbols
** ctypes — A foreign function library for Python
* Concurrent Execution
** threading — Thread-based parallelism
** threading & queue
*** install
#+BEGIN_SRC shell
pip install queuelib
#+END_SRC
*** example
#+BEGIN_SRC python
from Queue import Queue
import threading

#+END_SRC
** multiprocessing — Process-based parallelism
** The concurrent package
** concurrent.futures — Launching parallel tasks
** subprocess — Subprocess management
** sched — Event scheduler
** queue — A synchronized queue class
** dummy_threading — Drop-in replacement for the threading module
** _thread — Low-level threading API
** _dummy_thread — Drop-in replacement for the _thread module

* Internet Data Handling
** Jupyter notebook
- ipython memory usage:
#+BEGIN_SRC python
import ipython_memory_usage
%ipython_memory_usage_start
#+END_SRC

*** Using a virtualenv in an IPython notebook
1. Install the ipython kernel module into your virtualenv
#+BEGIN_SRC python
workon my-virtualenv-name  # activate your virtualenv, if you haven't already
pip install ipykernel
#+END_SRC

2. Now run the kernel "self-install" script:
#+BEGIN_SRC python
python -m ipykernel install --user --name=my-virtualenv-name
#+END_SRC

3. list all the kernels
#+BEGIN_SRC python
jupyter kernelspec list
#+END_SRC

4. remove uninstall kernel
#+BEGIN_SRC python
jupyter kernelspec uninstall anaconda2.7
#+END_SRC
*** Extension & Configuration
- install extension:
#+BEGIN_SRC bash
conda install -c conda-forge jupyter_contrib_nbextensions
#+END_SRC
- Enable line number by default
#+BEGIN_SRC bash
touch ~/.jupyter/static/custom/custom.js
#+END_SRC
add below text in the file:
#+BEGIN_SRC js
define([
    'base/js/namespace',
    'base/js/events'
],
       function(IPython, events) {
           events.on("app_initialized.NotebookApp",
                     function () {
                         IPython.Cell.options_default.cm_config.lineNumbers = true;
                     }
                    );
       }
      );
#+END_SRC
- enable auto complete
#+BEGIN_SRC bash
jupyter nbextension enable hinterland/hinterland
#+END_SRC

- clear jupyter notebook ouput in all cells:
#+BEGIN_SRC bash
jupyter nbconvert --ClearOutputPreprocessor.enabled=True --inplace xxx.ipynb
#+END_SRC

- ipython history size
#+BEGIN_SRC ipython
%config TerminalInteractiveShell.history_length=100000
#+END_SRC

** typical structure of the ipython notebook ipynb:
1. imports
2. get data
3. transform data
4. modeling
5. visualization
6. making sense of the data

summary:
- notebook should have one hypothesis data interpretation loop
- make a multi-project utils library
- each cell should have one and only one output
- try to keep code inside notebooks.


** fetch data from yahoo
install pandas-datareader first.
#+BEGIN_SRC shell
conda install pandas-datareader
#+END_SRC

#+begin_src python
import pandas as pd
import datetime as dt
import numpy as np
from pandas_datareader import data as web

data = pd.DataFrame()
symbols = ['GLD', 'GDX']
for sym in symbols:
    data[sym] = web.DataReader(sym, data_source='yahoo', start='20100510')['Adj Close']
data = data.dropna()
#+end_src

** email — An email and MIME handling package
- attach multiple charts:
#+BEGIN_SRC python
def attach_image(path, filename, msg, text=''):
    with open(path, 'rb') as f:
        msgAlternative = MIMEMultipart('alternative')
        msg.attach(msgAlternative)

        msgText = MIMEText(filename)
        msgAlternative.attach(msgText)

        # We reference the image in the IMG SRC attribute by the ID we give it below
#         msgText = MIMEText('<b>Some <i>HTML</i> text</b> and an image.<br><img src="cid:%s"><br>'%filename, 'html')
        msgText = MIMEText('%s<br><img src="cid:%s"><br>'%(text, filename), 'html')
        msgAlternative.attach(msgText)

        # This example assumes the image is in the current directory
        fp = open(path, 'rb')
        msgImage = MIMEImage(fp.read())
        fp.close()

        # Define the image's ID as referenced above
        msgImage.add_header('Content-ID', '<%s>'%filename)
        msg.attach(msgImage)
#+END_SRC
** json — JSON encoder and decoder
** Graph
*** networkx
- add node to a graph
- add edges to a graph
- find a loop/cycle in a graph
#+BEGIN_SRC python
nx.find_cycle(G)
list(nx.simple_cycles(G))
#+END_SRC

- find sub graph:
#+BEGIN_SRC python
data_sub = nx.connected_component_subgraphs(G, copy=True)
subgraphs = list(data_sub)
#+END_SRC

- draw graph:
#+BEGIN_SRC python
nx.draw(G)

#+END_SRC

* Internet Protocols and Support
** webbrowser — Convenient Web-browser controller
** cgi — Common Gateway Interface support
** cgitb — Traceback manager for CGI scripts
** wsgiref — WSGI Utilities and Reference Implementation
** urllib — URL handling modules
** urllib.request — Extensible library for opening URLs
** urllib.response — Response classes used by urllib
** urllib.parse — Parse URLs into components
** urllib.error — Exception classes raised by urllib.request
** urllib.robotparser — Parser for robots.txt
** http — HTTP modules
** http.client — HTTP protocol client
** ftplib — FTP protocol client
** poplib — POP3 protocol client
** imaplib — IMAP4 protocol client
** nntplib — NNTP protocol client
** smtplib — SMTP protocol client
** smtpd — SMTP Server
** telnetlib — Telnet client
** uuid — UUID objects according to RFC 4122
- random hex:
#+BEGIN_SRC python
import uuid
uuid.uuid4().hex
#+END_SRC
** socketserver — A framework for network servers
** http.server — HTTP servers
** http.cookies — HTTP state management
** http.cookiejar — Cookie handling for HTTP clients
** xmlrpc — XMLRPC server and client modules
** xmlrpc.client — XML-RPC client access
** xmlrpc.server — Basic XML-RPC servers
** ipaddress — IPv4/IPv6 manipulation library

* Development Tools
** typing — Support for type hints
** pydoc — Documentation generator and online help system
** doctest — Test interactive Python examples
** unittest — Unit testing framework
- check data operation:
  - create, select, update, delete.

- purpose of unit test
  - checking parameter types, classes, or values.
  - checking data structure invariants.
  - checking “can’t happen” situations (duplicates in a list, contradictory state variables.)
  - after calling a function, to make sure that its return is reasonable.

** unittest.mock — mock object library
** unittest.mock — getting started

** test — Regression tests package for Python
** test.support — Utilities for the Python test suite
* Debugging and Profiling
** bdb — Debugger framework
** faulthandler — Dump the Python traceback
** pdb — The Python Debugger
- s(tep):

Execute the current line, stop at the first possible occasion (either in a function that is called or in the current function).

- n(ext):

Continue execution until the next line in the current function is reached or it returns.

- unt(il):

Continue execution until the line with a number greater than the current one is reached or until the current frame returns.

- r(eturn):
- c(ont(inue)):

Continue execution, only stop when a breakpoint is encountered.

- l(ist): [first [,last]]

List source code for the current file. Without arguments, list 11 lines around the current line or continue the previous listing. With one argument, list 11 lines starting at that line. With two arguments, list the given range; if the second argument is less than the first, it is a count.

- a(rgs):

Print the argument list of the current function.

- p expression:

Print the value of the expression.
** The Python Profilers
STEPS:
1). install snakeviz using pip from cmd.
#+BEGIN_SRC shell
pip install snakeviz
#+END_SRC

2). profile the test python file using below command.
#+BEGIN_SRC shell
$ python -m cProfile -o profile.stats test.py
#+END_SRC
#+BEGIN_SRC python
# test.py
from random import randint
max_size = 10**4
data = [randint(0, max_size) for _ in range(max_size)]
test = lambda: insertion_sort(data)

#+END_SRC

3). check the efficiency result from profile.stats file.
#+BEGIN_SRC shell
$ snakeviz profile.stats
#+END_SRC

** timeit — Measure execution time of small code snippets

** trace — Trace or track Python statement execution
** tracemalloc — Trace memory allocations
- memory usage of an object:
#+BEGIN_SRC python
print('all_data占据内存约: {:.2f} GB'.format(df_temp.memory_usage().sum()/ (1024**3)))
#+END_SRC
* Software Packaging and Distribution
** pip

- pip download modules from requirements.txt:
#+BEGIN_SRC bash
pip download --destination-directory packages -r requirements.txt
#+END_SRC

- pip install from local files alied with requirements.txt
#+BEGIN_SRC bash
pip install --no-index --find-links=packages -r requirements.txt
#+END_SRC

- pip cache dir:
#+BEGIN_SRC bash
import pip
from distutils.version import LooseVersion

if LooseVersion(pip.__version__) < LooseVersion('10'):
    # older pip version
    from pip.utils.appdirs import user_cache_dir
else:
    # newer pip version
    from pip._internal.utils.appdirs import user_cache_dir

print(user_cache_dir('pip'))
print(user_cache_dir('wheel'))

#+END_SRC
- install opencv, pytorch:
#+BEGIN_SRC bash
pip install opencv-python torch
#+END_SRC

- install with a wheel .whl file:
#+BEGIN_SRC bash
pip install *.whl
#+END_SRC
- Upgrading pip
#+BEGIN_SRC bash
pip install -U pip
#+END_SRC
- add below setup to ~/.pip/pip.conf on unix or %APPDATA%\pip\pip.conf on Windows
#+begin_src txt
[global]
#index-url=https://pypi.mirrors.ustc.edu.cn/simple/
#index-url=https://pypi.python.org/simple/
index-url=http://mirrors.aliyun.com/pypi/simple/
#index-url=https://pypi.gocept.com/pypi/simple/
#index-url=https://mirror.picosecond.org/pypi/simple/
[install]
trusted-host=mirrors.aliyun.com
#trusted-host=mirrors.ustc.edu.cn
#+end_src

or run pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

- generate a requirements file:
#+BEGIN_SRC bash
pip freeze > requirements.txt
#+END_SRC
- pip install directly:
requirements.txt
#+begin_src txt
--index-url http://mirrors.aliyun.com/pypi/simple/
pandas
pylint
pep8
sphinx
ipython
numpy
ipdb
mock
nose
#+end_src
** distutils — Building and installing Python modules
** ensurepip — Bootstrapping the pip installer
** venv — Creation of virtual environments
** zipapp — Manage executable python zip archives
** pyenv — Simple Python version management
- check installed versions
#+BEGIN_SRC shell
pyenv versions
#+END_SRC

#+RESULT:
:  system
:  2.7.13
:  3.6.0
:  3.6.0/envs/general
:  3.6.0/envs/simulate
:  3.6.0/envs/venv3.6.0
:  3.6.0/envs/venv3.6.0.1
: * anaconda3-4.4.0 (set by /home/weiwu/projects/simulate/.python-version)
:  general
:  simulate
:  venv3.6.0
:  venv3.6.0.1

* Python Runtime Services
** sysconfig — Provide access to Python’s configuration information
** os, sys — System-specific parameters and functions
- set environment in windows:
#+BEGIN_SRC shell
set PYTHONPATH="F:\projects\env\Scripts"
python -c "import distutils.sysconfig; print(distutils.sysconfig.get_config_var('VERSION'))"
pause
#+END_SRC

- get environment variables
#+BEGIN_SRC python
import os

env_dist = os.environ # environ是在os.py中定义的一个dict environ = {}

print(env_dist.get('JAVA_HOME'))
print(env_dist['JAVA_HOME'])

#+END_SRC
- check if file or directory exists, if not then make directory:
#+BEGIN_SRC python
import os
os.path.exists(test_file.txt)
os.path.isfile("test-data")
export_dir = "export/"
if not os.path.exists(export_dir):
    os.mkdir(export_dir)
#+END_SRC

 - read a file:
import os
folder = '/file/path'
file = os.path.join(folder, 'file_name')

- list all the files under a directory:
#+BEGIN_SRC python
# os.listdir() 方法用于返回指定的文件夹包含的文件或文件夹的名字的列表。这个列表以字母顺序。 它不包括 '.' 和'..' 即使它在文件夹中.
path = os.getcwd()
dirs = os.listdir(path)
#+END_SRC

- check if the file readable:
#+BEGIN_SRC python
import os
if os.access("/file/path/foo.txt", os.F_OK):
    print "Given file path is exist."

if os.access("/file/path/foo.txt", os.R_OK):
    print "File is accessible to read"

if os.access("/file/path/foo.txt", os.W_OK):
    print "File is accessible to write"

if os.access("/file/path/foo.txt", os.X_OK):
    print "File is accessible to execute"

#+END_SRC
- use sys to get command arguments:
#+BEGIN_SRC python
#!/usr/bin/python3

import sys

print ('参数个数为:', len(sys.argv), '个参数。')
print ('参数列表:', str(sys.argv))

#+END_SRC
#+BEGIN_SRC shell
$ python3 test.py arg1 arg2 arg3
参数个数为: 4 个参数。
参数列表: ['test.py', 'arg1', 'arg2', 'arg3']
#+END_SRC

** builtins — Built-in objects
** __main__ — Top-level script environment
** warnings — Warning control
- SettingWithCopyWarning in Pandas, ignore pandas warning
#+BEGIN_SRC python
pd.options.mode.chained_assignment = None  # default='warn'
#+END_SRC

- future warning:
#+BEGIN_SRC python
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

#+END_SRC

- RuntimeWarning in numpy:
numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
#+BEGIN_SRC python
import warnings
warnings.filterwarnings("ignore", message="numpy.dtype size changed")
warnings.filterwarnings("ignore", message="numpy.ufunc size changed")

#+END_SRC

** contextlib — Utilities for with-statement contexts
** abc — Abstract Base Classes
** atexit — Exit handlers
** traceback — Print or retrieve a stack traceback
- print function will convert object in memory into string and out to the standard io port. convert print content into an object:
#+BEGIN_SRC python
from io import StringIO
import sys

def print_audio_tagging_result(value):
    print(f"value = {value}")

tag_list = []
for i in range(0,1):
    save_stdout = sys.stdout
    result = StringIO()
    sys.stdout = result
    print_audio_tagging_result(i)
    sys.stdout = save_stdout
    tag_list.append(result.getvalue())
print(tag_list)

#+END_SRC


** __future__ — Future statement definitions
从Python 2.7到Python 3.x就有不兼容的一些改动，比如2.x里的字符串用'xxx'表示str，Unicode字符串用u'xxx'表示unicode，而在3.x中，所有字符串都被视为unicode，因此，写u'xxx'和'xxx'是完全一致的，而在2.x中以'xxx'表示的str就必须写成b'xxx'，以此表示“二进制字符串”。

要直接把代码升级到3.x是比较冒进的，因为有大量的改动需要测试。相反，可以在2.7版本中先在一部分代码中测试一些3.x的特性，如果没有问题，再移植到3.x不迟。

Python提供了__future__模块，把下一个新版本的特性导入到当前版本，于是我们就可以在当前版本中测试一些新版本的特性。
#+BEGIN_SRC python
from __future__ import print_function
from __future__ import division
from __future__ import unicode_literals
from __future__ import absolute_import
#+END_SRC
- unicode vs utf-8 vs binary strings vs strings
unicode 是编码unique code,例如把一个汉字编成了一个码(计算机不可读).

A chinese character:      汉

it's unicode value:       U+6C49

convert 6C49 to binary:   01101100 01001001

UTF-8是把character转为binary code的规范, and vice versa. 方便存储。
|   binary |          |          |          |                     |                                   |
| 1st Byte | 2nd Byte | 3rd Byte | 4th Byte | Number of Free Bits | Maximum Expressible Unicode Value |
| 0xxxxxxx |          |          |          | 7                   | 007F hex (127)                    |
| 110xxxxx | 10xxxxxx |          |          | (5+6)=11            | 07FF hex (2047)                   |
| 1110xxxx | 10xxxxxx | 10xxxxxx |          | (4+6+6)=16          | FFFF hex (65535)                  |
| 11110xxx | 10xxxxxx | 10xxxxxx | 10xxxxxx | (3+6+6+6)=21        | 10FFFF hex (1,114,111)            |

已知“严”的unicode是4E25（100111000100101），根据上表，可以发现4E25处在第三行的范围内（0000 0800-0000 FFFF），因此“严”的UTF-8编码需要三个字节，即格式是“1110xxxx 10xxxxxx 10xxxxxx”。然后，从“严”的最后一个二进制位开始，依次从后向前填入格式中的x，多出的位补0。这样就得到了，“严”的UTF-8编码是“11100100 10111000 10100101”，转换成十六进制就是E4B8A5。

You can use a different encoding from UTF-8 by putting a specially-formatted comment as the first or second line of the source code:

# -*- coding: <encoding name> -*-

*是为了让解释器在执行该文件的时候知道该文件是何种编码方式，从而顺利读取指令去执行计算。*
- division
新的除法特性，本来的除号`/`对于分子分母是整数的情况会取整，但新特性中在此情况下的除法不会取整，取整的使用`//`。如下可见，只有分子分母都是整数时结果不同。
- print_function
新的print是一个函数，如果导入此特性，之前的print语句就不能用了。
- unicode_literals
这个是对字符串使用unicode字符

** gc — Garbage Collector interface
** inspect — Inspect live objects
- find the folder of a module:
#+BEGIN_SRC python
import inspect
inspect.getfile(Module)
#+END_SRC
** site — Site-specific configuration hook
** fpectl — Floating point exception control
* Custom Python Interpreters
** code — Interpreter base classes
** codeop — Compile Python code
* Importing Modules
** Module
- reload a module/lib in ipython without killing it.
#+BEGIN_SRC python
# For Python 2 use built-in function reload():

reload(module)
# For Python 2 and 3.2–3.3 use reload from module imp:

import imp
imp.reload(module)
# or
import importlib
importlib.reload(module)
#+END_SRC
- 当你运行一个Python模块 python fibo.py <arguments>
  Remember, everything in python is an object.

- python解释器CPython.

- 如果字符串里面有’\’,而实际上要把斜杠加到字符串里面，要在前面加r，代表raw. 例如r’Y:\codes’.

- 如果主目录下有子目录packages，记得要在子目录下加init.py，最好在主目录下建main.py函数。

- 解释器如果执行哪个一个文件为主程序，如 python program1.py，it will set the special variable name to program1.py equals to ‘main’
- One of the reasons for doing this is that sometimes you write a module (a .py file) where it can be executed directly. Alternatively, it can also be imported and used in another module. By doing the main check, you can have that code only execute when you want to run the module as a program and not have it execute when someone just wants to import your module and call your functions themselves.

- 模块中的代码将会被执行，就像导入它一样，不过此时name 被设置为 “main“。这意味着，通过在你的模块末尾添加此代码.

- can’t import module from upper directory.

    - need to add the working directory to .bashrc PYTHONPATH
    - using ipython.
- Add custom folder path to the Windows environment.
  add PYTHONEXE%; to System Variable PATH;
  add System variable name: PYTHONEXE , value: C:\Users\Wei Wu\Anaconda2;C:\Users\Wei Wu\Python\ylib\src\py\;
  add PYTHONPATH:  C:\Users\Wei Wu\Anaconda2;C:\Users\Wei Wu\Python\ylib\src\py\;
  or add module path in Spyder directly;
- import module temperarily from parental directory without add path to the system.
#+begin_src python :tangle yes
# folder1
#    \__init__.py
#    \State.py
#    \StateMachine.py
#    \mouse_folder
#        \MouseAction.py
import os,sys,inspect
currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))
parentdir = os.path.dirname(currentdir)
sys.path.insert(0,parentdir+'\\mouse')
sys.path.insert(0,parentdir)

from State import State
from StateMachine import StateMachine
from MouseAction import MouseAction
# or
if os.path.split(os.path.abspath(__file__))[0] != os.getcwd():
    from .graph import Graph, get_shortest_paths_among_nodes, complement_paths,\
        get_nodes_from_paths, get_edges_from_paths_on_graph
    from .utils import convert_string, FastIndexing, get_topn_values
else:
    from graph import Graph, get_shortest_paths_among_nodes, complement_paths,\
# or
import sys
from pathlib import Path
sys.path.append(str(Path('.').absolute().parent))
#+end_src

- move pyc to root directory:
#+BEGIN_SRC python
import os
import shutil
for root, dirs, files in os.walk(cwd):
    print(root, "consumes, ")
    for file in files:
        if file.endswith('.pyc'):
            names = file.split('.')
            shutil.move(join(root, file), join(os.path.dirname(root),names[0]+'.'+names[2]))
#+END_SRC

- check module path:
#+begin_src python
import os
print os.path.abspath(ylib.__file__)
#+end_src

- make a python 3 virtual environment:
#+begin_src sh
mkvirtual -p python3 ENVNAME
#+end_src

- install setup.py:
python setup.py install
to virtual environment:
/home/weiwu/.virtualenvs/data_analysis/bin/python2 setup.py install

- install from github:
pip install git+https://github.com/quantopian/zipline.git
#+BEGIN_SRC bash
conda uninstall tqdm
easy_install git+https://github.com/quantopian/zipline.git
#+END_SRC

- change conda source/configuration:
#+BEGIN_SRC shell
vim ~/.condarc
channels:
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/
show_channel_urls: true

# or
conda config --add channels 'https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/'
conda config --set show_channel_urls yes
#+END_SRC

- easy_install multiple versions, remove version:
#+BEGIN_SRC python
import pkg_resources
pkg_resources.require("gensim")  # latest installed version
pkg_resources.require("gensim==3.7.2")  # this exact version
pkg_resources.require("gensim>=3.7.2")  # this version or higher

#+END_SRC
- Removing an environment
To remove an environment, in your terminal window or an Anaconda Prompt, run:
#+BEGIN_SRC bash
conda remove --name myenv --all
# You may instead use conda env remove --name myenv.
# To verify that the environment was removed, in your terminal window or an Anaconda Prompt, run:
conda info --envs
#+END_SRC

- create conda virtualenv:
#+BEGIN_SRC shell
# 创建 conda 虚拟环境（ :code:`env_name` 是您希望创建的虚拟环境名）
$ conda create --name env_name python=3.5

# 如您想创建一个名为rqalpha的虚拟环境
$ conda create --name rqalpha python=3.5

# 使用 conda 虚拟环境
$ source activate env_name
# 如果是 Windows 环境下 直接执行 activcate
$ activate env_name

# 退出 conda 虚拟环境
$ source deactivate env_name
# 如果是 Windows 环境下 直接执行 deactivate
$ deactivate env_name

# 删除 conda 虚拟环境
$ conda-env remove --name env_name

# add conda for all users
sudo ln -s /share/anaconda3/etc/profile.d/conda.sh /etc/profile.d/conda.sh
# Previous to conda 4.4, the recommended way to activate conda was to modify PATH in
# your ~/.bashrc file.  You should manually remove the line that looks like

    export PATH="/share/anaconda3/bin:$PATH"

# ^^^ The above line should NO LONGER be in your ~/.bashrc file! ^^^

#+END_SRC

- percentage output format:
#+begin_src python
from future import division
print “%s %.4f%%” % (sid, (len(not_close)/len(ctp)))
#+end_src
** zipimport — Import modules from Zip archives
** pkgutil — Package extension utility
** modulefinder — Find modules used by a script
** runpy — Locating and executing Python modules
** importlib — The implementation of import
* Python Language Services
** parser — Access Python parse trees
** ast — Abstract Syntax Trees
** symtable — Access to the compiler’s symbol tables
** symbol — Constants used with Python parse trees
** token — Constants used with Python parse trees
** keyword — Testing for Python keywords
** tokenize — Tokenizer for Python source
** tabnanny — Detection of ambiguous indentation
** pyclbr — Python class browser support
** py_compile — Compile Python source files
** compileall — Byte-compile Python libraries
** dis — Disassembler for Python bytecode
** pickletools — Tools for pickle developers
* Miscellaneous Services
** formatter — Generic output formatting
* MS Windows Specific Services
** msilib — Read and write Microsoft Installer files
** msvcrt — Useful routines from the MS VC++ runtime
** winreg — Windows registry access
** winsound — Sound-playing interface for Windows
* Unix Specific Services
** posix — The most common POSIX system calls
** pwd — The password database
** spwd — The shadow password database
** grp — The group database
** crypt — Function to check Unix passwords
** termios — POSIX style tty control
** tty — Terminal control functions
** pty — Pseudo-terminal utilities
** fcntl — The fcntl and ioctl system calls
** pipes — Interface to shell pipelines
** resource — Resource usage information
** nis — Interface to Sun’s NIS (Yellow Pages)
** syslog — Unix syslog library routines
* Superseded Modules
** optparse — Parser for command line options
** imp — Access the import internals
* Undocumented Modules
** Platform specific modules
** call java service
#+BEGIN_SRC python
import subprocess
try:
    subprocess.call(["java",
                     "-jar", grobid_jar,
                     # Avoid OutOfMemoryException
                     "-Xmx1024m",
                     "-gH", grobid_home,
                     "-gP", os.path.join(grobid_home,
                                         "config/grobid.properties"),
                     "-dIn", pdf_folder,
                     "-exe", "processReferences"])
    return True
except subprocess.CalledProcessError:
    return False

#+END_SRC
* Data Analysis:
** pandas:
[[file:./pandas.org][advanced pandas]]
- apply to a dataframe is more efficient than access to index and change value.

- set a column to tuple:
#+BEGIN_SRC python
pd_tmp = pd.DataFrame(np.random.rand(3,3))
# pd_tmp["new_column"] = ("a",2)
# ValueError: Length of values does not match length of index

pd_tmp.assign(newc_olumn=pd_tmp.apply(lambda x: ('a', 2), 1))
#+END_SRC

- select index without some values in the index:
#+BEGIN_SRC python
df_entropy.loc[~df_entropy.index.isin(df_risky_sample_count.index)]
#+END_SRC

- scientific annotation in float:
#+BEGIN_SRC python
pd.set_option('display.float_format', lambda x: '%.4f' % x)
#+END_SRC

- Find the column name which has the maximum value for each row
#+BEGIN_SRC python
df.idxmax(axis=1)
#+END_SRC

- change jupyter view:
#+BEGIN_SRC python
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)
#+END_SRC

- add new columns to a dataframe:
#+BEGIN_SRC python
>>> df = pd.DataFrame([[i] for i in range(10)], columns=['num'])
>>> df
    num
0    0
1    1
2    2
3    3

>>> def powers(x):
>>>     return x, x**2, x**3, x**4, x**5, x**6

>>> df['p1'], df['p2'], df['p3'], df['p4'], df['p5'], df['p6'] = \
>>>     zip(*df['num'].map(powers))

#+END_SRC

- first day of the previous month:
#+BEGIN_SRC python
today = datetime.datetime.today()
the_last_day_of_previous_month = today.datetime.timedelta(days=1)
the_first_day_of_previous_month = the_last_day_of_previous_month.replace(day=1)
#+END_SRC

- N month before:
#+BEGIN_SRC python
today - pd.Timedelta(1, unit='M')
#+END_SRC

- set a list to a location in dataframe ValueError: Must have equal len keys and value when setting with an iterable
#+BEGIN_SRC python
df = pd.DataFrame(data={'A': [1, 2, 3], 'B': ['x', None, 'z']})
df.at[1, 'B'] = ['m', 'n']
# or
df.set_value(1, 'B', abc)
#+END_SRC

- replace nan with None fillna:
#+BEGIN_SRC python
df1 = df.where((pd.notnull(df)), None)
#+END_SRC

- insert a column at the front of a dataframe:
#+BEGIN_SRC python
df.insert(loc=0, column='项目是否一样', value=list)
#+END_SRC

- sort all the columns:
#+BEGIN_SRC python
df.sort_values(df.columns.tolist())
#+END_SRC

- pandas to list of tuples:
#+BEGIN_SRC python
list(zip(df_top_p['person_id'], df_top_p['hdd']))
#+END_SRC

- pandas columns into default dictionary:
#+BEGIN_SRC python
df_patient[['pid','med_clinic_id']].groupby('pid').apply(lambda x:x['med_clinic_id'].tolist()).to_dict()
#+END_SRC

- groupby order:
#+BEGIN_SRC python
# select the first row in each group, group have multindex.
df_item.groupby(level=0, group_keys=False).apply(lambda x: x.sort_values('abs_diff', ascending=False)).groupby(level=0).head(3)

# select rows by person id and icd 3, not only person id
df_most_visited_hospitals.sort_values(['PERSON_ID','DISCHARGE_DATE'],ascending=True).groupby(['PERSON_ID', 'ICD3']).first()
#+END_SRC

- multiindex join on level:
#+BEGIN_SRC python
df1.join(df).sort_index(level=0, axis=1)
#+END_SRC

- group by value counts contraint:
#+BEGIN_SRC python
df.groupby(['out_diag_doc_cd'])['in_hosp_date_date'].value_counts().loc[lambda x : x>1]
#+END_SRC

- filter by groupby count size:
#+BEGIN_SRC python
df_patient = df_patient.groupby(['hos_id', 'disease']).filter(lambda x:x['person_id'].unique().size>=3)
#+END_SRC

- apply to each group in groupby:
#+BEGIN_SRC python
for group in df2.groupby('type'):
    print(group_id, group[0])
    data = group[1]
    if group[0] == 'A':
        print group[1].min()
#+END_SRC

- groupby filter by quantile:

#+BEGIN_SRC python
df_query[df_query.groupby("entity")['entity_sim'].transform(lambda x : (x<(x.quantile(0.1)))).eq(1)]
#+END_SRC

- delete a subset from a list:
#+BEGIN_SRC python
def multidelete(values, todelete):
    todelete=np.array(todelete)
    shift=np.triu((todelete>=todelete[:,None]),1).sum(0)
    return np.delete(values,todelete+shift)
# or 
list_1 = ["a", "b"]
list_2 = ["a", "b", "c"]

for element in list_1:
    if element in list_2:
        list_2.remove(element)
#+END_SRC

- groupby function apply order:
#+BEGIN_SRC python
from numpy.random import randint
df = pd.DataFrame({'B': randint(10, size=10)})
df['sort=False'] = df.groupby('B', sort=False).ngroup()
df['sort=True'] = df.groupby('B', sort=True).ngroup()

def groupbyngroup(self, ascending=True):
    """
    Number each group from 0 to the number of groups - 1.
    This is the enumerative complement of cumcount.  Note that the
    numbers given to the groups match the order in which the groups
    would be seen when iterating over the groupby object, not the
    order they are first observed.
    ""
#+END_SRC

#+RESULT:
: In [9]: df
: Out[9]:
:    A  B  sort=False  sort=True
: 0  c  7           0          3
: 1  b  0           1          0
: 2  c  9           2          4
: 3  c  7           0          3
: 4  a  3           3          2
: 5  c  2           4          1
: 6  a  3           3          2
: 7  c  9           2          4
: 8  c  2           4          1
: 9  a  0           1          0

- groupby get the index name:
#+BEGIN_SRC python
from numpy.random import randint
df = pd.DataFrame({'B': randint(10, size=10)})
df.apply(lambda x: x.name)
#+END_SRC


- groupby qcut:
#+BEGIN_SRC python
df = pd.DataFrame({'A':'foo foo foo bar bar bar'.split(),
                   'B':[0.1, 0.5, 1.0]*2})

df['C'] = df.groupby(['A'])['B'].transform(
                     lambda x: pd.qcut(x, 3, labels=range(1,4)))
print(df)
# if error ValueError: Length mismatch: Expected axis has 5564 elements, new values have 78421 elements
# Groupby does not group the NaNs:
df_patient['disease_code'].fillna('unk', inplace=True)
#+END_SRC

- re-index:
#+BEGIN_SRC python
df_his_data = df_his_data.reset_index().drop('index', axis=1)
#+END_SRC

- average times groupby:
#+BEGIN_SRC python
df_patient.reset_index().groupby('disease_code').apply(lambda x:x['med_clinic_id'].count()/x['person_id'].nunique())
#+END_SRC
- ValueError: Bin edges must be unique:
#+BEGIN_SRC python
pd.qcut(df['column'].rank(method='first'), nbins)
#+END_SRC

- pd.to_datetimeindex error:
#+BEGIN_SRC python

#+END_SRC

- numpy value counts frequency:
#+BEGIN_SRC python
unique, counts = np.unique(continueGame, return_counts=True)
np.asarray((unique, counts)).T
#+END_SRC

- add multiple empty columns:
#+BEGIN_SRC python
mydf = mydf.reindex(columns = mydf.columns.tolist() + ['newcol1','newcol2'])
#+END_SRC

- return multiple columns when apply:
#+BEGIN_SRC python
def trans(x):
    queries = [x['med_name']]
    sim_indices, sim_values = tm.get_most_similarities(queries, topn=1)
    sim_docs = standard_terminologies[sim_indices]
    x['name'] = standard_terminologies[sim_indices][0][0]
    x['sim_val'] = sim_values[0][0]
    x['pay_pro'] = df_name_dict.iloc[sim_indices.tolist()[0]]['SELF_PAY_PRO'].values[0]
    return x
df_patient.apply(lambda x:trans(x), axis=1)
#+END_SRC

- apply to a column for each row:
#+BEGIN_SRC python
df = pd.DataFrame([[1,2,3],[4,5,6]], columns=['a','b','c'])
def rowFunc(row):
    return row['a'] + row['b'] * row['c']

def rowIndex(row):
    return row.name
df['d'] = df.apply(rowFunc, axis=1)
df['rowIndex'] = df.apply(rowIndex, axis=1)
df
Out[182]:
   a  b  c   d  rowIndex
0  1  2  3   7         0
1  4  5  6  34         1
#+END_SRC
- permutation of an array, combination:
#+BEGIN_SRC python
person_id = df_disease_sample['person_id'].unique()
from itertools import permutations
perm = permutations(person_id, 2)
df_person_similarity_adj_matrix = pd.DataFrame(index=perm, columns=['similarity'])
df_person_similarity_adj_matrix
#+END_SRC
- remove value in pandas index:
#+BEGIN_SRC python
index.drop('value')
#+END_SRC
- select rows with value in multiple columns:
#+BEGIN_SRC python
df_suspicious_pairs[df_suspicious_pairs[['person_id_1', 'person_id_2']].isin(['11076976']).any(axis=1)]
def add_edges(G, row, df_suspicious_pairs):
    person_id = row.index
    df_targets = df_suspicious_pairs[df_suspicious_pairs[['person_id_1', 'person_id_2']].isin(person_id).any(axis=1)]
    G.add_edges_from([tuple(x) for x in df_targets[['person_id_1', 'person_id_2']].values])


df_suspicious_person.progress_apply(lambda x: add_edges(G, x, df_suspicious_pairs))
#+END_SRC
- delete rows that contain string value in a column:
#+BEGIN_SRC python
df_disease_sample[~df_disease_sample['treatment_code'].str.contains('DE')]
#+END_SRC
- pandas values to dict:
#+BEGIN_SRC python
med_hos_id_mapping.set_index('med_clinic_id')['hos_id'].to_dict()
#+END_SRC
- create dataframe from a list of tuples:
#+BEGIN_SRC python
pd.DataFrame.from_records([tuples])
#+END_SRC

- find value with multiindex:
#+BEGIN_SRC python
arrays = pd.MultiIndex.from_arrays(idx_map_vi_levels.values.T, names=[1,2,3,4,5])
for item in ls_items:
    index = [vals for x, vals in enumerate(arrays) if vals[-1]==item or vals[-2]==item]

df_temp_order = pd.DataFrame()
ls_lvs = []
levels = [0, 1, 2, 3, 4]
for l in levels:
    s_temp = df_temp.groupby(level=l).sum()
    idx_lv1 = s_temp[s_temp['cost']>100].index
    drop_lv1 = [vals for x, vals in enumerate(mul_arrays) if vals[l] in s_temp[s_temp['cost']<=100].index]
    df_temp = df_temp.drop(index=drop_lv1)
    if len(df_temp) == len(idx_lv1):
        print('not enough depth')
        ls_lvs.append(s_temp[s_temp['cost']>100].index)
        break
    else:
        ls_lvs.append(s_temp[s_temp['cost']<=100].index)
#+END_SRC

- create multiple index for index:
#+BEGIN_SRC python
similarity_rank.index = pd.MultiIndex.from_tuples(similarity_rank.index)
#+END_SRC

- groupby, transform, agg:
#+BEGIN_SRC python
df = pd.DataFrame(dict(A=list('aabb'), B=[1, 2, 3, 4], C=[0, 9, 0, 9]))
# groupby is the standard use aggregater
df.groupby('A').mean()
# maybe you want these values broadcast across the whole group and return something with the same index as what you started with.
# use transform

df.groupby('A').transform('mean')
# is equalvilent to groupby("A").mean()
df.set_index('A').groupby(level='A').transform('mean')
# set column equal to groupby mean
df_item_dis_lv['mean'] = df_item_dis_lv.groupby(['name', 'dis', 'JGDJ'])['MODEL_JE'].transform('mean')
# agg is used when you have specific things you want to run for different columns or more than one thing run on the same column.

df.groupby('A').agg(['mean', 'std'])


#+END_SRC
- groupby generate new columns:
#+BEGIN_SRC python
df_result_summary = df_result_detail.groupby(
    ['risk_grp_no', 'risk_grp_name', 'fqz_type', 'tel', 'resd_regn']
).agg(
    sumamt=pd.NamedAgg('sumamt', 'sum'),
    hifp_pay=pd.NamedAgg('hifp_pay', 'sum'),
    acct_pay=pd.NamedAgg('acct_pay', 'sum'),
    grp_psncnt=pd.NamedAgg(d_col['person_id'], 'nunique'),
    mdtrt_psncnt=pd.NamedAgg(d_col['medical_id'], 'nunique'),
    maw_ipt_cnt=pd.NamedAgg(d_col['medical_id'], 'nunique'),
    mdtrt_cnt=pd.NamedAgg(d_col['medical_id'], 'count'))
df_result_summary.reset_index(inplace=True)
#+END_SRC

- groupby aggregate to list:
#+BEGIN_SRC python
list_int = df.groupby('a')['b'].apply(list)
for y in list_int:
    string = ','.join(str(x) for x in y)
#+END_SRC

- read clickhouse:
#+BEGIN_SRC python
result, columns = clickhouse_conn.execute(sql,
                                 with_column_types=True)
df = pd.DataFrame(result, columns=[tuple[0] for tuple in columns])
df.head()
#+END_SRC
- create clickhouse table:
#+BEGIN_SRC python
create table wuxi_ads_grouprisk.risk_zy_jtzy_table3 ENGINE=MergeTree() ORDER BY tuple() SETTINGS index_granularity = 8192 as select 1
#+END_SRC

- insert into clickhouse:
#+BEGIN_SRC python
client.execute("INSERT INTO your_table VALUES", df.to_dict('records'))
#+END_SRC

- bulk insert:
#+BEGIN_SRC python
from sqlalchemy import create_engine
from sqlalchemy import MetaData
from sqlalchemy.orm import sessionmaker

conn = engine.connect()
# Open the session
Session = sessionmaker(bind=engine)
session = Session()
# Create MetaData instance
metadata = MetaData()
# reflect db schema to MetaData
metadata.reflect(bind=engine)
# Get Table
table_name = 't_kc21'
ex_table = metadata.tables[table_name]
print(ex_table)

listToWrite = df_21.to_dict(orient='records')
conn.execute(ex_table.insert(), listToWrite)
#+END_SRC

- read oracle database UnicodeDecodeError 'gb' :
set the language of the editor from Chinese to English.
#+BEGIN_SRC python
import cx_Oracle as cx
import pandas as pd
import numpy as np
import os
from tqdm import *
import os
from sqlalchemy import create_engine

os.environ['NLS_LANG'] = 'SIMPLIFIED CHINESE_CHINA.UTF8'

engine = create_engine('oracle://MMAPV41:MMAPV411556@192.168.4.32:1521/orcl?charset=utf8')

conn=cx.connect('MMAPV41/MMAPV411556@192.168.4.32/orcl')
sql_regist = """
select med_clinic_id, person_id, person_nm, person_sex,
person_age, in_hosp_date, out_hosp_date,
med_ser_org_no, clinic_type, in_diag_dis_nm, out_diag_doc_cd,
med_amout, hosp_lev from t_kc21
"""
df_regist = pd.read_sql_query(sql_regist, engine)

s_med_clinic_id = pd.read_pickle('med_clinic_id.pkl')
n = 100
sql_regist = """
select med_clinic_id, person_id, person_nm, person_sex,
person_age, in_hosp_date, out_hosp_date,
med_ser_org_no, clinic_type, in_diag_dis_nm, out_diag_doc_cd,
med_amout, hosp_lev from t_kc21 where med_clinic_id in (%s)
"""
df_regist = pd.DataFrame()
for i in tqdm(range(0, int(len(s_med_clinic_id)/100), n)):
    s = "'"+','.join(s_med_clinic_id.ix[i:(i+n)].values.flatten()).replace(',',"','")+"'"
    sql = sql_regist%(s)
    try:
        df_regist_iter = pd.read_sql(sql, conn)
        df_regist = df_regist.append(df_regist_iter)
    except UnicodeDecodeError:
        continue
df_regist.to_pickle("registration_data.pkl")
#+END_SRC

- filter rows by number, groupby filter:
#+BEGIN_SRC python
df_item = t_order.groupby(['name']).filter(lambda x:x['hos_id'].unique().size>=10)
#+END_SRC

- resample groupby aggregate:
#+BEGIN_SRC python
df_simul_sample = df_simul_sample.resample('1H')['PERSON_ID'].agg(list)
#+END_SRC

- convert columns from capital to lower:
#+BEGIN_SRC python
df_patient = df_patient.rename(columns = lambda x: x.lower())
#+END_SRC

- find rows with nearest dates/value:
#+BEGIN_SRC python
df_result = pd.DataFrame()
for idx, value in enumerate(groups):
    df_target_group = df_patient.loc[value]
    df_target_group.sort_values('入院日期', inplace=True)
    df_target_group['checkin_diff'] = df_target_group['入院日期'].diff()/np.timedelta64(1, 'D')
    df_target_group.reset_index(inplace=True)
    index = df_target_group[df_target_group['checkin_diff']<=3].index
    result = pd.concat([df_target_group.loc[index], df_target_group.loc[index-1]]).sort_values('入院日期')
    result.drop_duplicates(['个人ID','入院日期'], inplace=True)
    result['group'] = idx
    result['hospitals'] = result['机构'].unique().shape[0]
    df_result = df_result.append(result.drop('checkin_diff',axis=1))
#+END_SRC

- save to excel sheet:
#+BEGIN_SRC python
writer = pd.ExcelWriter('nanjing_result.xlsx')
df_nanjing.to_excel(writer,'全部分组')
big_groups.to_excel(writer,'超大组')
writer.save()
#+END_SRC

- append dataframe at the end of an excel sheet:
#+BEGIN_SRC python
if os.path.isfile(path):
    #writer = pd.ExcelWriter(path, engine='openpyxl')
    startrow = writer.sheets['Sheet1'].max_row
    print('max sheet row number', startrow)
    patterns.to_excel(writer, sheet_name='Sheet1', startrow=startrow, index = False, header= False)
    writer.save()
else:
    writer = pd.ExcelWriter(path)
    patterns.to_excel(writer, sheet_name='Sheet1', index=False)
    writer.save()

#+END_SRC

- add extra sheet to excel file:
#+BEGIN_SRC python
import pandas as pd
import numpy as np
from openpyxl import load_workbook

path = r"C:\Users\fedel\Desktop\excelData\PhD_data.xlsx"

book = load_workbook(path)
writer = pd.ExcelWriter(path, engine = 'openpyxl')
writer.book = book

x3 = np.random.randn(100, 2)
df3 = pd.DataFrame(x3)

x4 = np.random.randn(100, 2)
df4 = pd.DataFrame(x4)

df3.to_excel(writer, sheet_name = 'x3')
df4.to_excel(writer, sheet_name = 'x4')
writer.save()
writer.close()
#+END_SRC

- concate multiple columns into one column:
#+BEGIN_SRC python
diff_checkout_next_checkin['date'] = dataframe.loc[index, columns].stack().sort_values()
diff_checkout_next_checkin['diff'] = diff_checkout_next_checkin['date'].diff()/np.timedelta64(1, 'M')

#+END_SRC
- sort values:
#+BEGIN_SRC python
df_nanjing.sort_values(['group_sn', '个人ID', '入院日期'], inplace=True)
#+END_SRC
- read excel:
#+BEGIN_SRC python
df_nanjing = pd.read_excel('result.xlsx',dtype={'证件号':str,
                                                '个人ID':str},
                         parse_dates=['入院日期','出院日期'])
#+END_SRC

- groupby ratio:
#+BEGIN_SRC python
df_items_sum = df_items.groupby(['disease_code', 'soc_srt_dire_nm']).agg({'amount': 'sum'})
# Change: groupby state_office and divide by sum
df_items_ratio = df_items_sum.groupby(level=0).apply(lambda x:
                                                 100 * x / float(x.sum()))
#+END_SRC
- groupby group size:
#+BEGIN_SRC python
group_size = df_nanjing.groupby(['group_sn'])['个人ID'].unique().apply(len)
big_groups = df_nanjing[df_nanjing['group_sn'].isin(group_size[group_size >= 8].index)]
small_groups = df_nanjing[df_nanjing['group_sn'].isin(group_size[group_size < 8].index)]
#+END_SRC
- create a dataframe from a dictionary:
#+BEGIN_SRC python
# from a dictionary of dictionaries
df = pd.DataFrame.from_dict({}, orient='index')
#+END_SRC

-filter by value counts:
#+BEGIN_SRC python
df_person_count = df_simul['PERSON_ID'].value_counts()
# filter person id count is less than 3 times
df_simul = df_simul[df_simul['PERSON_ID'].isin(df_person_count[df_person_count>3].index)]

#+END_SRC
- get max value counts column name in each group:
#+BEGIN_SRC python
df_sample1 = df_sample.sort_values(['person_id','discharge_date'],ascending=True).groupby(['person_id', 'icd3']).first().agg({'hosp_lev':pd.Series.value_counts})
df_sample1.groupby(['person_id']).sum().idxmax(axis=1)
#+END_SRC

- groupby value counts max count name:
#+BEGIN_SRC python
s_count = df_patient.groupby('disease_code')['mon'].value_counts()
s_count.name = 'cnt_mon'
df_disease['cnt_mon'] = s_count.reset_index().pivot(index='disease_code', columns='mon', values='cnt_mon').idxmax(axis=1)
#+END_SRC

- get the index of max value in 2d array:
#+BEGIN_SRC python
np.unravel_index(np.argmax(sim_indices, axis=None), sim_indices.shape)
#+END_SRC

- get unique value counts within each group:
#+BEGIN_SRC python
import pandas as pd
df = pd.DataFrame({'date': ['2013-04-01','2013-04-01','2013-04-01','2013-04-02', '2013-04-02'],
    'user_id': ['0001', '0001', '0002', '0002', '0002'],
    'duration': [30, 15, 20, 15, 30]})
df.groupby('date').agg({'user_id':pd.Series.nunique})
df_items = df_patient.groupby(['hos_id', d_v4_columns['medical_id']], as_index=True)[d_v4_columns['soc_med_name']].aggregate(lambda x: set([y for y in x if y is not None]))
#+END_SRC
- get unique columns value groups:
#+BEGIN_SRC python
rels = ['疾病名称','诊疗大类2']
rels_cure = df_cure.groupby(rels).size().reset_index()[rels].values.tolist()
#+END_SRC
- fill value must be in categories:
#+BEGIN_SRC python

#+END_SRC
- fill na with previous column:
#+BEGIN_SRC python
df_tree.fillna(method='pad', axis=1, inplace=True)
#+END_SRC
- delete a column in a dataframe:
#+BEGIN_SRC python
del df['column']
#+END_SRC
- create node edges
#+BEGIN_SRC python
def create_node(label, nodes):
    count = 0
    for node_name in tqdm(nodes):
        node = Node(label, name=node_name)
        # g.schema.create_uniqueness_constraint(label, node_name)
        try:
            g.create(node)
            count += 1
        except ClientError:
            continue
        # debug(count)
    return


'''创建实体关联边'''
def create_relationship(start_node, end_node, edges, rel_type, rel_name):
    count = 0
    # 去重处理
    set_edges = []
    for edge in edges:
        try:
            set_edges.append('###'.join(edge))
        except TypeError:
            continue
    all = len(set(set_edges))
    for edge in tqdm(set(set_edges)):
        edge = edge.split('###')
        p = edge[0]
        q = edge[1]
        if p==q:
            continue
        query = "match(p:%s),(q:%s) where p.name='%s'and q.name='%s' create (p)-[rel:%s{name:'%s'}]->(q)" % (
            start_node, end_node, p, q, rel_type, rel_name)
        try:
            g.run(query)
            count += 1
            # debug(rel_type)
        except Exception as e:
            info(e)
    return

'''创建知识图谱中心疾病的节点'''
def create_diseases_nodes(disease_infos):
    count = 0
    for disease_dict in tqdm(disease_infos):
        node = Node("Disease", name=disease_dict['name'], desc=disease_dict['desc'],
                    prevent=disease_dict['prevent'] ,cause=disease_dict['cause'],
                    easy_get=disease_dict['easy_get'],cure_lasttime=disease_dict['cure_lasttime'],
                    cure_department=disease_dict['cure_department']
                    ,cure_way=disease_dict['cure_way'] , cured_prob=disease_dict['cured_prob'])
        g.create(node)
        # count += 1
        # debug(count)
    return
#+END_SRC
- select rows by column values:
#+BEGIN_SRC python
df[(df['a'].isin(condition1))&(df['a'].isin(condition2))]
df[df['a']==a]
df_insurance_disease = df_insurance[(df_insurance['type']=='医疗费用-疾病') & (df_insurance['year'].isin(years[4:]))]

#+END_SRC
- create tuples or dictionary from two columns:
#+BEGIN_SRC python
subset = hos_ids[['GHDJID', 'REAL_JE', 'hos_id']].reset_index()
tuples = [tuple(x) for x in subset.values]
# another method
dict(df[['a', 'b']].values.tolist())
[tuple(x) for x in df[['a', 'b', 'c']].values]
#+END_SRC
- mapping:
#+BEGIN_SRC python
dictionary = df_mapping.set_index('a')['b'].to_dict()
df['a'].map(dict)
#+END_SRC
- filter not null, filter not NaT rows:
since strings data types have variable length, it is by default stored as object dtype. If you want to store them as string type, you can do something like this.
#+BEGIN_SRC python
df_text.ix[df_text.Conclusion.values.nonzero()[0]]
df['column'] = df['column'].astype('|S80') #where the max length is set at 80 bytes,
# or alternatively

df['column'] = df['column'].astype('|S') # which will by default set the length to the max len it encounters
#+END_SRC
- change datatypes
#+BEGIN_SRC python
df.a.astype(float)
drinks['beer_servings'] = drinks.beer_servings.astype(float)
#+END_SRC
- change date type to string:
#+BEGIN_SRC python
drinks['beer_servings'] = drinks.beer_servings.astype('str)

#+END_SRC
- read csv error:
encoding="iso-8859-1"
encoding='utf-8'
encoding='gbk'

- read csv without header, delimiter is space:
#+BEGIN_SRC python
vocab = pd.read_csv("/home/weiwu/share/deep_learning/data/model/phrase/zhwiki/categories/三板.vocab",delim_whitespace=True,header=None)
#+END_SRC
- parse text in csv
#+BEGIN_SRC python
reddit_news = pd.read_csv('/home/weiwu/share/deep_learning/data/RedditNews.csv')
DJA_news = pd.read_csv(
    '/home/weiwu/share/deep_learning/data/Combined_News_DJIA.csv')
na_str_DJA_news = DJA_news.iloc[:, 2:].values
na_str_DJA_news = na_str_DJA_news.flatten()
na_str_reddit_news = reddit_news.News.values
sentences_reddit = [s.encode('utf-8').split() for s in na_str_reddit_news]
sentences_DJA = [s.encode('utf-8').split() for s in na_str_DJA_news]
#+END_SRC
- rank
DataFrame.rank(axis=0, method='average', numeric_only=None, na_option='keep', ascending=True, pct=False)[source]
Compute numerical data ranks (1 through n) along axis. Equal values are assigned a rank that is the average of the ranks of those values

- n largest value
DataFrame.nlargest(n, columns, keep='first')
Get the rows of a DataFrame sorted by the n largest values of columns.

#+BEGIN_SRC python
>>> df = DataFrame({'a': [1, 10, 8, 11, -1],
...                 'b': list('abdce'),
...                 'c': [1.0, 2.0, np.nan, 3.0, 4.0]})
>>> df.nlargest(3, 'a')
    a  b   c
3  11  c   3
1  10  b   2
2   8  d NaN

#+END_SRC
- quantile
DataFrame.quantile(q=0.5, axis=0, numeric_only=True, interpolation='linear')[source]
Return values at the given quantile over requested axis, a la numpy.percentile.
#+BEGIN_SRC python
>>> df = DataFrame(np.array([[1, 1], [2, 10], [3, 100], [4, 100]]),
                   columns=['a', 'b'])
>>> df.quantile(.1)
a    1.3
b    3.7
dtype: float64
>>> df.quantile([.1, .5])
       a     b
0.1  1.3   3.7
0.5  2.5  55.0
#+END_SRC

- generate a dataframe:
#+begin_src python
dates = pd.date_range('1/1/2000', periods=8)
df = pd.DataFrame(np.random.randn(8, 4), index=dates, columns=['A', 'B', 'C', 'D'])
# or
df = pd.DataFrame(data={'a':[1,2],'b':[3,3]})
#+end_src

- create diagonal matrix/dataframe using a series:
#+BEGIN_SRC python
df = pd.DataFrame(np.diag(s), columns=Q.index)
#+END_SRC

- plsql switch schema:
#+BEGIN_SRC python
sql_switch_schema = """alter session set current_schema=jxnc"""
cursor.execute(sql_switch_schema)
#+END_SRC

- connection with mysql:
#+begin_src python
pandas.read_sql_query(sql, con=engine):
pandas.read_sql_table(table_name, con=engine):
pandas.read_sql(sql, con=engine)
sql = 'DROP TABLE IF EXISTS etf_daily_price;'
result = engine.execute(sql)
#+end_src
- dropna:
#+BEGIN_SRC python
DataFrame.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)

#+END_SRC
- melt.
pandas.melt(frame, id_vars=None, value_vars=None, var_name=None, value_name='value', col_level=None)[source]

This function is useful to massage a DataFrame into a format where one or more columns are identifier variables (id_vars), while all other columns, considered measured variables (value_vars), are “unpivoted” to the row axis, leaving just two non-identifier columns, ‘variable’ and ‘value’.
#+BEGIN_SRC python
"""
Parameters:
frame : DataFrame
id_vars : tuple, list, or ndarray, optional
Column(s) to use as identifier variables.
value_vars : tuple, list, or ndarray, optional
Column(s) to unpivot. If not specified, uses all columns that are not set as id_vars.
var_name : scalar
Name to use for the ‘variable’ column. If None it uses frame.columns.name or ‘variable’.
value_name : scalar, default ‘value’
Name to use for the ‘value’ column.
col_level : int or string, optional
If columns are a MultiIndex then use this level to melt.
"""
DataFrame['idname'] = DataFrame.index
pd.melt(DataFrame, id_vars=['idname'])
#+END_SRC
#+RESULT:
: >>> import pandas as pd
: >>> df = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c'},
: ...                    'B': {0: 1, 1: 3, 2: 5},
: ...                    'C': {0: 2, 1: 4, 2: 6}})
: >>> df
:    A  B  C
: 0  a  1  2
: 1  b  3  4
: 2  c  5  6
: >>> pd.melt(df, id_vars=['A'], value_vars=['B'])
:    A variable  value
: 0  a        B      1
: 1  b        B      3
: 2  c        B      5

- fill nan:
#+BEGIN_SRC python
DataFrame.fillna(value=None, method=None, axis=None, inplace=False, limit=None, downcast=None, **kwargs)
# method : {‘backfill’, ‘bfill’, ‘pad’, ‘ffill’, None}, default None
#+END_SRC

- select non zero rows from series:
#+BEGIN_SRC python
s[s.nonzero()]
#+END_SRC

- create value by cretics
#+BEGIN_SRC python
df[df.col1.map(lambda x: x != 0)] = 1
#+END_SRC

- dataframe to series:
#+BEGIN_SRC python
s = df[df.columns[0]]
#+END_SRC

- replace value:
#+BEGIN_SRC python
DataFrame.replace(to_replace=None, value=None, inplace=False, limit=None, regex=False, method='pad', axis=None)
df = df[df.line_race != 0]
#+END_SRC
- pandas has value:
#+BEGIN_SRC python
value in df['column_name']
set(a).issubset(df['a'])
#+END_SRC

- calculate percentage of sum on a row:
#+BEGIN_SRC python
df.apply(lambda x: x / x.sum() * 100, axis=0)
#+END_SRC

- pandas has null value:
#+BEGIN_SRC python
df.isnull().values.any()
pd.isna(v['omaha_entity1'])
#+END_SRC
- find all the values of TRUE in a dataframe:
#+begin_src python
z=(a!=b)
pd.concat([a.ix[z[reduce(lambda x, y: x | z[y], z, False)].index],b.ix[z[reduce(lambda x, y: x | z[y], z, False)].index]],axis=1)
#+end_src

- map:
#+BEGIN_SRC python
>>> numbers = [-2, -1, 0, 1, 2]

>>> abs_values = list(map(abs, numbers))
>>> abs_values
[2, 1, 0, 1, 2]

#+END_SRC

- reduce
#+BEGIN_SRC python
from functools import reduce
reduce(lambda x, y: x+y, range(1,101))
reduce(lambda x, y: set(x) | set(y), df_targets[df_targets['ratio']>0.7]['person_id'].apply(set))
#+END_SRC

- find extra items:
#+BEGIN_SRC python
def extra_item(row):
    if row['subsets'] is not None:
        if len(row) > 0:
            subsets = reduce(lambda x, y: x|y, [z[1] for z in row['subsets']])
            extra = row['itemsets'] - subsets
            return [dict_vi[x] for x in extra]
        else:
            return None
    else:
        return None
#+END_SRC

- if array a is a subset of another array b:
#+BEGIN_SRC python
set(B).issubset(set(A))
#+END_SRC
- remove negative value from a column:
#+begin_src python
filtered_1=b[‘TRADE_size’].apply(lambda x: 0 if x < 0 else x)
b[‘TRADE_size’].loc[ b[‘TRADE_size’]<0, ‘TRADE_size’] = 0
#+end_src

- drop all rows value equal to 0:
#+BEGIN_SRC python
df.loc[~(df==0).all(axis=1)]
#+END_SRC

- drop columns/lable:
#+BEGIN_SRC python
DataFrame.drop(labels, axis=1, level=None, inplace=False, errors='raise')

# delete column with all zeros:
df.loc[:, (df != 0).all(axis=0)]
#+END_SRC

- check if any value is NaN in DataFrame
#+BEGIN_SRC python
df.isnull().values.any()
df.isnull().any().any()
#+END_SRC

- maximum & minimum value of a dataframe:
#+BEGIN_SRC python
df.values.max()
df.values.min()
#+END_SRC

- select value by creteria:
#+BEGIN_SRC python
logger.debug("all weight are bigger than 0? %s", (df_opts_weight>0).all().all())
logger.debug("all weight are smaller than 1? %s", (df_opts_weight<=1).all().all())
logger.debug("weight sum smaller than 0: %s", df_opts_weight[df_opts_weight<0].sum(1))
#+END_SRC

- count all duplicates:
#+BEGIN_SRC python
import pandas as pd
In [15]: a=pd.DataFrame({'a':['KBE.US','KBE.US','KBE.US','KBE.US','KBE.US','KBE.US','O.US','O.US','O.US','O.US','O.US'],'b':['KBE','KBE','KBE','KBE','KBE','KBE','O','O','O','O','O']})

In [16]: count = a.groupby('a').count()

In [20]: (count>5).all().all()
Out[20]: False

In [21]: (count>4).all().all()
Out[21]: True

- datetime64[ns] missing data, null:
For datetime64[ns] types, NaT represents missing values. This is a pseudo-native sentinel value that can be represented by numpy in a singular dtype (datetime64[ns]). pandas objects provide intercompatibility between NaT and NaN.

#+BEGIN_SRC python
In [16]: df2
Out[16]:
        one       two     three four   five  timestamp
a -0.166778  0.501113 -0.355322  bar  False 2012-01-01
c -0.337890  0.580967  0.983801  bar  False 2012-01-01
e  0.057802  0.761948 -0.712964  bar   True 2012-01-01
f -0.443160 -0.974602  1.047704  bar  False 2012-01-01
h -0.717852 -1.053898 -0.019369  bar  False 2012-01-01

In [17]: df2.loc[['a','c','h'],['one','timestamp']] = np.nan

In [18]: df2
Out[18]:
        one       two     three four   five  timestamp
a       NaN  0.501113 -0.355322  bar  False        NaT
c       NaN  0.580967  0.983801  bar  False        NaT
e  0.057802  0.761948 -0.712964  bar   True 2012-01-01
f -0.443160 -0.974602  1.047704  bar  False 2012-01-01
h       NaN -1.053898 -0.019369  bar  False        NaT
#+END_SRC
- rename column names:
#+begin_src python
df_bbg = df_bbg.rename(columns = lambda x: x[:4].replace(' ',''))
df = df.rename(columns={'a':'A'})
#+end_src

  - rename according to column value type:
    #+BEGIN_SRC python
    name = {2:'idname', 23:'value', 4:'variable'}
    df.rename(columns=lambda x: name[(gftIO.get_column_type(df,x))], inplace=True)
    #+END_SRC
  - rename column according to value:
  #+BEGIN_SRC python
name = {'INNERCODE': 'contract_code', 'OPTIONCODE': 'contract_name',
        'SETTLEMENTDATE': 'settlement_date', 'ENDDATE': 'date',
        'CLOSEPRICE': 'close_price'}
data.rename(columns=lambda x: name[x], inplace=True)

  #+END_SRC
- remove characters after space:
#+begin_src python
df_bbg = df_bbg.rename(columns = lambda x: x.)
#+end_src
- apply by group:
#+BEGIN_SRC python
df_long_term = small_groups.groupby('个人ID').progress_apply(lambda x: long_term_hospitalization(x[['入院日期', '出院日期']], days=30))
#+END_SRC

- pivot table with value True or False:
#+BEGIN_SRC python
df = pd.DataFrame({'company':['a','b','c','b'], 'partner':['x','x','y','y'], 'str':['just','some','random','words']})
df.groupby(['company', 'partner']).size().unstack(fill_value=0).astype(bool)
#+END_SRC

- pandas long format to pivot:
#+begin_src python
pivoted = df.pivot('name1','name2','name3')
specific_risk = self.risk_model['specificRisk'].pivot(
    index='date', columns='symbol', values='specificrisk')
df_pivot_industries_asset_weights = pd.pivot_table(
        df_industries_asset_weight, values='value', index=['date'],
        columns=['industry', 'symbol'])
#+end_src

- pivot time series to hourly
#+BEGIN_SRC python
df['hour'] = df['date'].dt.hour
df['day'] = df['date'].dt.datetime
pd.pivot_table(df,index='hour',columns='day',values='pb',aggfunc=np.sum)
#+END_SRC
- change the time or date or a datetime:
#+begin_src python
end = end.replace(hour=23, minute=59, second=59)
#+end_src

- every weekday business day before 20th in each month:
#+BEGIN_SRC python
ls = pd.bdate_range(start='2021', end='2030')
ls_20 = [x.replace(day=20) for x in ls]
df = pd.concat([pd.Series(ls), pd.Series(ls_20)], axis=1)
df[2] = df[0] - df[1]
df = df[df[2]<np.timedelta64(1, 'D')]
df = df.loc[df.groupby(1)[2].idxmax()]
#+END_SRC

- Keep the first rows of continuous specific values in a pandas data frame?
#+BEGIN_SRC python
a = df['is_open']
mask = (a.shift(1) != a) & ((a.shift(-1) == a) | (a.shift(-2) == a)) & (a == 1)
df[mask]
#+END_SRC

- 万德 wind python pandas
#+begin_src python
df = pd.Dataframe(data = w.wsd().Data[0], index=w.wsd().Times)
#+end_src

- check DatetimeIndex difference:
#+BEGIN_SRC python
# to check the frequency of the strategy, DAILY or MONTHLY
dt_diff = df_single_period_return.index.to_series().diff().mean()
if dt_diff < pd.Timedelta('3 days'):
#+END_SRC
- time delta
#+BEGIN_SRC python
import datetime
s + datetime.timedelta(minutes=5)
#+END_SRC
- resample by a column:
need to set the index as a datetime index, then use the resample function.

- resample at a fraction but keep at least one:
#+BEGIN_SRC python
med_id_sample = df_patient.groupby(['JGID','CYZDDM3']).apply(lambda x :x.iloc[random.choice(range(0,len(x)))])['GHDJID'].values
med_id_sample1 = df_patient.groupby(['JGID','CYZDDM3']).apply(lambda x: x.sample(frac=0.1))['GHDJID'].values
med_id_samples = np.unique(np.concatenate((med_id_sample, med_id_sample1)))
df_patient[df_patient['GHDJID'].isin(med_id_samples)]
#+END_SRC
- resample by month and keep the last valid row
#+BEGIN_SRC python
benchmark_weight.index.name = 'Date'
m = benchmark_weight.index.to_period('m')
benchmark_weight = benchmark_weight.reset_index().groupby(m).last().set_index('Date')
benchmark_weight.index.name = ''
#+END_SRC

- groupby boolean sum ratio:
#+BEGIN_SRC python
df_items['boolean'] = df_items['boolean'].map({True:1, False:0})
df_items_sum = df_items.groupby(['disease_code', 'soc_srt_dire_nm'])['boolean'].sum()
#+END_SRC

- groupby item ratio:
#+BEGIN_SRC python
df_items_sum = df_items.groupby(['disease_code', 'soc_srt_dire_nm']).agg({'amount': 'sum'})
# Change: groupby state_office and divide by sum
df_items_ratio = df_items_sum.groupby(level=0).apply(lambda x:
                                                 100 * x / float(x.sum()))
#+END_SRC

- groupby and sort by another column:
#+BEGIN_SRC python
df_input_text_entity.sort_values(['score'],ascending=False).groupby('mention').head(1) # only take the largest value of score
#+END_SRC

- filter two dataframe by columns' value
#+BEGIN_SRC python
pd.merge(df_input_text_entity_0, df_input_text_entity_1, on=['mention', 'entity'])
#+END_SRC

*** multiplying
- the multiplying calculation is not about the sequence of the index or column.

pandas will calculate on a sorted index and column value.
#+BEGIN_SRC python
In [87]: a=pd.DataFrame({'dog':[1,2],'fox':[3,4]},index=['a','b'])

In [88]: a
Out[88]:
   dog  fox
a    1    3
b    2    4

In [89]: b=pd.DataFrame({'fox':[1,2],'dog':[3,4]},index=['b','a'])

In [94]: b
Out[94]:
   dog  fox
b    3    1
a    4    2

In [95]: a*b
Out[95]:
   dog  fox
a    4    6
b    6    4
#+END_SRC

- dot multiplying
dot multiplying will sort the value.
#+BEGIN_SRC python
In [99]: a.dot(b.T)
Out[99]:
    b   a
a   6  10
b  10  16

In [100]: b.T
Out[104]:
     b  a
dog  3  4
fox  1  2

In [105]: a
Out[105]:
   dog  fox
a    1    3
b    2    4
#+END_SRC
*** Index

**** Index manuplication
- assign list to dataframe:
#+BEGIN_SRC python
df_result.at[index_1, 'subset'] = [v for x,v in enumerate(subsets) if x in qualified_doc_num]
#+END_SRC

- set column as datetime index
#+begin_src python
index = index.set_index(pd.DatetimeIndex(index['tradeDate'])).drop('tradeDate', axis=1)
# df = df.set_index(pd.DatetimeIndex(df['Date']))
#+end_src

- concaterate:
#+begin_src python
pd.concat([df1, df2], axis=0).sort_index()
pd.concat([df1, df2], axis=1)
result = df1.join(df2, how='outer')
#+end_src

- check if the index is datetimeindex:
#+BEGIN_SRC python
if isinstance(df_otv.index, pd.DatetimeIndex):
    df_otv.reset_index(inplace=True)

#+END_SRC
- pandas are two dataframe identical
#+BEGIN_SRC python
pandas.DataFrame.equals()

#+END_SRC
- change index name:
#+begin_src python
df.index.names = ['Date']
#+end_src

- split train test for a dataframe:
#+BEGIN_SRC python
train=df.sample(frac=0.8,random_state=200) #random state is a seed value
test=df.drop(train.index)
# or
from sklearn.model_selection import train_test_split

train, test = train_test_split(df, test_size=0.2)
#+END_SRC

- for loop in pandas dataframe:
#+begin_src python
for index, value in DataFrame:
#+end_src

- compare two time series:
#+begin_src python
s1[s1.isin(s2)]
ax = df1.plot()
df2.plot(ax=ax)
#+end_src

- datetime to string:
#+begin_src python
df.index.strftime("%Y-%m-%d %H:%M:%S")
#+end_src

- concaterate index
#+begin_src python
pd.concat([df1, df2], axis=1)
#+end_src
concate will take two dataframe to a new dataframe by index, preserving the columns.
A:
index variable value
B:
index variable value

pd.concat([A, B])
index variable value variable value
**** merge
merge will take two dataframe to a new dataframe by index, on the columns.
A:
index variable value
B:
index variable value

pd.merge(A, B, how='left', on=['index', 'variable'])
index variable value value

result = pd.merge(left, right, on=['key1', 'key2'])

**** update
update dataframe1 with dataframe2

**** access hierarchical index.
  - A MultiIndex can be created from a list of arrays (using MultiIndex.from_arrays), an array of tuples (using MultiIndex.from_tuples), or a crossed set of iterables (using MultiIndex.from_product).
#+begin_src python
df.loc[‘date’,’col’], df[‘date’], df.ix[[‘date1’, ‘date2’]]
#+end_src

- slicing:
#+begin_src python
df.loc['start':'end',], df['start': 'end']
#+end_src

- slice with a ‘range’ of values, by providing a slice of tuples:
#+begin_src python
df.loc[('2006-11-02','USO.US'):('2006-11-06','USO.US')]
df.loc(axis=0)[:,['SPY.US']]
#+end_src

- select certain columns:
#+begin_src python
df.loc(axis=0)[:,['SPY.US']]['updatedTime']
#+end_src

- select rows with certain column value:
#+BEGIN_SRC python
df.loc[df['column_name'].isin(some_values)]
#+END_SRC

- select date range using pd series.
#+begin_src python
date_not_inserted = whole_index[~whole_index.isin(date_in_database['date'])]
df_need_to_be_updated = whole_df_stack.ix[days_not_in_db]
#+end_src

**** remove pandas duplicated index
***** #1
#+begin_src python
grouped = sym.groupby(level=0)
sym = grouped.last()
#+end_src

***** #2
#+begin_src python
df2[~df2.index.duplicated()]
#+end_SRC

***** remove duplicated rows
#+BEGIN_SRC python
pandas.DataFrame.drop_duplicates(subset=None, keep='first', inplace=False)
# subset : column label or sequence of labels, optional
#+END_SRC
**** convert a dataframe to an array:
#+begin_src emacs-lisp :tangle yes
pd.dataframe.to_matrix()
#+end_src

**** panel:
- create from dictionary:
#+BEGIN_SRC python
datetime_index = pd.DatetimeIndex(assets_group['date'].unique())
panel_model = pd.Panel({date: pd.DataFrame(0, index=assets.loc[date,'variable'],
                                           columns=assets.loc[date,'variable']) for date in datetime_index})
#+END_SRC
pandas panel item axis should be datetime64, this should not be an array.
**** unpivot multindex, multindex into colum:
#+BEGIN_SRC python
df_med_similarity_adj_matrix['similarity'] = df_med_similarity_adj_matrix.apply(
    lambda x: 1 - jaccard(docs[x.name[0]], docs[x.name[1]]), axis=1)

df_med_similarity_adj_matrix.index = pd.MultiIndex.from_tuples(df_med_similarity_adj_matrix.index)
df_med_similarity_adj_matrix.reset_index().pivot(index='level_0', columns='level_1', values='similarity')
#+END_SRC
** numpy
- convert a list of arrays to dataframe:
#+BEGIN_SRC python
pd.DataFrame(np.concatenate(ls_df))
#+END_SRC

- convert a list of arrays to 3d array:
#+BEGIN_SRC python
import numpy as np

l1 = []
# create list of arrays
for i in range(5):
    l1.append(np.random.random((5, 3)))

# convert list of arrays into 3-dimensional array
d = np.dstack(l1)

d.shape #(5,3,5)
#+END_SRC

- get index of a value from array:
#+BEGIN_SRC python
np.where(array==value)
#+END_SRC

- numpy unique without sort:
#+BEGIN_SRC python
>>> import numpy as np
>>> a = [4,2,1,3,1,2,3,4]
>>> np.unique(a)
array([1, 2, 3, 4])
>>> indexes = np.unique(a, return_index=True)[1]
>>> [a[index] for index in sorted(indexes)]
[4, 2, 1, 3]

#+END_SRC

- bokeh plot:
#+BEGIN_SRC python
x_column = "datetime"
y_column1 = "close"
y_column2 = "vol"
y_column3 = "cum_delta"
# Bokeh plot
# cross = CrosshairTool()
# cross.line_color = 'white'
# cross.line_alpha = 0.7
# Bokeh plot
output_file("twin_axis.html")

y_overlimit = 0.05 # show y axis below and above y min and max value
# Add hover to this comma-separated string and see what changes
TOOLS = 'wheel_zoom, box_zoom, pan, box_select, crosshair,reset'

# create a new plot with the toolbar below
p = figure(plot_width=800, plot_height=400, x_axis_type='datetime', tools=TOOLS)
#         HoverTool(tooltips=[('date', '@DateTime{%F}')],
#           formatters={'@DateTime': 'datetime'})

hover = HoverTool(
    tooltips = [
        ('datetime', '@DateTime{%F}'),
        ("(x,y)", "($y, $x)")
    ],
    formatters={
        '@DateTime': 'datetime',
        'Value' : 'printf',
    },    
)
p.add_tools(hover)
p.toolbar.active_scroll = p.select_one(WheelZoomTool)
#p = figure()
#p.toolbar.active_inspect = [cross]

# FIRST AXIS
p.line(df[x_column], df[y_column1], legend=y_column1, line_width=1, color="blue")
p.y_range = Range1d(
    df[y_column1].min(), df[y_column1].max()
)
# SECOND AXIS
y_column2_range = y_column2 + "_range"
p.extra_y_ranges = {
    y_column2_range: Range1d(
        start=df[y_column2].min() * (1 - y_overlimit),
        end=df[y_column2].max() * (1 + y_overlimit),
    )
}
p.add_layout(LinearAxis(y_range_name=y_column2_range), "right")

p.line(
    df[x_column],
    df[y_column2],
    legend=y_column2,
    line_width=1,
    y_range_name=y_column2_range,
    color="green",
)
# Second figure
p2 = figure(plot_width=800, plot_height=400, x_axis_type='datetime', tools=TOOLS)
p2.add_tools(hover)
p2.toolbar.active_scroll = p.select_one(WheelZoomTool)
# FIRST AXIS
p2.line(df[x_column], df[y_column1], legend=y_column1, line_width=1, color="blue")
p2.y_range = Range1d(
    df[y_column1].min(), df[y_column1].max()
)

# SECOND AXIS
y_column3_range = y_column3 + "_range"
p2.extra_y_ranges = {
    y_column3_range: Range1d(
        start=df[y_column3].min() * (1 - y_overlimit),
        end=df[y_column3].max() * (1 + y_overlimit),
    )
}
p2.add_layout(LinearAxis(y_range_name=y_column3_range), "right")

p2.line(
    df[x_column],
    df[y_column3],
    legend=y_column3,
    line_width=1,
    y_range_name=y_column3_range,
    color="green",
)

show(row(p, p2))
#+END_SRC

- plot histogram:
#+BEGIN_SRC python
>>> import matplotlib.pyplot as plt
>>> rng = np.random.RandomState(10)  # deterministic random data
>>> a = np.hstack((rng.normal(size=1000),
...                rng.normal(loc=5, scale=2, size=1000)))
>>> plt.hist(a, bins='auto')  # arguments are passed to np.histogram
>>> plt.title("Histogram with 'auto' bins")
>>> plt.show()
#+END_SRC
- which quantile:
#+BEGIN_SRC python
df_average_cost['COST_LEVEL'] = pd.qcut(df_average_cost['MED_AMOUNT'], 3, labels=["high", "medium", "low"])
#+END_SRC
- quantile:
#+BEGIN_SRC python
numpy.quantile(a, q, axis=None, out=None, overwrite_input=False, interpolation='linear', keepdims=False)
#+END_SRC
- upper triangle matrix:
#+BEGIN_SRC python
import numpy as np

a = np.array([[1,2,3],[4,5,6],[7,8,9]])

#array([[1, 2, 3],
#       [4, 5, 6],
#       [7, 8, 9]])

a[np.triu_indices(3, k = 1)]

# this returns the following
array([2, 3, 6])
#+END_SRC
- sort an array by descending:
#+BEGIN_SRC python
In [25]: temp = np.random.randint(1,10, 10)

In [26]: temp
Out[26]: array([5, 2, 7, 4, 4, 2, 8, 6, 4, 4])

In [27]: id(temp)
Out[27]: 139962713524944

In [28]: temp[::-1].sort()

In [29]: temp
Out[29]: array([8, 7, 6, 5, 4, 4, 4, 4, 2, 2])

In [30]: id(temp)
Out[30]: 139962713524944
#+END_SRC
- save an array:
#+BEGIN_SRC python
import numpy as np
np.save(filename, array)
#+END_SRC
- maximum value in each row
#+BEGIN_SRC python
np.amax(ar, axis=1)
#+END_SRC
- from 2-D array to 1-D array with one column
#+BEGIN_SRC python
import numpy as np
a = np.array([[1],[2],[3]]))
a.flattern()
#+END_SRC
- Take a sequence of 1-D arrays and stack them as columns to make a single 2-D:
#+BEGIN_SRC python
numpy.column_stack(tup)
Parameters:
tup : sequence of 1-D or 2-D arrays.
Arrays to stack. All of them must have the same first dimension.
>>> a = np.array((1,2,3))
>>> b = np.array((2,3,4))
>>> np.column_stack((a,b))

- expand 1-D numpy array to 2-D:

#+END_SRC

- expand the shape of an array:
#+BEGIN_SRC python
numpy.expand_dims(a, axis)
# Expand the shape of an array.
# Insert a new axis that will appear at the axis position in the expanded array shape.
>>> x = np.array([1,2])
>>> x.shape
(2,)
>>> y = np.expand_dims(x, axis=0)
>>> y
array([[1, 2]])
>>> y.shape
(1, 2)
#+END_SRC

- count nan:
#+begin_src python
np.count_nonzero(~np.isnan(df['series']))
#+end_src

- count number of negative value:
#+begin_src python
np.sum((df < 0).values.ravel())
#+end_src

- check the difference of two arrays:
numpy.setdiff1d:
Return the sorted, unique values in ar1 that are not in ar2
#+BEGIN_SRC python
np.setdiff1d(ar1, ar2)
#+END_SRC
- turn a list of tuples into a list:
#+BEGIN_SRC python
[item for t in lt for item in t]
#+END_SRC

- pinyin sorted:
#+BEGIN_SRC python
from itertools import chain
from pypinyin import pinyin, Style


def to_pinyin(s):
    '''转拼音

    :param s: 字符串或列表
    :type s: str or list
    :return: 拼音字符串
    >>> to_pinyin('你好吗')
    'ni3hao3ma'
    >>> to_pinyin(['你好', '吗'])
    'ni3hao3ma'
    '''
    return ''.join(chain.from_iterable(pinyin(s, style=Style.TONE3)))


print(sorted(['美国', '中国', '日本']))  # 美m 中z 日r abcdefghijkl[m]nopq[r]stuvwsy[z]
# ['中国', '日本', '美国']
print(sorted(['美国', '中国', '日本'], key=to_pinyin))  # 美m 中z 日r abcdefghijkl[m]nopq[r]stuvwsy[z]

#+END_SRC

- sorted a list of tuples
#+BEGIN_SRC python
sorted(enumerate(sims), key=lambda item: -item[1])
#+END_SRC
- reshape:
np.reshape((1, -1)), -1 means automatic number of columns.

- select random symbols from a listdir:
#+BEGIN_SRC python
# get random symbols at the target position limit
position_limit = 8
arr = list(range(len(target_symbols)))
np.random.shuffle(arr)
target_symbols = target_symbols[arr[:position_limit]]
#+END_SRC
** plot:
- %matplotlib inline
To set this up, before any plotting or import of matplotlib is performed you must execute the %matplotlib magic command. This performs the necessary behind-the-scenes setup for IPython to work correctly hand in hand with matplotlib; it does not, however, actually execute any Python import commands, that is, no names are added to the namespace.
*** subplot with the same axis:
pandas plot.
using matplotlib:
- plot different series on the same chart.
#+BEGIN_SRC python
cl_active_contract_pricing.plot()
cl_pricing.plot(style='k--')
#+END_SRC
- plot in ipython or jupyter notebook:
#+BEGIN_SRC python
ax = contract_data.plot(legend=True)
continuous_price.plot(legend=True, style='k--', ax=ax)
plt.show()
#+END_SRC

- x axis label doesn't save and show:
#+BEGIN_SRC python
fig, ax = plt.subplots()
ax = df_final_result['item_amount'].reset_index().groupby('med_inv_item_type_name').boxplot(['item_amount'])
ax.set_xticklabels(ax.get_xticklabels(),rotation=90)
figure = ax.get_figure()
figure.savefig('./result/images/组%s.png'%(str(i_sub)), bbox_inches='tight')

#+END_SRC


*** multiple figure一次绘制多个图形
- same plot, 多个figure:
#+BEGIN_SRC python
# figure.py

import matplotlib.pyplot as plt
import numpy as np

data = np.arange(100, 201)
plt.plot(data)

data2 = np.arange(200, 301)
plt.figure()
plt.plot(data2)

plt.show()
#+END_SRC

- multiple subplots 在同一个窗口显示多个图形
#+begin_src python
import matplotlib.pyplot as plt
fig = plt.figure()
ax1 = fig.add_subplot(2, 2, 1)
ax2 = fig.add_subplot(2, 2, 2)
ax3 = fig.add_subplot(2, 2, 3)
fig, axes = plt.subplots(2,3)
fig, ax : tuple
# or
data = np.arange(100, 201)
plt.subplot(2, 1, 1)
plt.plot(data)

data2 = np.arange(200, 301)
plt.subplot(2, 1, 2)
plt.plot(data2)

plt.show()
#+end_src
*** subplot with different axis
#+BEGIN_SRC python
plt.subplot(2, 1, 1)
plt.boxplot(x1)
plt.plot(1, x1.ix[-1], 'r*', markersize=15.0)

plt.subplot(2, 1, 2)
x1.plot()
# or
fig, axes = plt.subplots(2, 1, figsize=(10, 14))
axes[0].boxplot(pe000001)
axes[0].plot(1, pe000001.ix[-1], 'r*', markersize=15.0)

pe000001.plot()
#+END_SRC

*** plot a secondary y scale
#+begin_src python
df.price.plot(legend=True)
(100-df.pct_long).plot(secondary_y=True, style='g', legend=True)
#+end_src
- highlight a certain value in the plot:
#+begin_src python
a['DGAZ.US'].hist(bins=50)
plt.axvline(a['DGAZ.US'][-1], color='b', linestyle='dashed', linewidth=2)
#+end_src

*** plot seaborn:
- plot heatmap:
#+BEGIN_SRC python
figure = plt.figure(figsize=(12,12))
ax = sns.heatmap(temp, vmin=0, vmax=10, fmt="d", cmap="YlGnBu", annot=True)
# save plot
ax.get_figure()
figure.savefig('./images/test.png')
#+END_SRC
- save seaborn heatmap:
#+BEGIN_SRC python
plt.subplots(figsize=(12,12))

# fig, ax = plt.subplots(figsize=(12,12))
ax = sns.heatmap(temp, vmin=0, vmax=10, fmt="d", cmap="YlGnBu", annot=True)
# !!! can't save figure directly, need to get figure first.
figure = ax.get_figure()
figure.savefig('./images/%s.png'%(str(person_ids)))

#+END_SRC

- convert list of lists to count matrix, plot heatmap:
#+BEGIN_SRC python
from gensim import corpora, models, similarities
from gensim.matutils import jaccard
from gensim.matutils import corpus2dense

processed_corpus = copy(df_count)
dictionary = corpora.Dictionary(processed_corpus)
corpus = [[dictionary.token2id[y] for y in text] for text in processed_corpus]
bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]
tfidf = models.TfidfModel(bow_corpus)
num_features = len(dictionary.dfs)
df_count_heatmap = pd.DataFrame(corpus2dense(bow_corpus, num_terms = num_features).T, columns=[list(dict(dictionary).values())])
ax = sns.heatmap(df_count_heatmap.T)

query_bow = dictionary.doc2bow(processed_corpus[-1])
sims = index[tfidf[query_bow]]
qualified_doc_num = [x for x, v in enumerate(sims) if v>sim_threashold and x!=(len(bow_corpus)-1)]
# empty rows and columns are excluded from subsets
df_result.at[index_1, 'subset'] = [v for x,v in enumerate(subsets) if x in qualified_doc_num]
#+END_SRC

*** plot a 3d figure:
#+begin_src python :tangle yes
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt

strike = np.linspace(50, 150, 5)
ttm = np.linspace(0.5, 2.5, 8)

strike, ttm = np.meshgrid(strike, ttm)
iv = (strike - 100) ** 2 / (100 * strike) / ttm
fig = plt.figure(figsize=(9,6))
ax = fig.gca(projection='3d')
surf = ax.plot_surface(strike, ttm, iv, rstride=2, cstride=2,
                       cmap=plt.cm.coolwarm, linewidth=0.5,
                       antialiased=True)
fig.colorbar(surf, shrink=0.5, aspect=5)
#+end_src
fig is the :class:matplotlib.figure.Figure object.

- ax can be either a single axis object or an array of axis
- objects if more than one subplot was created.

[http://docs.pythontab.com/interpy/args_kwargs/Usage_args/]

[http://python.usyiyi.cn/python_278/library/index.html]

[https://docs.python.org/2/reference/simple_stmts.html?highlight=assert]

*** display Chinese:
#+BEGIN_SRC bash
➜  log git:(master) ✗ fc-list :lang=zh
/usr/share/fonts/truetype/wqy/wqy-microhei.ttc: 文泉驿微米黑,文泉驛微米黑,WenQuanYi Micro Hei:style=Regular
/usr/share/fonts/truetype/droid/DroidSansFallbackFull.ttf: Droid Sans Fallback:style=Regular
/usr/share/fonts/truetype/wqy/wqy-microhei.ttc: 文泉驿等宽微米黑,文泉驛等寬微米黑,WenQuanYi Micro Hei Mono:style=Regular
#+END_SRC
#+BEGIN_SRC python
import matplotlib.font_manager as font_manager

font_dirs = ['/my/custom/font/dir', ]
font_files = font_manager.findSystemFonts(fontpaths=font_dirs)
font_list = font_manager.createFontList(font_files)
font_manager.fontManager.ttflist.extend(font_list)
print(font_list)
mpl.rcParams['font.family'] = 'My Custom Font'

# or
import matplotlib.font_manager as mfm
import matplotlib.pyplot as plt
font_path = "/usr/share/fonts/truetype/wqy/wqy-microhei.ttc"
prop = mfm.FontProperties(fname=font_path)
plt.text(0.5, 0.5, s=u'测试', fontproperties=prop)
plt.show()

# or
font_path = "/home/wuwei/projects/temp/captcha-tensorflow/fonts/huawenxihei.ttf"
from matplotlib.font_manager import FontProperties
myfont=FontProperties(fname=font_path,size=14)
sns.set(font=myfont.get_name())
# 房价数据存储到字典中，数据纯属虚构
city_price = {'深圳': 55000,'上海': 48000, '北京': 45000, '厦门': 35000, '广州': 33000}

# 作条形图
sns.barplot(x=[k for k, _ in city_price.items()], y=[v for _, v in city_price.items()])
sns.despine(left=True)
plt.title('房价排行', color='gray', fontsize=14, weight='bold')

# networkx chinese
# replace /home/wuwei/anaconda3/envs/tfgpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf with any other font
font_path = "/home/wuwei/projects/temp/captcha-tensorflow/fonts/huawenxihei.ttf"
from matplotlib.font_manager import FontProperties
myfont=FontProperties(fname=font_path,size=14)
nx.draw(subgraphs[1], with_labels=True, font_family='SimHei')
#+END_SRC


*** stacked barplot, portfolio change
#+BEGIN_SRC python
pivot_df_insurance_accident = df_insurance_accident.pivot(
    index='year', columns='cat_age_sex', values='basic_insurance_fee')
pivot_df_insurance_disease.plot.bar(title='医疗保险-疾病纯保费',stacked=True, figsize=(10,7))

#+END_SRC
*** tsne:
- visualize features:
#+BEGIN_SRC python
%matplotlib inline
from sklearn import datasets
digits = datasets.load_digits()
# Take the first 500 data points: it's hard to see 1500 points
X = digits.data[:500]
y = digits.target[:500]
from sklearn.manifold import TSNE
tsne = TSNE(n_components=2, random_state=0)
X_2d = tsne.fit_transform(X)
target_ids = range(len(digits.target_names))

from matplotlib import pyplot as plt
plt.figure(figsize=(6, 5))
colors = 'r', 'g', 'b', 'c', 'm', 'y', 'k', 'w', 'orange', 'purple'
for i, c, label in zip(target_ids, colors, digits.target_names):
    plt.scatter(X_2d[y == i, 0], X_2d[y == i, 1], c=c, label=label)
plt.legend()
plt.show()

#+END_SRC
** scipy
-  combination k from n.
$$
{\displaystyle {\binom {n}{k}}={\frac {n(n-1)\dotsb (n-k+1)}{k(k-1)\dotsb 1}},} {\binom {n}{k}}={\frac {n(n-1)\dotsb (n-k+1)}{k(k-1)\dotsb 1}}$$

which can be written using factorials as$$ {\displaystyle \textstyle {\frac {n!}{k!(n-k)!}}} \textstyle {\frac {n!}{k!(n-k)!}} $$
#+BEGIN_SRC python
>>> from scipy.special import comb
>>> k = np.array([3, 4])
>>> n = np.array([10, 10])
>>> comb(n, k, exact=False)
array([ 120.,  210.])
>>> comb(10, 3, exact=True)
120L
>>> comb(10, 3, exact=True, repetition=True)
220L
#+END_SRC
[https://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.comb.html]
** networkx:
- list edges of a node:
#+BEGIN_SRC python
G.neibors('node')
#+END_SRC
- count edges of a node:
#+BEGIN_SRC python
G.in_edges_degree()
G.out_edges_degree()
#+END_SRC
- create an edge:
#+BEGIN_SRC python
G.add_edges_from([(a,b)])
G.add_weighted_edges_from([(a,b,weight)])
#+END_SRC
- create a graph:
#+BEGIN_SRC python
G = nx.Graph()
# directed graph
G = nx.DiGraph()
#+END_SRC
- dump a graph:
#+BEGIN_SRC python
nx.write_gexf(G, 'file/path.gexf')
#+END_SRC
- draw a graph:
#+BEGIN_SRC python
nx.draw(G, with_labels=True)
#+END_SRC

- highlight node in network:
#+BEGIN_SRC python
# generate node positions:
pos = nx.spring_layout(G)
# draw graph
nx.draw_networkx(G, pos=pos, font_size=10, node_color='green', font_color='black', font_family='SimHei')
# draw subgraph for highlights
nx.draw_networkx(G.subgraph('胸膜炎'), pos=pos, font_size=10, node_color='red', font_color='green', font_family='SimHei')
#+END_SRC

* Machine learning:
** data processing:
- coding, mapping a list into range:
#+BEGIN_SRC python
from sklearn.preprocessing import LabelEncoder
class_label = LabelEncoder()
data["label"] = class_label.fit_transform(data["label"].values)
# or
label_mapping = {label:idx for idx,label in enumerate(np.unique(data["label"]))}

#+END_SRC
- one hot coding:
#+BEGIN_SRC python
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
X = data[["color", "price"]].values
#通过类标编码将颜色装换成为整数
color_label = LabelEncoder()
X[:,0] = color_label.fit_transform(X[:,0])
#设置颜色列使用oneHot编码
one_hot = OneHotEncoder(categorical_features=[0])
print(one_hot.fit_transform(X).toarray())
# or
pd.get_dummies(data[["color","price"]])
#+END_SRC

* Deep Learning
** Tensorflow
- convert string to number index:
#+BEGIN_SRC python
with tf.Session() as sess:
    mapping_strings = tf.constant(["emerson", "lake", "palmer"])
    feats = tf.constant(["emerson", "lake", "and", "palmer"])
    ids = tf.contrib.lookup.string_to_index(
        feats, mapping=mapping_strings, default_value=-1)
    tf.compat.v1.tables_initializer().run()
    idx = ids.eval()#   ==> [0, 1, -1, 2]
#+END_SRC
- disable tensorflow warnings:
#+BEGIN_SRC python
os.environ["TF_CPP_MIN_LOG_LEVEL"]="2"
#+END_SRC

- install tensorflow:
#+BEGIN_SRC bash
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
conda install tensorflow-gpu==1.3
conda config --set show_channel_urls yes
conda install tensorflow-gpu==1.3
#+END_SRC

- test drive:
#+BEGIN_SRC bash
python -m tensorflow.models.image.mnist.convolutional
#+END_SRC

*** GPU test:
[[file:./tensorflow_example.py][tensorflow]]
#+INCLUDE: "tensorflow_example.py"
** Computer Vision
*** style transfer
*** basic operation
- convert an image into grey:
#+BEGIN_SRC python
In [1]: from PIL import Image

In [2]: image = Image.open('/tmp/capcha.png')

In [7]: image = image.convert('L')

In [3]: data = image.load()
   ...: w, h = image.size
   ...: for i in range(w):
   ...:     for j in range(h):
   ...:         if data[i, j] > 125:
   ...:             data[i, j] = 255  # 纯白
   ...:         else:
   ...:             data[i, j] = 0  # 纯黑

image.save('clean_captcha.png')

img = cv2.imread(r'D:/UNI/Y3/DIA/2K18/lab.jpg')
RGB_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
gray = cv2.cvtColor(RGB_img, cv2.COLOR_RGB2GRAY)
equ = cv2.equalizeHist(gray)
res = np.hstack((img,equ))
plt.imshow(gray, cmap='gray', vmin = 0, vmax = 255)
#+END_SRC

- show image in jupyter notebook:
#+BEGIN_SRC python
from IPython.display import Image
Image(filename=content_seg_path)
#+END_SRC

- read image into numpy arrays:
#+BEGIN_SRC python
from scipy.misc import imread, imresize
import cv2
init_img = cv2.imread(initImg) # [y, x, 3] BGR
content = scipy.misc.imread(contentImg, mode='RGB')
#+END_SRC

- convert numpy array to image:
#+BEGIN_SRC python
import scipy.misc
rgb = scipy.misc.toimage(np_array)
cv2.imwrite('color_img.jpg', np_array)
#+END_SRC

- read image into torch tensor:
#+BEGIN_SRC python
import numpy as np
from PIL import Image
from torchvision import transforms
from torchvision.utils import save_image
def open_image(image_path, image_size=None):
    """

    """
    image = Image.open(image_path)
    _transforms = []
    if image_size is not None:
        image = transforms.Resize(image_size)(image)
        # _transforms.append(transforms.Resize(image_size))
    w, h = image.size
    _transforms.append(transforms.CenterCrop((h // 16 * 16, w // 16 * 16)))
    _transforms.append(transforms.ToTensor())
    transform = transforms.Compose(_transforms)
    result = transform(image)[:3].unsqueeze(0)
    return result

#+END_SRC
- concatenate images into one:
#+BEGIN_SRC python
def cat_images(fname, ls_images_path):
    images = []
    max_width = 0 # find the max width of all the images
    total_height = 0 # the total height of the images (vertical stacking)
    for name in image_names:
        # open all images and find their sizes
        images.append(cv2.imread(name))
        if images[-1].shape[1] > max_width:
            max_width = images[-1].shape[1]
        total_height += images[-1].shape[0]

    def cat_arrays(total_height,max_width,arrays):
        # create a new array with a size large enough to contain all the images
        final_array = np.zeros((total_height,max_width,3),dtype=np.uint8)

        current_y = 0 # keep track of where your current image was last placed in the y coordinate
        for array in arrays:
            # add an image to the final array and increment the y coordinate
            final_array[current_y:array.shape[0]+current_y,:array.shape[1],:] = array
            current_y += array.shape[0]
        return final_array
    final_image = cat_arrays(total_height, max_width, images)
    cv2.imwrite(fname,final_image)
cat_images(fname, image_names)
#+END_SRC

- overlay visualize:
#+BEGIN_SRC python
from matplotlib import gridspec
from matplotlib import pyplot as plt
import numpy as np
from PIL import Image

def vis_overlay(original_im, seg_map):
  """Visualizes input image, segmentation map and overlay view."""
  original_im = Image.open(original_im)
  seg_map = Image.open(seg_map)
  plt.figure(figsize=(15, 5))
  grid_spec = gridspec.GridSpec(1, 4, width_ratios=[6, 6, 6, 1])

  plt.subplot(grid_spec[0])
  plt.imshow(original_im)
  plt.axis('off')
  plt.title('input image')

  plt.subplot(grid_spec[1])
  # seg_image = label_to_color_image(seg_map).astype(np.uint8)
  plt.imshow(seg_map)
  plt.axis('off')
  plt.title('result image')

  plt.subplot(grid_spec[2])
  plt.imshow(original_im)
  plt.imshow(seg_map, alpha=0.7)
  plt.axis('off')
  plt.title('overlay image')
  plt.show()
#+END_SRC
* NLP
** Keywords
- association rules data mining:
#+BEGIN_SRC python
# categories
G = nx.DiGraph()
df_vi_mapping.apply(lambda x: G.add_edge(x['PARENT_ID'], x['ID']), axis=1)
Gl = nx.Graph()
df_vi_mapping.apply(lambda x: Gl.add_edge(x['PARENT_ID'], x['ID']), axis=1)
# check if there're cycle
data_sub = nx.connected_component_subgraphs(Gl, copy=True)
all_sub = [list(i.nodes()) for i in data_sub]

level_leaves_nodes = [x for x,y in G.out_degree() if y == 0]
level_head_nodes = [y for x,y in G.in_edges() if x=='0']
map_vi_levels = pd.DataFrame([list(path) for x in level_leaves_nodes for path in nx.all_simple_paths(G, source='0', target=x)])
map_vi_levels = map_vi_levels.sort_values(list([0, 1, 2, 3, 4, 5]))
idx_map_vi_levels = map_vi_levels[[1,2,3,4,5]].fillna(method='ffill', axis=1)
map_vi_levels = map_vi_levels[[1,2,3,4,5]].fillna(method='ffill', axis=1)
arrays = pd.MultiIndex.from_arrays(idx_map_vi_levels.values.T, names=[1,2,3,4,5])

levels = [0, 1, 2, 3, 4]
def shrink_concept(patient):
    print(patient['med_clinic_id'])
    ls_items = patient['item_code_check']
    dict_cost = dict(zip(patient['item_code'], patient['item_cost']))

    # 构建多索引项目花费DataFrame
    df_temp = pd.DataFrame()
    for item in ls_items:
        index = [vals for x, vals in enumerate(arrays) if vals[-1]==item or vals[-2]==item]
        if len(index) > 1:
            df_temp = df_temp.append(pd.DataFrame(index=pd.MultiIndex.from_tuples([index[0]]), columns=['cost'], data=float(dict_cost[item])))
        elif len(index) == 1:
            df_temp = df_temp.append(pd.DataFrame(index=pd.MultiIndex.from_tuples(index), columns=['cost'], data=float(dict_cost[item])))

    # 逐层获取概念
    df_temp_order = pd.DataFrame()
    ls_lvs = []
    # 先把大于100块的取出来，并删除
    threashold_idx = df_temp[df_temp['cost']>=100].index
    if len(threashold_idx) > 0:
        ls_lvs.append(threashold_idx.get_level_values(4))
        df_temp = df_temp.drop(index=threashold_idx)

    mul_arrays = df_temp.index
    for l in levels:
        s_temp = df_temp.groupby(level=l).sum()
        idx_lv1 = s_temp[s_temp['cost']>100].index
        drop_lv1 = [vals for x, vals in enumerate(mul_arrays) if vals[l] in s_temp[s_temp['cost']<=100].index]
        if len(drop_lv1) > 0:
            df_temp = df_temp.drop(index=drop_lv1)
        else:
            continue
        if len(df_temp) == len(idx_lv1):
            # print('not enough depth')
            ls_lvs.append(s_temp[s_temp['cost']>100].index)
            break
        else:
            ls_lvs.append(s_temp[s_temp['cost']<=100].index)

        # 最终的shinkage list
    ls_lvs_unique1 = set()

    for x in ls_lvs:
        ls_lvs_unique1.update(set(x))

    return ls_lvs_unique1
# find similar subsets and extra items
sim_threashold = 0.5
df_abnormal_items = pd.DataFrame(columns=['hos', 'dis', 'abnormal_item', 'rule', 'subset'])
for index in tqdm(range(len(dict_disease_seed_graph_1))):
    # print(index)
    index_1 = dict_disease_seed_graph_1[index][0]
    dis = df_result.loc[index_1, 'dis']
    hos = df_result.loc[index_1, 'hos']

    subsets = [x[1:-1] for x in dict_disease_seed_graph_1[index_1][1] if x[1][1]==dis and
                                      x[1][0]!=hos]
    # exclude rule whose unique hospital number is less than 3
    if len(set([x[0][0] for x in subsets])) < 3:
        continue

    # remove rule with items not like most of other rule itemsets
    processed_corpus = [x[1] for x in subsets] + [df_result.loc[index_1, 'itemsets_all']]
    dictionary = corpora.Dictionary(processed_corpus)
    corpus = [[dictionary.token2id[y] for y in text] for text in processed_corpus]
    bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]
    tfidf = models.TfidfModel(bow_corpus)
    num_features = len(reduce(lambda x, y: set(x) | set(y), corpus))
    index = similarities.SparseMatrixSimilarity(tfidf[bow_corpus], num_features=num_features)
    query_bow = dictionary.doc2bow(processed_corpus[-1])
    sims = index[tfidf[query_bow]]
    qualified_doc_num = [x for x, v in enumerate(sims) if v>sim_threashold and x!=(len(bow_corpus)-1)]

    # empty rows and columns are excluded from subsets
    df_result.at[index_1, 'subset'] = [v for x,v in enumerate(subsets) if x in qualified_doc_num]

    if len(df_result.at[index_1, 'subset']) < 3:
        continue

    all_subsets = set()
    for x in df_result.at[index_1, 'subset']:
            all_subsets.update(x[1])

    # extra_items = all sets - aggregation of subsets
    extra_items = df_result.loc[index_1, 'itemsets_all'].difference(all_subsets)
    if len(extra_items) > 0 and len(all_subsets) > 0:
        df_temp = pd.DataFrame([[df_result.loc[index_1, 'hos'],
                      df_result.loc[index_1, 'dis'],
                      [dict_vi_mapid2name[int(y)] for y in extra_items],
                     [dict_vi_mapid2name[int(x)] for x in df_result.loc[index_1, 'itemsets_all']],
                    [(y[0], [dict_vi_mapid2name[int(x)] for x in y[1]]) for y in df_result.at[index_1, 'subset']]]],
                               columns=['hos', 'dis', 'abnormal_item', 'rule', 'subset'])
        df_abnormal_items = pd.concat([df_abnormal_items, df_temp])
df_abnormal_items['abnormal_item'] = df_abnormal_items['abnormal_item'].apply(frozenset)
#+END_SRC

- corpus
#+BEGIN_SRC python
input_text_translations = """
The Chinese government’s top management obviously also hopes to avoid the deterioration of the Sino-US conflict. The Sino-US trade war has started. After the Sino-US trade war began, it emphasized that China has "five advantages" in the trade war. He stressed: "We must especially prevent Sino-US cooperation. Trade conflict spreads to the ideological field
"""
from gensim import corpora, models, similarities
from nltk.tokenize import word_tokenize, sent_tokenize
from sklearn import feature_extraction
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer

test_model = word_tokenize(input_text_translations.lower())

wordstest_model = sent_tokenize(input_text_translations)
test_model = [word_tokenize(_d.lower()) for _d in docs]
dictionary = corpora.Dictionary(test_model, prune_at=2000000)
# for key in dictionary.iterkeys():
#     print key,dictionary.get(key),dictionary.dfs[key]
corpus_model = [dictionary.doc2bow(test) for test in test_model]
tfidf_model = models.TfidfModel(corpus_model)
# 对语料生成tfidf
corpus_tfidf = tfidf_model[corpus_model]
d = {dictionary.get(id): value for doc in corpus_tfidf for id, value in doc}

# get the topic-word distribution
corpus = [dictionary.doc2bow(text) for text in tokenized_data]
dictionary1 = corpora.Dictionary(tokenized_data)
dictionary1.filter_n_most_frequent(10)
dictionary1.filter_extremes(no_above=0.9)
filtered_words = [dictionary[y] for y in [x for x in dictionary.keys() if x not in dictionary1.keys()]]
NUM_TOPICS = 5
# Build the LDA model
lda_model = models.LdaModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary1, per_word_topics=True, alpha='asymmetric', minimum_probability=0.0)
topic_distribution = lda_model.show_topics(num_words=50)
df_topic_word_dis = pd.DataFrame([x[1].split(' + ') for x in topic_distribution]).T
# frequency of word
dictionary.dfs[dictionary.token2id["血清α羟基丁酸脱氢酶测定"]]/dictionary.num_docs

from collections import defaultdict
frequency = defaultdict(int)
for text in df_result['itemsets_name']:
    for token in text:
        frequency[token] += 1
sorted(frequency.items(), key=lambda k_v: k_v[1], reverse=True)

# get the frequency of word in corpus:
tokenized_data = [list(jieba.cut(x)) for x in standard_terminologies]
dictionary = corpora.Dictionary(tokenized_data)
dictionary1 = copy(dictionary)
dictionary1.filter_extremes(no_below=10, no_above=0.05)
d_freq = {}
for i in dictionary.token2id.keys():
    d_freq[i] = dictionary.dfs[dictionary.token2id[i]]/dictionary.num_docs
frequency_of_words = [(k, d_freq[k]) for k in sorted(d_freq, key=d_freq.get, reverse=True)]
#+END_SRC

- one hot transformation:
#+BEGIN_SRC python
dataset = [['Milk', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],
           ['Dill', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],
           ['Milk', 'Apple', 'Kidney Beans', 'Eggs'],
           ['Milk', 'Unicorn', 'Corn', 'Kidney Beans', 'Yogurt'],
           ['Corn', 'Onion', 'Onion', 'Kidney Beans', 'Ice cream', 'Eggs']]
dictionary = corpora.Dictionary(dataset)
corpus = [[dictionary.token2id[y] for y in text] for text in dataset]
#+END_SRC

- word to id:
#+BEGIN_SRC python
    word_to_id = dict()
    id_to_word = dict()
    for i, token in enumerate(set(df_vi_mapping['ITEM_NAME'])):
        word_to_id[token] = i
        id_to_word[i] = token

#+END_SRC

- word2vec:
#+BEGIN_SRC python
from gensim.models import Word2Vec
from gensim.models import KeyedVectors
file5000 = '/share/Tencent_AILab_ChineseEmbedding.txt'
wv_from_text = KeyedVectors.load_word2vec_format(file5000, binary=False)
[x[0] for x in wv_from_text.most_similar('俗称',topn=20)]
#+END_SRC

- sentence embedding:

#+BEGIN_SRC python
index2word_set = set(wv_from_text.index2word)

def avg_feature_vector(sentence, model, index2word_set, stop_words, num_features=200):
    #words = cut_words.max_forward_cut(sentence)
    feature_vec = np.zeros((num_features, ), dtype='float32')
    n_words = 0
    for word in sentence:
        word = word.lower()
        if word in index2word_set and word not in stop_words:
            n_words += 1
            feature_vec = np.add(feature_vec, model[word])
    if (n_words > 0):
        feature_vec = np.divide(feature_vec, n_words)
    return feature_vec
#+END_SRC


- doc2vec:

#+BEGIN_SRC python
def segment(sentence: str):
    """
    结巴分词，并去除停用词
    """
    resp = []
    sentence_depart = jieba.cut(sentence.strip())
    for word in sentence_depart:
        if word != "":
            resp.append(word)
    return resp

def read_corpus(ls):
    for i, line in enumerate(ls):
        yield gensim.models.doc2vec.TaggedDocument(segment(line), [i])
"""
训练 Doc2Vec 模型
"""
train_corpus = list(read_corpus(ls_words))
model = gensim.models.doc2vec.Doc2Vec(vector_size=300, min_count=2, epochs=10, window=2)
print(len(train_corpus))
model.build_vocab(train_corpus)
model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)
model.save("doc2vec.model")

model = gensim.models.doc2vec.Doc2Vec.load("doc2vec.model")
def embed_text(sentences):
    """
    将所有的句子转化为向量
    """
    resp = []
    for s in sentences:
        resp.append(model.infer_vector(segment(s)).tolist())
    return resp
query = '白内障'
input_vector = model.infer_vector(segment(query)).tolist()
#+END_SRC

- max cut jieba segment:
#+BEGIN_SRC python
from gensim.models import KeyedVectors
file5000 = '/share/Tencent_AILab_ChineseEmbedding.bin'
wv_from_text = KeyedVectors.load_word2vec_format(file5000, binary=True)

import max_cut
import pickle
dict_cut = pickle.load(open("/home/wuwei/projects/temp/text_matching/data/dict_cut_words.pkl", "rb" ))
stop_words_file = './data/stop_words.txt'
with open(stop_words_file, encoding='utf8') as fin:
    stop_words = set(line.strip() for line in fin if line.strip())
cut_words = max_cut.CutWords('/home/wuwei/projects/temp/text_matching/Model/word_embedding/800w-dict.txt', stop_words)

def avg_feature_vector(phrase, model, index2word_set, num_features=200):
    #words = cut_words.max_forward_cut(sentence)
    feature_vec = np.zeros((num_features, ), dtype='float32')
    n_words = 0
    phrase = phrase.lower()
    sentence = [x for x in jieba.cut(phrase)]
    new_sentence = []
    for i, word in enumerate(sentence):
        if word not in index2word_set:
            if word in dict_cut:
                words = dict_cut[word]
            else:
                words = cut_words.max_forward_cut(word)
                dict_cut[word] = words
            new_sentence.extend(words)
        else:
            new_sentence.extend([word])
    new_sentence = [x for x in new_sentence if x in index2word_set]
    for i, word in enumerate(new_sentence):
        n_words += 1
        feature_vec = np.add(feature_vec, model[word])
    if (n_words > 0):
        feature_vec = np.divide(feature_vec, n_words)
        return feature_vec.tolist()
    else:
        print(phrase)
#+END_SRC

- calculate vector similarity:
#+BEGIN_SRC python
def vec_sim(query1, query2):
    subject_vector1, subject_vector2 = avg_feature_vector(query1, wv_from_text, index2word_set),\
                        avg_feature_vector(query2, wv_from_text, index2word_set)
    sim = 1 - spatial.distance.cosine(subject_vector1, subject_vector2)
    if math.isnan(sim):
        sim = 0
    return sim

from scipy import spatial
import math
from tqdm import notebook, tqdm
df_query['entity_sim'] = None
for i, v in notebook.tqdm(df_query.iterrows()):
    entity_vector, value_vector = avg_feature_vector(df_query.loc[i, 'lclj'], wv_from_text, index2word_set),\
                            avg_feature_vector(df_query.loc[i, 'omaha_entity1'], wv_from_text, index2word_set)
    sim = 1 - spatial.distance.cosine(entity_vector, value_vector)
    if math.isnan(sim):
        sim = 0
    df_query.loc[i, 'entity_sim'] = sim
#+END_SRC

- string query similarities:
#+BEGIN_SRC python
def get_topn_values(arr, topn, axis=-1, ascending=False):
    '''
    从相似度一维数组获取最大(或最小)的top_n个索引及相似度.
    '''
    if type(arr) is not np.ndarray:
        arr = np.array(arr)
    if len(arr.shape) == 1:
        if not ascending:
            ind = np.argpartition(arr, -topn)[-topn:]
            ind = ind[np.argsort(arr[ind])[::-1]]
        else:
            ind = np.argpartition(arr, topn)[:topn]
            ind = ind[np.argsort(arr[ind])]
        return (ind, arr[ind])
    elif len(arr.shape) == 2:
        if axis == 0 or axis == -2:
            arr = arr.T
        ind = np.zeros([arr.shape[0], topn], dtype='int')
        val = np.zeros([arr.shape[0], topn], dtype=arr.dtype)
        for i in range(arr.shape[0]):
            ind[i], val[i] = get_topn_values(arr[i], topn=topn, ascending=ascending)
        if axis == 0 or axis == -2:
            ind = ind.T
            val = val.T
        return (ind, val)
    else:
        assert len(arr.shape) in (1, 2)


def get_semantic_sims(queries, candidates=None):
    sims = np.array([[vec_sim(s1, s2) for s2 in candidates] for s1 in queries], dtype=np.float32)
    return sims
#+END_SRC

- deep walk:
#+BEGIN_SRC python
G = nx.from_pandas_edgelist(df, "entityName1", "entityName2", edge_attr=True, create_using=nx.Graph())
get_randomwalk('布洛芬颗粒', 10)
def get_randomwalk(node, path_length):

    random_walk = [node]

    for i in range(path_length-1):
        temp = list(G.neighbors(node))
        temp = list(set(temp) - set(random_walk))
        if len(temp) == 0:
            break

        random_node = random.choice(temp)
        random_walk.append(random_node)
        node = random_node

    return random_walk

# get list of all nodes from the graph
all_nodes = list(G.nodes())

random_walks = []
for n in tqdm(all_nodes):
    for i in range(5):
        random_walks.append(get_randomwalk(n,10))

#+END_SRC
